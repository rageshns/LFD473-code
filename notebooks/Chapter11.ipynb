{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706bdbf0"
      },
      "source": [
        "# Chapter 11: Serving Models with TorchServe\n",
        "Installation Notes :\n",
        "To run this notebook on Google Colab, you will need to install the following libraries: torch-model-archiver, torchserve, captum, and pyngrok.\n",
        "\n",
        "In Google Colab, you can run the following command to install these libraries:"
      ],
      "id": "706bdbf0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2906de38"
      },
      "outputs": [],
      "source": [
        "!pip install torch-model-archiver torchserve captum pyngrok"
      ],
      "id": "2906de38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860cbba6"
      },
      "source": [
        "## 11.2 Learning Objectives\n",
        "\n",
        "By the end of this chapter, you should be able to:\n",
        "- understand, build, and assemble the necessary components into a model archive\n",
        "- serve a trained model locally using TorchServe"
      ],
      "id": "860cbba6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcdd57d"
      },
      "source": [
        "## 11.3 Archiving and Serving Models"
      ],
      "id": "7dcdd57d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Archiving and Serving Models: Overview\n",
        "It is surely fun to train models but, just like the expression \"pics or it didn't happen\", in deep learning it is \"deployed or it didn't happen.\" We're not going as far as developing a mobile app for users to classify images of fruits and vegetables, but we will neatly package all the necessary files in a model archive, and use TorchServe to serve this model on Google Colab.\n",
        "\n",
        "Before proceeding, let's load the model trained in the previous chapter first. You're free to use your own saved checkpoint, but we also provide our own for your convenience. You can download the fomo_model.pth file from the following link:\n",
        "\n",
        "https://github.com/lftraining/LFD273-code/releases/download/model/fomo_model.pth\n",
        "\n",
        "If you're using Google Colab, you can just run the command below to download it:"
      ],
      "metadata": {
        "id": "s7HqrX6rxG19"
      },
      "id": "s7HqrX6rxG19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iN52_d0ebee"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/dvgodoy/assets/releases/download/model/fomo_model.pth"
      ],
      "id": "9iN52_d0ebee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the file is downloaded, we can load the trained model:"
      ],
      "metadata": {
        "id": "WDRE3e59xLAV"
      },
      "id": "WDRE3e59xLAV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlYk-bqXfIBB",
        "outputId": "1161e176-91d4-4bb5-8d10-27d3dfa13844"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/pytorch_vision_main/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "repo = 'pytorch/vision:v0.15.2'\n",
        "model = torch.hub.load(repo, 'resnet18', weights=None)\n",
        "model.fc = nn.Linear(512, 4)\n",
        "\n",
        "state = torch.load('fomo_model.pth', map_location='cpu')\n",
        "model.load_state_dict(state)"
      ],
      "id": "YlYk-bqXfIBB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f58045c"
      },
      "source": [
        "### 11.3.1 Model Archiver\n",
        "\n",
        "Let's start with the model archive (`.mar`) file, a collection of files and folders zipped together that contains:\n",
        "- a `MAR-INF` folder with a `MANIFEST.json` file inside that describes the contents of the model archive itself, such as model and archiver versions, and the files that make up the archive\n",
        "- a serialized file containing the model's weights/state (`--serialized-file` argument)\n",
        "- a Python file containing only one class definition of our model's class inherited from `nn.Module` (only required if the model isn't scripted - more on that later) (`--model-file` argument)\n",
        "- an optional Python file containing one class definition of the handler's class inherited from `ts.torch_handler.BaseHandler` that performs the necessary transformations for pre- and post-processing  OR the name of a predefined handler (`--handler` argument)\n",
        "- an optional extra file `index_to_name.json` for mapping predicted class indices to its corresponding category names (automatically used by some predefined handlers) (`--extra-files` argument)\n",
        "\n",
        "It is typical to assemble the model archive file through the command line interface:\n",
        "\n",
        "```\n",
        "torch-model-archiver --model-name <your_model_name> \\\n",
        "                     --version <your_model_version> \\\n",
        "                     --model-file <your_model_file>.py \\\n",
        "                     --serialized-file <your_model_name>.pth \\\n",
        "                     --handler <handler-script OR name> \\\n",
        "                     --extra-files ./index_to_name.json\n",
        "```\n",
        "\n",
        "However, let's take a closer look at each one of its components and assemble it ourselves instead."
      ],
      "id": "7f58045c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dfd4a88"
      },
      "source": [
        "### 11.3.2 Model File\n",
        "\n",
        "The model file must contain a single class definition (inherited from nn.Module) corresponding to our model's architecture and forward pass. Ours is a slightly modified ResNet18 model, but how can we define its class without having to write down ResNet18 architecture from scratch?\n",
        "\n",
        "We need to:\n",
        "- define our own class\n",
        "- create an instance of an untrained ResNet18 model\n",
        "- replace its head (`fc` layer) with our own\n",
        "- update our own class internal dictionary with the entries from ResNet's dictionary\n",
        "- set ResNet's forward pass to our own class using `setattr`\n",
        "\n",
        "It looks like this:"
      ],
      "id": "1dfd4a88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbe811f3"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "class FOMONet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create an instance of an untrained ResNet18\n",
        "        resnet = resnet18(weights=None)\n",
        "        # Modifies the architecture to our task\n",
        "        resnet.fc = nn.Linear(512, 4)\n",
        "\n",
        "        # Replicate ResNet's modified architecture to FOMONet\n",
        "        self.__dict__.update(resnet.__dict__)\n",
        "        # Replicate Resnet's forward method to FOMONet\n",
        "        setattr(self, 'forward', resnet.forward)"
      ],
      "id": "cbe811f3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is hacky for sure, but it works. Let's make sure of it by loading our modified model's state dictionary into an instance of our FOMONet."
      ],
      "metadata": {
        "id": "gKLZ6NGTy2BF"
      },
      "id": "gKLZ6NGTy2BF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17dee7e",
        "outputId": "1f4b71fb-2a6e-4e64-9cd4-a3b188df6d37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fomo = FOMONet()\n",
        "fomo.load_state_dict(model.state_dict())"
      ],
      "id": "b17dee7e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "All keys matched! Time to try out the forward pass:"
      ],
      "metadata": {
        "id": "NJuetlw5y6LN"
      },
      "id": "NJuetlw5y6LN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eed5b52c",
        "outputId": "93210edf-1535-4627-91dd-c26ff2a1b6a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 0.2412, -2.8556, -1.1869,  0.8597]], grad_fn=<AddmmBackward0>),\n",
              " tensor([[ 0.2412, -2.8556, -1.1869,  0.8597]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fomo.eval()\n",
        "model.eval()\n",
        "\n",
        "torch.manual_seed(32)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "fomo(x), model.cpu()(x)"
      ],
      "id": "eed5b52c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's also a match!\n",
        "\n",
        "We have patched together our own model class, now we only need to write it to a Python file:"
      ],
      "metadata": {
        "id": "TGD3OK7i5jIG"
      },
      "id": "TGD3OK7i5jIG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75308290"
      },
      "outputs": [],
      "source": [
        "model_file_script = \"\"\"\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class FOMONet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create an instance of an untrained ResNet18\n",
        "        resnet = resnet18(weights=None)\n",
        "        # Modifies the architecture to our task\n",
        "        resnet.fc = nn.Linear(512, 4)\n",
        "\n",
        "        # Replicate ResNet's modified architecture to FOMONet\n",
        "        self.__dict__.update(resnet.__dict__)\n",
        "        # Replicate Resnet's forward method to FOMONet\n",
        "        setattr(self, 'forward', resnet.forward)\n",
        "\"\"\"\n",
        "\n",
        "with open('model_file.py', 'w') as fp:\n",
        "    fp.write(model_file_script)"
      ],
      "id": "75308290"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does it feel too hacky for you? Don't worry, the idea here was to prove that there's a (hacky) way of coming up with our own model file - if needed - without having to go through the whole ResNet's architecture.\n",
        "\n",
        "But, what if we did not need a model file at all?"
      ],
      "metadata": {
        "id": "b_u5R1eJ5moV"
      },
      "id": "b_u5R1eJ5moV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4042419b"
      },
      "source": [
        "### 11.3.3 Scripted Models\n",
        "\n",
        "We briefly touched upon the topic of scripting models when we discussed data augmentation and transformations that aren't \"scriptable\". Now, let's talk about TorchScript and what scripting a model actually means.\n",
        "\n",
        "\"*TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.*\"\n",
        "\n",
        "Source: [Torchscript](https://pytorch.org/docs/stable/jit.html)\n",
        "\n",
        "The key element here is \"*no Python dependency*\", meaning the model can be run in a standalone C++ program, for example. This preserves the best of both worlds: the ease and friendliness of the Python language for development, and the speed and reliability of the C++ language for deploying in production.\n",
        "\n",
        "We're not going into details here, we're just showing you an example of using PyTorch JIT, an optimized compiler for PyTorch programs, to script our model:"
      ],
      "id": "4042419b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e7631af"
      },
      "outputs": [],
      "source": [
        "# once it is scripted, there is no need for the model class def anymore\n",
        "scripted_model = torch.jit.script(model)"
      ],
      "id": "3e7631af"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script() method inspects the source code (our model), compiles it as TorchScript code using the compiler, and returns a ScriptModule (a wrapper around a C++ module) back.\n",
        "\n",
        "If you'd like to learn more about it, check the Introduction to [TorchScript tutorial](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)."
      ],
      "metadata": {
        "id": "yGNGACsP6A_E"
      },
      "id": "yGNGACsP6A_E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af97c6fa"
      },
      "source": [
        "### 11.3.4 Serialized File\n",
        "\n",
        "The serialized file has the model state/weights saved to disk. It may be the model in eager mode, saving it in the typical way we use for saving checkpoints, or it may be the scripted version of the model, which we can save using its own save() method instead:"
      ],
      "id": "af97c6fa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5151de67"
      },
      "outputs": [],
      "source": [
        "# We already saved the model to disk in the previous chapter\n",
        "# eager mode version\n",
        "torch.save(model.state_dict(), 'fomo_model.pth')\n",
        "\n",
        "# scripted version\n",
        "scripted_model.save(\"fomo_model.pt\")"
      ],
      "id": "5151de67"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although there are no enforced rules for naming the files, TorchServe assumes internally (in the BaseHandler code) that the .pt file extension corresponds to a saved scripted model, so we're adhering to this convention, and we're saving eager models using .pth as file extension."
      ],
      "metadata": {
        "id": "ZXQamP8g6S7k"
      },
      "id": "ZXQamP8g6S7k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb9e487"
      },
      "source": [
        "### 11.3.5 Inference Handler\n",
        "\n",
        "The handler takes care of pre- and post-processing the inputs and outputs, respectively, sending the former to the model and returning the latter to the user.\n",
        "\n",
        "There are several implemented [default handlers](https://pytorch.org/serve/default_handlers.html) in Torchserve:\n",
        "- `image_classifier`\n",
        "- `object_detector`\n",
        "- `text_classifier`\n",
        "- `image_segmenter`\n",
        "\n",
        "The first three handles also implement mapping the predicted class to its corresponding names/categories using an standard `index_to_name.json` extra file."
      ],
      "id": "bfb9e487"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the default handlers can seamlessly integrate pretrained models into TorchServe, more often than not you'll need to tweak them to accommodate changes in pre- or post-processing, or the fact that your task may have only two or a few classes (instead of the typical 1,000 classes of ImageNet).\n",
        "\n",
        "Therefore, we're taking a closer look at the internals of a handler, so you can more easily adjust them should the need arise. Each method is briefly introduced and it depicts an abbreviated version of its implementation in TorchServe."
      ],
      "metadata": {
        "id": "qvD28hme6i4P"
      },
      "id": "qvD28hme6i4P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3848245e"
      },
      "source": [
        "#### 11.3.5.1 Initialize\n",
        "The initialize() method mainly takes care of loading the pretrained model and setting it to evaluation mode. Notice that it assumes that, if a model file (.py) that defines the model class is available, the model is in eager mode, and it will load its saved state. Otherwise, if the file extension is .pt, it assumes it's dealing with a scripted model. You probably won't need to modify this method.\n",
        "```python\n",
        "def initialize(self, context):\n",
        "    \"\"\"Initialize function loads the model.pt file and initialized the model object.\n",
        "       First try to load torchscript else load eager mode state_dict based model.\n",
        "    \"\"\"\n",
        "    model_file = self.manifest[\"model\"].get(\"modelFile\", \"\")\n",
        "    if model_file:\n",
        "        self.model = self._load_pickled_model(model_dir, model_file, self.model_pt_path)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "    elif self.model_pt_path.endswith(\".pt\"):\n",
        "        self.model = self._load_torchscript_model(self.model_pt_path)\n",
        "        self.model.eval()\n",
        "```"
      ],
      "id": "3848245e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14be43e5"
      },
      "source": [
        "#### 11.3.5.2 Handle\n",
        "The handle() method manages the flow of information inside the handler. It calls the other methods in order: pre-processing, inference, and post-processing. You probably won't need to modify this method either.\n",
        "```python\n",
        "def handle(self, data, context):\n",
        "    \"\"\"Entry point for default handler. It takes the data from the input request and returns\n",
        "       the predicted outcome for the input.\n",
        "    \"\"\"\n",
        "    data_preprocess = self.preprocess(data)\n",
        "    output = self.inference(data_preprocess)\n",
        "    output = self.postprocess(output)\n",
        "\n",
        "    return output\n",
        "```"
      ],
      "id": "14be43e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c378c5d"
      },
      "source": [
        "#### 11.3.5.3 Preprocess\n",
        "The preprocess() method takes input data, as sent by the user, and turns it into a PyTorch-appropriate format, namely, tensors.\n",
        "\n",
        "In the VisionHandler class, this method extracts data from the HTTP request's body, applies the required transformations (image_processing() method) to normalize the data according to ImageNet statistics for mean and standard deviation, and stacks the results together.\n",
        "\n",
        "You may need to overwrite this method in your own handler class if your images do not follow typical ImageNet statistics, or if you're using a pretrained model that does not require this kind of preprocessing, as we've already discussed in the previous chapter.\n",
        "```python\n",
        "def preprocess(self, data):\n",
        "    \"\"\"\n",
        "    Preprocess function to convert the request input to a tensor(Torchserve supported format).\n",
        "    The user needs to override to customize the pre-processing\n",
        "    \"\"\"\n",
        "    images = []\n",
        "\n",
        "    for row in data:\n",
        "        # Compat layer: normally the envelope should just return the data\n",
        "        # directly, but older versions of Torchserve didn't have envelope.\n",
        "        image = row.get(\"data\") or row.get(\"body\")\n",
        "        if isinstance(image, str):\n",
        "            # if the image is a string of bytesarray.\n",
        "            image = base64.b64decode(image)\n",
        "\n",
        "        # If the image is sent as bytesarray\n",
        "        if isinstance(image, (bytearray, bytes)):\n",
        "            image = Image.open(io.BytesIO(image))\n",
        "            image = self.image_processing(image)\n",
        "        else:\n",
        "            # if the image is a list\n",
        "            image = torch.FloatTensor(image)\n",
        "\n",
        "        images.append(image)\n",
        "\n",
        "    return torch.stack(images).to(self.device)\n",
        "```"
      ],
      "id": "4c378c5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a0cdd07"
      },
      "source": [
        "Let's take a quick look at the `image_processing()` function that's called by the `preprocess()` method:"
      ],
      "id": "3a0cdd07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e67e050",
        "outputId": "36bae002-1d49-45f3-fd86-4de6a475f08d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:ts.torch_handler.base_handler:proceeding without onnxruntime\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ts.torch_handler.image_classifier import ImageClassifier\n",
        "\n",
        "ImageClassifier.image_processing"
      ],
      "id": "5e67e050"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, compare to the prescribed transformations when using our ResNet18 model:"
      ],
      "metadata": {
        "id": "gP7fpXgy6-Cw"
      },
      "id": "gP7fpXgy6-Cw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a427b9e",
        "outputId": "ef5f5540-769a-4679-a0e9-9177f5884f26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.models import get_weight\n",
        "\n",
        "weights = get_weight('ResNet18_Weights.DEFAULT')\n",
        "weights.transforms()"
      ],
      "id": "6a427b9e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "They look similar, but do they do exactly the same thing? Let's throw the same image at both of them and compare the transformed results. You can download one of the fig images from the link below:\n",
        "\n",
        "https://raw.githubusercontent.com/lftraining/LFD273-code/main/images/ch9/fig_0_100.jpg\n",
        "\n",
        "If you're using Google Colab, you can simply run the command below to download the image:"
      ],
      "metadata": {
        "id": "2cJmKucC7RZw"
      },
      "id": "2cJmKucC7RZw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmRWcNYLyttW"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch9/fig_0_100.jpg"
      ],
      "id": "MmRWcNYLyttW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the image is downloaded, let's transform it:"
      ],
      "metadata": {
        "id": "yFYJy9MH7Ur0"
      },
      "id": "yFYJy9MH7Ur0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8f01145",
        "outputId": "439793f6-4d27-4c50-dddf-665cddcd6d56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open('./fig_0_100.jpg')\n",
        "\n",
        "(ImageClassifier.image_processing(img) == weights.transforms()(img)).all()"
      ],
      "id": "d8f01145"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f2de09c"
      },
      "source": [
        "#### 11.3.5.4 Inference\n",
        "The inference() method sends the data to the device and sends it to the model to get predictions. You probably won't have to modify this method.\n",
        "```python\n",
        "def inference(self, data, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    The Inference Function is used to make a prediction call on the given input request.\n",
        "    The user needs to override the inference function to customize it.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        marshalled_data = data.to(self.device)\n",
        "        results = self.model(marshalled_data, *args, **kwargs)\n",
        "    return results\n",
        "```"
      ],
      "id": "1f2de09c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86477c8b"
      },
      "source": [
        "#### 11.3.5.5 Postprocess\n",
        "The postprocess() method takes the results of the inference() method and converts them back to a list so it can be returned to the user. In the ImageClassifier handler, the method converts logits into probabilities using the softmax function, and takes the top-K most likely classes to return to the user. The number of returned classes, indicated by the topk class attribute, is set to five by default.\n",
        "\n",
        "You may need to overwrite this method in your own handler class to suit your needs when it comes to the output expected by your users.\n",
        "```python\n",
        "def postprocess(self, data):\n",
        "    \"\"\"\n",
        "    The post process function makes use of the output from the inference and converts into a\n",
        "    Torchserve supported response output.\n",
        "    \"\"\"\n",
        "    ps = F.softmax(data, dim=1)\n",
        "    probs, classes = torch.topk(ps, self.topk, dim=1)\n",
        "    probs = probs.tolist()\n",
        "    classes = classes.tolist()\n",
        "    return map_class_to_label(probs, self.mapping, classes)\n",
        "```"
      ],
      "id": "86477c8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ee61f2"
      },
      "source": [
        "#### 11.3.5.6 Custom Handler\n",
        "In our example, the default value of topk is inconvenient (we only have four classes in total) and will raise an error if left like that. Fortunately, the ImageClassifier class also implements a set_max_result_classes() method, and we can leverage it to tweak the topk parameter in our very own handler class that inherits from it. We don't need to implement/modify any of the methods, other than the constructor:"
      ],
      "id": "f0ee61f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "815d7cf5"
      },
      "outputs": [],
      "source": [
        "handler_file_script = \"\"\"\n",
        "from ts.torch_handler.image_classifier import ImageClassifier\n",
        "\n",
        "class FOMOHandler(ImageClassifier):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      # By default, ImageClassifier uses top-5 classes\n",
        "      # but our task has only 4, so we need to tweak it\n",
        "      self.set_max_result_classes(4)\n",
        "\"\"\"\n",
        "\n",
        "with open('handler_file.py', 'w') as fp:\n",
        "    fp.write(handler_file_script)"
      ],
      "id": "815d7cf5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd336550"
      },
      "source": [
        "### 11.3.6 Extra Files"
      ],
      "id": "fd336550"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extra files include anything you may need in order to transform your user's requests into proper model inputs, or model outputs into proper responses to those requests. The typical, and already handled by default, extra file is the index_to_name.json file that allows handlers inherited from the VisionHandler class to map the predicted class indices back to their categorical names. The appropriate file for models pretrained on the ImageNet dataset can be found here, and it looks like this:\n",
        "\n",
        "{\n",
        " \"0\": [\"n01440764\", \"tench\"],\n",
        " \"1\": [\"n01443537\", \"goldfish\"]\n",
        "\n",
        " \"998\": [\"n13133613\", \"ear\"],\n",
        " \"999\": [\"n15075141\", \"toilet_tissue\"]\n",
        "}\n",
        "\n",
        "For our own FOMO dataset, we could reverse keys and values from the dataset's class_to_idx attribute."
      ],
      "metadata": {
        "id": "EO9OrKFx8Tys"
      },
      "id": "EO9OrKFx8Tys"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e3cf975"
      },
      "outputs": [],
      "source": [
        "# We didn't load the dataset in this chapter, so we're building the dict manually\n",
        "# class_to_idx = datasets['train'].class_to_idx\n",
        "\n",
        "class_to_idx = {'Fig': 0, 'Mandarine': 1, 'Onion White': 2, 'Orange': 3}"
      ],
      "id": "6e3cf975"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f0dab9",
        "outputId": "dcb764ba-d716-4db0-a0d8-c8d0c0dc16ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'Fig', 1: 'Mandarine', 2: 'Onion White', 3: 'Orange'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_to_name = {v: k for k, v in class_to_idx.items()}\n",
        "index_to_name"
      ],
      "id": "86f0dab9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's save this information to the corresponding file:"
      ],
      "metadata": {
        "id": "CEVWnqe38bkI"
      },
      "id": "CEVWnqe38bkI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3883ada"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('index_to_name.json', 'w') as f:\n",
        "    json.dump(index_to_name, f)"
      ],
      "id": "b3883ada"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab4ab790"
      },
      "source": [
        "### 11.3.7 Packaging\n",
        "At this point, we could go back to the CLI and run the following command to assemble our .mar file containing everything we'll need to serve our model:\n",
        "```\n",
        "torch-model-archiver --model-name FOMO> \\\n",
        "                     --version 1.0 \\\n",
        "                     --model-file ./model_file.py \\\n",
        "                     --serialized-file fomo_model.pth \\\n",
        "                     --handler ./handler_file.py \\\n",
        "                     --extra-files ./index_to_name.json\n",
        "```"
      ],
      "id": "ab4ab790"
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can also call the generate_model_archive() function directly, as if we were passing the arguments using the command line, and it will build the model archive for us.\n",
        "\n",
        "First, though, let's create a folder to store our models: model_store.\n",
        "\n",
        "If you're in Google Colab, you can create a folder running the command below:"
      ],
      "metadata": {
        "id": "CeObqKIa87az"
      },
      "id": "CeObqKIa87az"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5_jJT25FUzk"
      },
      "outputs": [],
      "source": [
        "!mkdir ./model_store"
      ],
      "id": "Y5_jJT25FUzk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the folder is created, we can generate the archive file that will be placed inside it:"
      ],
      "metadata": {
        "id": "tV60FcWS9I7g"
      },
      "id": "tV60FcWS9I7g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "044917c6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from model_archiver.model_packaging import generate_model_archive\n",
        "\n",
        "sys.argv = ['',\n",
        "            '--model-name', 'FOMO',\n",
        "            '--version', '1.0',\n",
        "            '--model-file', 'model_file.py',\n",
        "            '--serialized-file', 'fomo_model.pth',\n",
        "            '--handler', 'handler_file.py',\n",
        "            '--extra-files', 'index_to_name.json',\n",
        "            '--export-path', './model_store',\n",
        "            '--force']\n",
        "\n",
        "generate_model_archive()"
      ],
      "id": "044917c6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "All set! We have a model archive now, one we can easily serve using TorchServe."
      ],
      "metadata": {
        "id": "_1gn5rYo9MB0"
      },
      "id": "_1gn5rYo9MB0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9beb18f5"
      },
      "source": [
        "## 11.4 TorchServe\n",
        "\n",
        "[TorchServe](https://pytorch.org/serve/) is a flexible and easy to use tool for serving and scaling PyTorch eager mode and scripted models in production. It offers APIs for querying, managing, and analyzing the performance of its served models (by default, they are only accessible from localhost):\n",
        "\n",
        "- [Inference API](https://github.com/pytorch/serve/blob/master/docs/inference_api.md): it listens to port 8080, and it offers the following services\n",
        "  - description (`OPTIONS /`)\n",
        "  - health check (`GET /ping`)\n",
        "  - predictions (`POST {/predictions/{model_name}`)\n",
        "  - explanations (`POST /explanations/{model_name}`)\n",
        "  - kserve (`/v1/models/{model_name}:predict:`)\n",
        "  - kserve explanations (`/v1/models/{model_name}:explain:`)\n",
        "  \n",
        "- [Management API](https://github.com/pytorch/serve/blob/master/docs/management_api.md): it listens to port 8081, and it offers the following services\n",
        "  - description (`OPTIONS /`)\n",
        "  - list models (`GET /models`)\n",
        "  - describe a model (`GET /models/{model_name}`)\n",
        "  - register a model (`POST /models`)\n",
        "  - scale workers (`POST /models/{model_name}`)\n",
        "  - set default version (`PUT /models/{model_name}/{version}/set-default`)\n",
        "  - unregister a model (`DELETE /models/{model_name}/{version}`)\n",
        "  \n",
        "- [Metrics API](https://github.com/pytorch/serve/blob/master/docs/metrics_api.md): it listens to port 8082, and it returns Prometheus-formatted frontend and backend metrics, such as number of requests, CPU and memory utilization, handler and prediction time, and many more.\n",
        "\n",
        "In this course, we're only illustrating the basic functionalities of TorchServe, so we're focusing on the /predictions service of the Inference API only.\n",
        "\n",
        "It is typical to run TorchServe through the command line interface. To start TorchServe, serving our FOMO model (archived as FOMO.mar in the model_store folder) through a custom port (as in the config.properties file), we would run the following command:\n",
        "```\n",
        "torchserve --start \\\n",
        "           --disable-token-auth \\\n",
        "           --model-store ./model_store \\\n",
        "           --models fomo=FOMO.mar \\\n",
        "           --ts-config config.properties\n",
        "```"
      ],
      "id": "9beb18f5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, since you're likely running this notebook on Google Colab or some other platform, it is probably more convenient to start TorchServe by calling the appropriate Python function instead.\n",
        "\n",
        "First, though, let's configure it in such a way that it uses a different port for inference:"
      ],
      "metadata": {
        "id": "IPnaxnqx9aoA"
      },
      "id": "IPnaxnqx9aoA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06345f96"
      },
      "outputs": [],
      "source": [
        "config_properties = \"\"\"\n",
        "inference_address=http://127.0.0.1:7777\n",
        "\"\"\"\n",
        "\n",
        "with open('config.properties', 'w') as fp:\n",
        "    fp.write(config_properties)"
      ],
      "id": "06345f96"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will allow you to locally submit requests to your model. If you'd like to bind the inference API to all network interfaces, you should use 0.0.0.0 as the address instead. It won't work on Google Colab, though.\n",
        "\n",
        "Now, we can call the start() function as if we were passing arguments using the command line:"
      ],
      "metadata": {
        "id": "3RJoiYjl9fOI"
      },
      "id": "3RJoiYjl9fOI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7f6429f"
      },
      "outputs": [],
      "source": [
        "from ts.model_server import start\n",
        "\n",
        "sys.argv = ['',\n",
        "            '--start',\n",
        "            '--disable-token-auth',\n",
        "            '--model-store', './model_store',\n",
        "            '--models', 'fomo=FOMO.mar',\n",
        "            '--ts-config', 'config.properties']\n",
        "start()"
      ],
      "id": "b7f6429f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After a few seconds (don't rush into running the following cell or you may get a connection refused error), your server should be up and running, and you can submit requests to it using your local address (127.0.0.1) and the inference API port (7777), invoking your model (as named in the models argument, so it's fomo in our case) together with the data (an image):"
      ],
      "metadata": {
        "id": "_c7yMvPw9kJf"
      },
      "id": "_c7yMvPw9kJf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e458c243",
        "outputId": "3ce26f4f-454d-4e27-90de-d1352d9dcaa7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Fig': 0.9934685230255127,\n",
              " 'Orange': 0.004324017558246851,\n",
              " 'Onion White': 0.0012627042597159743,\n",
              " 'Mandarine': 0.0009447108022868633}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "with open('./fig_0_100.jpg', 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "response = requests.put('http://127.0.0.1:7777/predictions/fomo', data=data)\n",
        "response.json()"
      ],
      "id": "e458c243"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You deployed a model in production using TorchServe!\n",
        "\n",
        "Once we're done with it, we can stop the server by, somewhat ironically, calling the start() function once again with the --stop argument:"
      ],
      "metadata": {
        "id": "S1Eaacx09sD8"
      },
      "id": "S1Eaacx09sD8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f64a41a",
        "outputId": "be7595a3-45c3-4572-d22e-961eec3d9a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TorchServe has stopped.\n"
          ]
        }
      ],
      "source": [
        "#!torchserve --stop\n",
        "sys.argv = ['', '--stop']\n",
        "start()"
      ],
      "id": "8f64a41a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is really cool to see your own code up and running in Google Colab, being able to send HTTP requests to it and getting predictions back, but wouldn't it be even cooler to be able to show it to your friends or colleagues?"
      ],
      "metadata": {
        "id": "nVXCwxXd9veJ"
      },
      "id": "nVXCwxXd9veJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb4111d"
      },
      "source": [
        "### 11.4.1 Ngrok (optional)\n",
        "\n",
        "\"*Online in One Line*\" reads the [ngrok](https://ngrok.com/) website. It is an easy and convenient way of serving your model through a tunnel, thus allowing it to handle incoming requests from the outside world in your own Jupyter Notebook.\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: You should NOT use Google Colab notebooks as backend for your deployed models. This is just a proof-of-concept, and a way to make your model available to the world for a brief amount of time, so you can showcase it to your family, friends, or colleagues.\n",
        "***\n",
        "\n",
        "If you want to try the code below, you'll need to [signup](https://dashboard.ngrok.com/signup) for a free account on [ngrok](https://ngrok.com/) and, once you're done, you can install the [pyngrok](https://pypi.org/project/pyngrok/) package that takes care of downloading and installing ngrok:"
      ],
      "id": "4cb4111d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6811ea0"
      },
      "source": [
        "You'll need to copy your [authorization token](https://dashboard.ngrok.com/get-started/your-authtoken) and paste it in the appropriate command below:\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: The responsibility for keeping your credentials and/or authorization tokens safe and private is your own. Make sure to remove any credentials and/or authorizations tokens from your notebook before saving or pushing it to public repositories, such as GitHub.\n",
        "***"
      ],
      "id": "b6811ea0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f621f45",
        "outputId": "a950a5e0-1a74-4283-914d-c13c01a06f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Option 1\n",
        "# You can call ngrok with your token\n",
        "# Uncomment the line below and replace ... with your token\n",
        "# !ngrok authtoken ...\n",
        "\n",
        "# Option 2\n",
        "# Or you can save it to a configuration file\n",
        "# Uncomment the line below and replace ... with your token\n",
        "# !echo \"authtoken: ...\" >> /root/.ngrok2/ngrok.yml"
      ],
      "id": "5f621f45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78bdddb"
      },
      "source": [
        "Once ngrok is setup, let's start Torchserve once again with a few modifications in the `config.properties` file:\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: CORS stands for cross-origin resource sharing, and the configuration below makes Torchserve wide open to requests from anywhere. You SHOULD NOT use these configuration parameters in production as they're not safe. The responsibility for ensuring the security of your application, model, and data, is your own.\n",
        "***"
      ],
      "id": "a78bdddb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef8fa355"
      },
      "outputs": [],
      "source": [
        "config_properties = \"\"\"\n",
        "inference_address=http://127.0.0.1:7777\n",
        "cors_allowed_origin=*\n",
        "cors_allowed_methods=GET, POST, PUT, OPTIONS\n",
        "\"\"\"\n",
        "\n",
        "with open('config_cors.properties', 'w') as fp:\n",
        "    fp.write(config_properties)"
      ],
      "id": "ef8fa355"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f57fa8c"
      },
      "outputs": [],
      "source": [
        "sys.argv = ['',\n",
        "            '--start',\n",
        "            '--model-store', './model_store',\n",
        "            '--models', 'fomo=FOMO.mar',\n",
        "            '--ts-config', 'config_cors.properties']\n",
        "start()"
      ],
      "id": "2f57fa8c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "TorchServe should be up and running already, so we can use ngrok to build a tunnel and forward external requests to it. Since we're using a non-standard 7777 port, we need to specify it as the port that's handling HTTP requests:"
      ],
      "metadata": {
        "id": "I73bX-2m-e12"
      },
      "id": "I73bX-2m-e12"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bc063d6",
        "outputId": "f43f3562-5934-4c89-b2e7-ae660d4fb919"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-07-02T12:14:34+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# <NgrokTunnel: \"http://<public_sub>.ngrok.io\" -> \"http://localhost:7777\">\n",
        "http_tunnel = ngrok.connect(7777, \"http\")"
      ],
      "id": "4bc063d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e3014e2b",
        "outputId": "bfcb68d1-b71f-495f-a97d-3ce8385e24fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://f295-35-202-252-169.ngrok-free.app'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "http_tunnel.public_url"
      ],
      "id": "e3014e2b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tunnel's public URL can be found in the public_url attribute:"
      ],
      "metadata": {
        "id": "opYUhGVH-nHW"
      },
      "id": "opYUhGVH-nHW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you (or anyone else) can send requests to your model, provided they know the public URL and the name of your model (fomo, in our example).\n",
        "\n",
        "To make predictions, we need to send a PUT request with the image data, as shown below:"
      ],
      "metadata": {
        "id": "03op95cQ_Sqc"
      },
      "id": "03op95cQ_Sqc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5118d4aa",
        "outputId": "9bd3fb2e-eeab-408d-8893-831474567311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Fig': 0.9934685230255127,\n",
              " 'Orange': 0.004324017558246851,\n",
              " 'Onion White': 0.0012627042597159743,\n",
              " 'Mandarine': 0.0009447108022868633}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('./fig_0_100.jpg', 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "response = requests.put(f'{http_tunnel.public_url}/predictions/fomo', data=data)\n",
        "response.json()"
      ],
      "id": "5118d4aa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response, in JSON, contains the predictions of our served model! Congratulations! Now you can showcase your model to your family, friends, and colleagues!\n",
        "\n",
        "Once you're done, you can disconnect the tunnel and stop TorchServe:"
      ],
      "metadata": {
        "id": "uJYkNagh_bb6"
      },
      "id": "uJYkNagh_bb6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9fad67d"
      },
      "outputs": [],
      "source": [
        "ngrok.disconnect(http_tunnel.public_url)"
      ],
      "id": "a9fad67d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dd0aaf5",
        "outputId": "555412ca-3dbc-45a9-8bd7-1ed8be392851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TorchServe has stopped.\n"
          ]
        }
      ],
      "source": [
        "sys.argv = ['', '--stop']\n",
        "start()"
      ],
      "id": "3dd0aaf5"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}