{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "210855b6",
      "metadata": {
        "id": "210855b6"
      },
      "source": [
        "# Chapter 8: Pretrained Models for Natural Language Processing\n",
        "\n",
        "Installation Notes\n",
        "To run this notebook on Google Colab, you will need to install the following libraries: transformers and datasets.\n",
        "\n",
        "In Google Colab, you can run the following command to install these libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b0d7f4",
      "metadata": {
        "id": "72b0d7f4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9269443",
      "metadata": {
        "id": "c9269443"
      },
      "source": [
        "## 8.2 Learning Objectives\n",
        "\n",
        "By the end of this chapter, you should be able to:\n",
        "- understand the role of tokenization in preprocessing sentences as inputs\n",
        "- load pretrained models and pipelines for NLP using HuggingFace\n",
        "- understand the general idea behind generative models for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4115c436",
      "metadata": {
        "id": "4115c436"
      },
      "source": [
        "## 8.3 Natural Language Processing\n",
        "\n",
        "There are many datasets and models for Natural Language Processing available in the Hugging Face Hub. Each model has a corresponding tokenizer, which can be used to preprocess and format the text, turning it into a proper input for the model. First, we'll use the RoBERTa model to have an overview of the architecture and capabilities of language models in general, and then we'll use Hugging Face pipelines to perform some typical NLP tasks out-of-the-box. In the fourth part of the course, we'll explore these topics in further detail."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e71f7ac",
      "metadata": {
        "id": "1e71f7ac"
      },
      "source": [
        "### 8.3.1 Model\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create an instance of a [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta) model, we load its corresponding RobertaConfig, that specifies many aspects of the model architecture and configuration, such as the number of embedding dimensions, the maximum sequence length, and the vocabulary size, and use it as an argument to the RobertaModel class."
      ],
      "metadata": {
        "id": "dRadwY8TT3Df"
      },
      "id": "dRadwY8TT3Df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204d8cf4",
      "metadata": {
        "id": "204d8cf4",
        "outputId": "ae36c665-81fa-4572-f4e7-23aa6d03876a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.44.0\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import RobertaConfig, RobertaModel\n",
        "\n",
        "configuration = RobertaConfig()\n",
        "configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loaded configuration specifies the architectural details of the model. We can then use it to create an instance of the RoBERTa model:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsLmhk68UPBq"
      },
      "id": "ZsLmhk68UPBq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9bd155",
      "metadata": {
        "id": "1a9bd155",
        "outputId": "75873c3b-d26c-4e7b-fc0e-7acce1136035"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# random weights\n",
        "model = RobertaModel(configuration)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The created model is, however, untrained. If, instead of training it from scratch, we would like to load its pretrained weights, we can call its from_pretrained() method:"
      ],
      "metadata": {
        "id": "aA06aY4tV7Nr"
      },
      "id": "aA06aY4tV7Nr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a627ab10",
      "metadata": {
        "id": "a627ab10",
        "outputId": "7b38d2b6-4aa5-4cfd-e9af-d83e758ce8e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "repo_id = \"FacebookAI/roberta-base\"\n",
        "model = RobertaModel.from_pretrained(repo_id)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an encoder-based model, as stated by its name, and it's based on the Transformer architecture (which we'll discuss in further detail in \"Contextual Word Embeddings with Transformers\"). While encoder-based models are useful for tasks like classification, decoder-based models are mostly used for generating data (e.g. GPT), as we'll see later in this chapter.\n",
        "\n",
        "Talking about classification, what about RoBERTa's \"head\", that is, the classifier part that we've been seeing at the top of every computer vision model we used so far?\n",
        "\n",
        "It turns out, this model is headless: there's no classifier head. Moreover, its last layer, pooler.dense, was not loaded either (notice the warning message above, suggesting the model needs to be further trained in a down-stream task)."
      ],
      "metadata": {
        "id": "APl9su96V_mD"
      },
      "id": "APl9su96V_mD"
    },
    {
      "cell_type": "markdown",
      "id": "1952f613",
      "metadata": {
        "id": "1952f613"
      },
      "source": [
        "### 8.3.2 Tokenizers\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to computer vision models, NLP models also have prescribed transformations that we must apply to our input texts. These are carried out by tokenizers, an important and often overlooked part of language models.\n",
        "\n",
        "Let's load RoBERTa's tokenizer and see what it does:"
      ],
      "metadata": {
        "id": "bcFA8w4yWINK"
      },
      "id": "bcFA8w4yWINK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2574faab",
      "metadata": {
        "id": "2574faab",
        "outputId": "abeab659-759e-4655-999b-0326e6006f60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dvgodoy/anaconda3/envs/pyt20/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaTokenizer(name_or_path='FacebookAI/roberta-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(repo_id)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenizer performs a whole sequence of operations, as indicated by its many configuration arguments:\n",
        "\n",
        "vocabulary (vocab_size)\n",
        "truncation (model_max_length and truncation_side)\n",
        "padding (padding_side)\n",
        "special token prepending and/or appending (special_tokens)\n",
        "Let's take a quick look at each one of these components. We'll get back to all these operations in much more detail in \"Contextual Word Embeddings with Transformers\"."
      ],
      "metadata": {
        "id": "3pX4KHcmWV4q"
      },
      "id": "3pX4KHcmWV4q"
    },
    {
      "cell_type": "markdown",
      "id": "df279346",
      "metadata": {
        "id": "df279346"
      },
      "source": [
        "#### 8.3.2.1 Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP: Tokenizers: Tokenization\n",
        "The tokenizer breaks up a sentence into its components - these are typically words or subwords (one or more syllables, in general). Each token will eventually be converted into an array of numerical values - its corresponding embedding. In computer vision, it was of the utmost importance to properly standardize input images using the same mean and standard deviation used during pretraining. The same holds true for NLP when it comes to using the right tokenizer and vocabulary: you must use the same ones as the pretrained model. We'll get back to it in more detail in \"Contextual Word Embeddings with Transformers\".\n",
        "\n",
        "For now, let's see what the output of the tokenizer's tokenize() method looks like:"
      ],
      "metadata": {
        "id": "RcYbAJCvWdPi"
      },
      "id": "RcYbAJCvWdPi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43c9b55a",
      "metadata": {
        "id": "43c9b55a",
        "outputId": "1e6ec5ed-e209-4d34-caca-b86a7b7fd522"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'Ġam', 'Ġreally', 'Ġliking', 'Ġthis', 'Ġcourse', '!']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch = [\"I am really liking this course!\", \"This course is too complicated!\"]\n",
        "\n",
        "tokenized = tokenizer.tokenize(input_batch[0])\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentence was split into several components. Even though the tokens themselves represent slightly different versions of the words, they still hold a correspondence to the original words. Let's convert a single token - \"I\" - to its corresponding numerical ID and back (using the decode() method):"
      ],
      "metadata": {
        "id": "7pPG2PuYW_Bv"
      },
      "id": "7pPG2PuYW_Bv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d13b8fb",
      "metadata": {
        "id": "9d13b8fb",
        "outputId": "df931a02-ab5d-494a-9a3a-011beafdcf4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 'I')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_tokens_to_ids('I'), tokenizer.decode(tokenizer.convert_tokens_to_ids('I'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also do the same for the whole sentence:"
      ],
      "metadata": {
        "id": "Y-guPPDWXao_"
      },
      "id": "Y-guPPDWXao_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d568fe3",
      "metadata": {
        "id": "9d568fe3",
        "outputId": "02c84e4a-f985-4c56-dac4-e2326b05c5ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([100, 524, 269, 25896, 42, 768, 328], 'I am really liking this course!')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenized), tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenized))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example above, each word corresponded to a single token, but that's not always the case. Different tokenizers may split a word into multiple components, depending on their vocabularies."
      ],
      "metadata": {
        "id": "kGZwWscJXfp6"
      },
      "id": "kGZwWscJXfp6"
    },
    {
      "cell_type": "markdown",
      "id": "62b784f0",
      "metadata": {
        "id": "62b784f0"
      },
      "source": [
        "#### 8.3.2.2 Vocabulary\n",
        "\n",
        "#### NLP: Tokenizers: Vocabulary\n",
        "The vocabulary is the exhaustive list of tokens that can be handled by the model. If a token is not present in the vocabulary, though, it still can default to an \"unknown\" token, as long as the \"unknown\" token is part of the vocabulary.\n",
        "\n",
        "The tokenizer's vocabulary can be easily accessed through its get_vocab() method. It returns a dictionary that maps every token to a corresponding numerical ID:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9966d29",
      "metadata": {
        "id": "a9966d29",
        "outputId": "ad136bd3-eae3-46d8-ba9d-8973960b64af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<s>': 0,\n",
              " '<pad>': 1,\n",
              " '</s>': 2,\n",
              " '<unk>': 3,\n",
              " '.': 4,\n",
              " 'Ġthe': 5,\n",
              " ',': 6,\n",
              " 'Ġto': 7,\n",
              " 'Ġand': 8,\n",
              " 'Ġof': 9,\n",
              " 'Ġa': 10,\n",
              " 'Ġin': 11,\n",
              " '-': 12,\n",
              " 'Ġfor': 13,\n",
              " 'Ġthat': 14,\n",
              " 'Ġon': 15,\n",
              " 'Ġis': 16,\n",
              " 'âĢ': 17,\n",
              " \"'s\": 18,\n",
              " 'Ġwith': 19,\n",
              " 'ĠThe': 20,\n",
              " 'Ġwas': 21,\n",
              " 'Ġ\"': 22,\n",
              " 'Ġat': 23,\n",
              " 'Ġit': 24,\n",
              " 'Ġas': 25,\n",
              " 'Ġsaid': 26,\n",
              " 'Ļ': 27,\n",
              " 'Ġbe': 28,\n",
              " 's': 29,\n",
              " 'Ġby': 30,\n",
              " 'Ġfrom': 31,\n",
              " 'Ġare': 32,\n",
              " 'Ġhave': 33,\n",
              " 'Ġhas': 34,\n",
              " ':': 35,\n",
              " 'Ġ(': 36,\n",
              " 'Ġhe': 37,\n",
              " 'ĠI': 38,\n",
              " 'Ġhis': 39,\n",
              " 'Ġwill': 40,\n",
              " 'Ġan': 41,\n",
              " 'Ġthis': 42,\n",
              " ')': 43,\n",
              " 'ĠâĢ': 44,\n",
              " 'Ġnot': 45,\n",
              " 'Ŀ': 46,\n",
              " 'Ġyou': 47,\n",
              " 'ľ': 48,\n",
              " 'Ġtheir': 49,\n",
              " 'Ġor': 50,\n",
              " 'Ġthey': 51,\n",
              " 'Ġwe': 52,\n",
              " 'Ġbut': 53,\n",
              " 'Ġwho': 54,\n",
              " 'Ġmore': 55,\n",
              " 'Ġhad': 56,\n",
              " 'Ġbeen': 57,\n",
              " 'Ġwere': 58,\n",
              " 'Ġabout': 59,\n",
              " ',\"': 60,\n",
              " 'Ġwhich': 61,\n",
              " 'Ġup': 62,\n",
              " 'Ġits': 63,\n",
              " 'Ġcan': 64,\n",
              " 'Ġone': 65,\n",
              " 'Ġout': 66,\n",
              " 'Ġalso': 67,\n",
              " 'Ġ$': 68,\n",
              " 'Ġher': 69,\n",
              " 'Ġall': 70,\n",
              " 'Ġafter': 71,\n",
              " '.\"': 72,\n",
              " '/': 73,\n",
              " 'Ġwould': 74,\n",
              " \"'t\": 75,\n",
              " 'Ġyear': 76,\n",
              " 'Ġwhen': 77,\n",
              " 'Ġfirst': 78,\n",
              " 'Ġshe': 79,\n",
              " 'Ġtwo': 80,\n",
              " 'Ġover': 81,\n",
              " 'Ġpeople': 82,\n",
              " 'ĠA': 83,\n",
              " 'Ġour': 84,\n",
              " 'ĠIt': 85,\n",
              " 'Ġtime': 86,\n",
              " 'Ġthan': 87,\n",
              " 'Ġinto': 88,\n",
              " 'Ġthere': 89,\n",
              " 't': 90,\n",
              " 'ĠHe': 91,\n",
              " 'Ġnew': 92,\n",
              " 'ĠâĢĶ': 93,\n",
              " 'Ġlast': 94,\n",
              " 'Ġjust': 95,\n",
              " 'ĠIn': 96,\n",
              " 'Ġother': 97,\n",
              " 'Ġso': 98,\n",
              " 'Ġwhat': 99,\n",
              " 'I': 100,\n",
              " 'Ġlike': 101,\n",
              " 'a': 102,\n",
              " 'Ġsome': 103,\n",
              " 'S': 104,\n",
              " 'Ã«': 105,\n",
              " 'Ġthem': 106,\n",
              " 'Ġyears': 107,\n",
              " \"'\": 108,\n",
              " 'Ġdo': 109,\n",
              " 'Ġyour': 110,\n",
              " 'Ġ-': 111,\n",
              " 'Ġ1': 112,\n",
              " '\"': 113,\n",
              " 'Ġif': 114,\n",
              " 'Ġcould': 115,\n",
              " '?': 116,\n",
              " 'Ġno': 117,\n",
              " 'i': 118,\n",
              " 'm': 119,\n",
              " 'Ġget': 120,\n",
              " 'ĠU': 121,\n",
              " 'Ġnow': 122,\n",
              " 'Ġhim': 123,\n",
              " 'Ġback': 124,\n",
              " 'ĠBut': 125,\n",
              " 'ĠâĢĵ': 126,\n",
              " 'Ġmy': 127,\n",
              " \"Ġ'\": 128,\n",
              " 'Ġonly': 129,\n",
              " 'Ġthree': 130,\n",
              " ';': 131,\n",
              " 'Ġ2': 132,\n",
              " 'The': 133,\n",
              " '1': 134,\n",
              " 'Ġpercent': 135,\n",
              " 'Ġagainst': 136,\n",
              " 'Ġbefore': 137,\n",
              " 'Ġcompany': 138,\n",
              " 'o': 139,\n",
              " 'ĠTrump': 140,\n",
              " 'Ġhow': 141,\n",
              " 'Ġbecause': 142,\n",
              " 'Ġany': 143,\n",
              " 'Ġmost': 144,\n",
              " 'Ġbeing': 145,\n",
              " 'Ġmake': 146,\n",
              " 'Ġwhere': 147,\n",
              " 'Ġduring': 148,\n",
              " 'Ġthrough': 149,\n",
              " 'Ġwhile': 150,\n",
              " '000': 151,\n",
              " 'ĠThis': 152,\n",
              " 'Ġmillion': 153,\n",
              " 'ing': 154,\n",
              " 'Ġ3': 155,\n",
              " 'Ġmade': 156,\n",
              " 'Ġwell': 157,\n",
              " 'Ġ10': 158,\n",
              " 'Ġdown': 159,\n",
              " 'Ġoff': 160,\n",
              " 'Ġsays': 161,\n",
              " 'Ġme': 162,\n",
              " 'ĠB': 163,\n",
              " 'Ġgoing': 164,\n",
              " 'Ġteam': 165,\n",
              " 'ĠWe': 166,\n",
              " 'Ġthose': 167,\n",
              " 'Ġgovernment': 168,\n",
              " 'Ġway': 169,\n",
              " 'We': 170,\n",
              " 'Ġmany': 171,\n",
              " 'Ġthen': 172,\n",
              " 'Ġwork': 173,\n",
              " 'Ġtold': 174,\n",
              " 'com': 175,\n",
              " '2': 176,\n",
              " 'Ġgame': 177,\n",
              " 'ĠAnd': 178,\n",
              " 'in': 179,\n",
              " 'year': 180,\n",
              " 'Ġp': 181,\n",
              " 'Ġvery': 182,\n",
              " 'Ġday': 183,\n",
              " 'Ġhome': 184,\n",
              " 'Ġtake': 185,\n",
              " 'Ġweek': 186,\n",
              " 'Ġsince': 187,\n",
              " 'ĠNew': 188,\n",
              " 'Ġmay': 189,\n",
              " 'Ġeven': 190,\n",
              " 'Ġseason': 191,\n",
              " 'Ġsee': 192,\n",
              " 'Ġ2017': 193,\n",
              " 'Ġstate': 194,\n",
              " 'Ġ5': 195,\n",
              " 'ed': 196,\n",
              " 'Ġshould': 197,\n",
              " 'Ġaround': 198,\n",
              " 'Ġ2018': 199,\n",
              " 'Ġsecond': 200,\n",
              " 'Ġus': 201,\n",
              " 'Ġstill': 202,\n",
              " 'Ġmuch': 203,\n",
              " 'Ġ4': 204,\n",
              " 'Ġgood': 205,\n",
              " 'Ġthink': 206,\n",
              " '%': 207,\n",
              " 'ĠS': 208,\n",
              " 'Ġthese': 209,\n",
              " 'Ġmarket': 210,\n",
              " 'ĠD': 211,\n",
              " 'th': 212,\n",
              " 'Ġgo': 213,\n",
              " \"'re\": 214,\n",
              " 'Ġsuch': 215,\n",
              " 'Ġknow': 216,\n",
              " 'Ġincluding': 217,\n",
              " 'Ġdon': 218,\n",
              " 'y': 219,\n",
              " 'Ġnext': 220,\n",
              " 'ĠP': 221,\n",
              " 'Ġdid': 222,\n",
              " 'Ġunder': 223,\n",
              " 'Ġsay': 224,\n",
              " 'en': 225,\n",
              " 'ĠL': 226,\n",
              " 'Ġbetween': 227,\n",
              " 'Ġper': 228,\n",
              " 'ĠK': 229,\n",
              " 'ĠC': 230,\n",
              " 'Ġ6': 231,\n",
              " 'Ġworld': 232,\n",
              " 'Ġpart': 233,\n",
              " 'ĠN': 234,\n",
              " 'Ġright': 235,\n",
              " 'Ġwant': 236,\n",
              " 'Ġfour': 237,\n",
              " '),': 238,\n",
              " 'Ġhigh': 239,\n",
              " 'Ġneed': 240,\n",
              " 're': 241,\n",
              " 'e': 242,\n",
              " 'It': 243,\n",
              " 'Ġhelp': 244,\n",
              " '5': 245,\n",
              " '3': 246,\n",
              " 'Ġcountry': 247,\n",
              " 'ĠR': 248,\n",
              " 'Ġpolice': 249,\n",
              " 'A': 250,\n",
              " 'Ġlong': 251,\n",
              " 'ĠThey': 252,\n",
              " 'Ġend': 253,\n",
              " 'er': 254,\n",
              " 'ĠT': 255,\n",
              " 'ĠM': 256,\n",
              " 'u': 257,\n",
              " 'Ġboth': 258,\n",
              " 'Ġhere': 259,\n",
              " 'an': 260,\n",
              " 'on': 261,\n",
              " 'Ġ7': 262,\n",
              " 'Ġde': 263,\n",
              " 'ĠShe': 264,\n",
              " 'Ġbusiness': 265,\n",
              " 'Ġreport': 266,\n",
              " 'j': 267,\n",
              " 'ers': 268,\n",
              " 'Ġreally': 269,\n",
              " 'ĠPresident': 270,\n",
              " 'ar': 271,\n",
              " 'ĠG': 272,\n",
              " 'ĠFriday': 273,\n",
              " 'ĠF': 274,\n",
              " 'Ġbest': 275,\n",
              " 'Ġsame': 276,\n",
              " 'Ġanother': 277,\n",
              " 'Ġset': 278,\n",
              " 'old': 279,\n",
              " 'ĠThat': 280,\n",
              " 'as': 281,\n",
              " 'n': 282,\n",
              " 'Ġcome': 283,\n",
              " 'Ġfamily': 284,\n",
              " 'Ġpublic': 285,\n",
              " 'ĠFor': 286,\n",
              " 'ĠAs': 287,\n",
              " '0': 288,\n",
              " 'ĠH': 289,\n",
              " 'Ġ8': 290,\n",
              " 'Ġ20': 291,\n",
              " 'Ġfive': 292,\n",
              " 'es': 293,\n",
              " 'ĠTuesday': 294,\n",
              " 'Ġn': 295,\n",
              " 'ĠThursday': 296,\n",
              " 'Ġquarter': 297,\n",
              " 'h': 298,\n",
              " 'Ġtop': 299,\n",
              " 'Ġgot': 300,\n",
              " 'Ġlife': 301,\n",
              " 'ĠMonday': 302,\n",
              " 'Ġfound': 303,\n",
              " 'Ġuse': 304,\n",
              " 'ĠW': 305,\n",
              " '4': 306,\n",
              " 'ĠWednesday': 307,\n",
              " 'Ġown': 308,\n",
              " 'Ġaccording': 309,\n",
              " 'Ġplay': 310,\n",
              " 'Ġshow': 311,\n",
              " 'ĠSt': 312,\n",
              " 'Ġman': 313,\n",
              " 'Ġleft': 314,\n",
              " 'ĠUnited': 315,\n",
              " 'Ġ12': 316,\n",
              " 'Ġplace': 317,\n",
              " 'ĠIf': 318,\n",
              " 'Ġlot': 319,\n",
              " 'Ġformer': 320,\n",
              " 'Ġ0': 321,\n",
              " ').': 322,\n",
              " 'Ġsupport': 323,\n",
              " 'ie': 324,\n",
              " 'Ġbillion': 325,\n",
              " 'Ġt': 326,\n",
              " 'Ġshares': 327,\n",
              " '!': 328,\n",
              " 'z': 329,\n",
              " 'k': 330,\n",
              " 'ĠState': 331,\n",
              " 'Ġpoints': 332,\n",
              " 'Ġgroup': 333,\n",
              " 'Ġschool': 334,\n",
              " 'Ġinformation': 335,\n",
              " 'Ġ2016': 336,\n",
              " 'al': 337,\n",
              " 'r': 338,\n",
              " 'Ġwin': 339,\n",
              " 'Ġnews': 340,\n",
              " 'Ġused': 341,\n",
              " 'Ġput': 342,\n",
              " 'Ġcity': 343,\n",
              " 'ĠJ': 344,\n",
              " 'ĠThere': 345,\n",
              " 'Ġnumber': 346,\n",
              " 'C': 347,\n",
              " \"'ve\": 348,\n",
              " 'Ġeach': 349,\n",
              " 'Ġtoo': 350,\n",
              " 'Ġwon': 351,\n",
              " 'ly': 352,\n",
              " 'Ġmonth': 353,\n",
              " 'is': 354,\n",
              " 'Ġadded': 355,\n",
              " 'Ġlook': 356,\n",
              " 'Ġbetter': 357,\n",
              " 'Ġevery': 358,\n",
              " 'Ġ&': 359,\n",
              " 'Ġdays': 360,\n",
              " 'Ġ9': 361,\n",
              " 'Ġtook': 362,\n",
              " 'Ġnight': 363,\n",
              " 'Ġe': 364,\n",
              " 'Ġ11': 365,\n",
              " 'os': 366,\n",
              " 'Ġfew': 367,\n",
              " 'or': 368,\n",
              " 'ĠNorth': 369,\n",
              " 'ĠYou': 370,\n",
              " 'Ġthird': 371,\n",
              " 'Ġgreat': 372,\n",
              " 'Ġcalled': 373,\n",
              " 'ĠOn': 374,\n",
              " 'Ġpast': 375,\n",
              " 'Ġcame': 376,\n",
              " 'Ġmonths': 377,\n",
              " 'ĠSaturday': 378,\n",
              " 'Ġ15': 379,\n",
              " 'Ġbig': 380,\n",
              " 'ĠE': 381,\n",
              " 'ĠUS': 382,\n",
              " 'Ġthings': 383,\n",
              " 'ĠO': 384,\n",
              " 'Ġd': 385,\n",
              " 'Ġstart': 386,\n",
              " 'B': 387,\n",
              " 'Ġstock': 388,\n",
              " 'Ġ30': 389,\n",
              " 'Ġwomen': 390,\n",
              " 'ĠSouth': 391,\n",
              " 'ĠMay': 392,\n",
              " 'Ġnever': 393,\n",
              " 'Ġpresident': 394,\n",
              " 'ĠSunday': 395,\n",
              " 'Ġwithout': 396,\n",
              " 'man': 397,\n",
              " '8': 398,\n",
              " 'Ġdidn': 399,\n",
              " 'Ġlocal': 400,\n",
              " '6': 401,\n",
              " 'Ġsomething': 402,\n",
              " 'Ġcase': 403,\n",
              " 'ĠAll': 404,\n",
              " 'it': 405,\n",
              " '7': 406,\n",
              " 'ĠSo': 407,\n",
              " 'Ġchildren': 408,\n",
              " 'Ġaway': 409,\n",
              " 'Ġlittle': 410,\n",
              " 'Ġsix': 411,\n",
              " 'ĠCity': 412,\n",
              " 'ĠCounty': 413,\n",
              " 'Ġdata': 414,\n",
              " 'at': 415,\n",
              " 'Ġalready': 416,\n",
              " 'd': 417,\n",
              " 'Ġmoney': 418,\n",
              " 'Ġearly': 419,\n",
              " 'Ġacross': 420,\n",
              " 'Ġexpected': 421,\n",
              " 'Ġrun': 422,\n",
              " 'Ġlater': 423,\n",
              " 'am': 424,\n",
              " 'Ġprice': 425,\n",
              " 'Ġgames': 426,\n",
              " 'ĠMr': 427,\n",
              " 'b': 428,\n",
              " 'Ġmight': 429,\n",
              " 'Ġdifferent': 430,\n",
              " 'Ġreported': 431,\n",
              " 'Ġdeal': 432,\n",
              " 'Ġmedia': 433,\n",
              " 'Ġgrowth': 434,\n",
              " 'Ġcommunity': 435,\n",
              " 'ĠChina': 436,\n",
              " \"'m\": 437,\n",
              " 'c': 438,\n",
              " 'Ġwent': 439,\n",
              " 'ĠNo': 440,\n",
              " 'Ġable': 441,\n",
              " 'Ġmaking': 442,\n",
              " 'Ġarea': 443,\n",
              " 'Ġfar': 444,\n",
              " 'Ġstatement': 445,\n",
              " 'ĠHouse': 446,\n",
              " 'Ġworking': 447,\n",
              " 'M': 448,\n",
              " 'Ġk': 449,\n",
              " 'Ġseen': 450,\n",
              " 'Ġcompanies': 451,\n",
              " 'Ġtoday': 452,\n",
              " 'Ġmembers': 453,\n",
              " 'Ġuntil': 454,\n",
              " 'Ġfull': 455,\n",
              " 'Ġagain': 456,\n",
              " 'Ġhalf': 457,\n",
              " 'Ġshare': 458,\n",
              " 'le': 459,\n",
              " 'Ġalways': 460,\n",
              " 'Ġcourt': 461,\n",
              " 'l': 462,\n",
              " 'and': 463,\n",
              " 'Ġchange': 464,\n",
              " 'Ġfind': 465,\n",
              " '9': 466,\n",
              " 'Ġsystem': 467,\n",
              " 'ĠV': 468,\n",
              " 'ĠYork': 469,\n",
              " 'ĠAmerican': 470,\n",
              " 'Ġhead': 471,\n",
              " 'Ġplayers': 472,\n",
              " 'Ġdoes': 473,\n",
              " 'Ġhealth': 474,\n",
              " 'Ġm': 475,\n",
              " 'Ġpower': 476,\n",
              " 'Ġpoint': 477,\n",
              " 'Ġhit': 478,\n",
              " 'Ġ.': 479,\n",
              " 'Ġ--': 480,\n",
              " 'Ġfree': 481,\n",
              " '.,': 482,\n",
              " 'Ġlead': 483,\n",
              " 'Ġseveral': 484,\n",
              " 'Ġrecent': 485,\n",
              " 'Ġcall': 486,\n",
              " 'N': 487,\n",
              " 'Ġlaw': 488,\n",
              " 'Ġkeep': 489,\n",
              " 'Ġopen': 490,\n",
              " 'ĠNews': 491,\n",
              " 'Ġgive': 492,\n",
              " 'ia': 493,\n",
              " 'ĠMarch': 494,\n",
              " 'D': 495,\n",
              " 'ĠNational': 496,\n",
              " 'ĠAt': 497,\n",
              " 'Ġtimes': 498,\n",
              " 'Ġfuture': 499,\n",
              " 'R': 500,\n",
              " 'Ġ14': 501,\n",
              " 'ĠJune': 502,\n",
              " 'Ġofficials': 503,\n",
              " 'Ġ18': 504,\n",
              " 'Ġimportant': 505,\n",
              " 'f': 506,\n",
              " 'Ġfinal': 507,\n",
              " 'Ġ13': 508,\n",
              " 'ĠOne': 509,\n",
              " 'P': 510,\n",
              " 'Ġfollowing': 511,\n",
              " 'Ġcar': 512,\n",
              " 'Ġleast': 513,\n",
              " 'Ġwater': 514,\n",
              " 'Ġevent': 515,\n",
              " 'Ġline': 516,\n",
              " 'Ġmove': 517,\n",
              " 'Ġservices': 518,\n",
              " 'Ġhaving': 519,\n",
              " 'ĠWhen': 520,\n",
              " 'Ġstudents': 521,\n",
              " 'ĠPolice': 522,\n",
              " 'el': 523,\n",
              " 'Ġam': 524,\n",
              " 'ĠZ': 525,\n",
              " 'Ġside': 526,\n",
              " 'Ġstory': 527,\n",
              " 'Ġdue': 528,\n",
              " 'Ġmeeting': 529,\n",
              " 'K': 530,\n",
              " 'Ġmust': 531,\n",
              " 'ĠStates': 532,\n",
              " 'Ġlikely': 533,\n",
              " 'G': 534,\n",
              " 'Ġcontinue': 535,\n",
              " 'Ġago': 536,\n",
              " 'Ġparty': 537,\n",
              " 'Ġmajor': 538,\n",
              " 'Ġindustry': 539,\n",
              " 'Ġless': 540,\n",
              " '30': 541,\n",
              " 'Ġun': 542,\n",
              " 'Ġhard': 543,\n",
              " 'Ġservice': 544,\n",
              " 'Ġ16': 545,\n",
              " 'Ġlooking': 546,\n",
              " 'Ġheld': 547,\n",
              " 've': 548,\n",
              " 'Ġwhether': 549,\n",
              " 'ĠJuly': 550,\n",
              " 'Ġtaken': 551,\n",
              " 'Ġalong': 552,\n",
              " 'Ġasked': 553,\n",
              " 'Ġstarted': 554,\n",
              " 'Ġbecome': 555,\n",
              " 'Ġforward': 556,\n",
              " 'Ġresearch': 557,\n",
              " 'Ġoffice': 558,\n",
              " 'Ġpolitical': 559,\n",
              " 'to': 560,\n",
              " 'Ġtogether': 561,\n",
              " 'Ġgetting': 562,\n",
              " 'Ġplan': 563,\n",
              " 'Ġ25': 564,\n",
              " 'T': 565,\n",
              " 'Ġamong': 566,\n",
              " 'Ġcoming': 567,\n",
              " 'Ġdecision': 568,\n",
              " 'Ġvideo': 569,\n",
              " 'Ġ2015': 570,\n",
              " 'g': 571,\n",
              " 'ĠAfter': 572,\n",
              " 'Ġsecurity': 573,\n",
              " 'L': 574,\n",
              " 'Ġcare': 575,\n",
              " 'Ġgiven': 576,\n",
              " 'Ġavailable': 577,\n",
              " 'âĢĶ': 578,\n",
              " 'Ġs': 579,\n",
              " 'ĠWest': 580,\n",
              " \"'ll\": 581,\n",
              " 'Ġpay': 582,\n",
              " 'Ġnear': 583,\n",
              " 'Ġsaying': 584,\n",
              " 'Ġannounced': 585,\n",
              " 'Ġprogram': 586,\n",
              " 'ĠApril': 587,\n",
              " 'Ġreal': 588,\n",
              " 'ĠUniversity': 589,\n",
              " 'ĠWith': 590,\n",
              " 'AP': 591,\n",
              " 'Ġsocial': 592,\n",
              " 'Ġclose': 593,\n",
              " 'et': 594,\n",
              " 'Ġcurrent': 595,\n",
              " 'Ġwhy': 596,\n",
              " 'F': 597,\n",
              " 'ĠTo': 598,\n",
              " 'ĠTwitter': 599,\n",
              " 'Ġthough': 600,\n",
              " 'Ġ17': 601,\n",
              " 'Ġtaking': 602,\n",
              " 'ĠInc': 603,\n",
              " 'Ġmen': 604,\n",
              " 'w': 605,\n",
              " 'Ġcomes': 606,\n",
              " 'ley': 607,\n",
              " 'Ġdoing': 608,\n",
              " 'Ġprocess': 609,\n",
              " 'ĠJohn': 610,\n",
              " 'ch': 611,\n",
              " '00': 612,\n",
              " 'Ġfinancial': 613,\n",
              " 'Ġlow': 614,\n",
              " 'Ġenough': 615,\n",
              " 'ĠWhile': 616,\n",
              " 'Ġfurther': 617,\n",
              " 'Ġpost': 618,\n",
              " 'Ġfeel': 619,\n",
              " 'st': 620,\n",
              " 'Ġperson': 621,\n",
              " 'ĠFacebook': 622,\n",
              " 'ĠWorld': 623,\n",
              " 'Ġwithin': 624,\n",
              " 'ad': 625,\n",
              " 'Ġdone': 626,\n",
              " 'the': 627,\n",
              " 'Ġlate': 628,\n",
              " 'Ġtax': 629,\n",
              " 'Ġdoesn': 630,\n",
              " 'Ġthing': 631,\n",
              " 'Ġnational': 632,\n",
              " 'Ġjob': 633,\n",
              " 'Ġusing': 634,\n",
              " 'ĠHowever': 635,\n",
              " 'ic': 636,\n",
              " 'Ġcampaign': 637,\n",
              " 'Ġrecord': 638,\n",
              " 'Ġbehind': 639,\n",
              " '://': 640,\n",
              " 'ĠDepartment': 641,\n",
              " 'p': 642,\n",
              " 'Ġothers': 643,\n",
              " 'ĠJanuary': 644,\n",
              " 'Ġorder': 645,\n",
              " 'Ġ[': 646,\n",
              " 'Ġsales': 647,\n",
              " 'Ġyet': 648,\n",
              " 'Ä': 649,\n",
              " 'Ġsmall': 650,\n",
              " 'Ġseries': 651,\n",
              " 'Ġface': 652,\n",
              " 'ĠWhat': 653,\n",
              " 'Ġ50': 654,\n",
              " 'Ġever': 655,\n",
              " 'Ġearlier': 656,\n",
              " 'Ġlove': 657,\n",
              " 'up': 658,\n",
              " 'Ġrights': 659,\n",
              " 'ĠAn': 660,\n",
              " 'ist': 661,\n",
              " 'Ġmorning': 662,\n",
              " 'ĠWashington': 663,\n",
              " 'Ġyoung': 664,\n",
              " 'Ġlatest': 665,\n",
              " 'ĠIndia': 666,\n",
              " 'Ġtrying': 667,\n",
              " 'Ġfire': 668,\n",
              " 'Ġled': 669,\n",
              " 'Ġstrong': 670,\n",
              " 'Ġreturn': 671,\n",
              " 'Ġlevel': 672,\n",
              " 'O': 673,\n",
              " 'Ġaverage': 674,\n",
              " 'Ġperiod': 675,\n",
              " 'Ġexperience': 676,\n",
              " 'ak': 677,\n",
              " 'Ġpossible': 678,\n",
              " 'Ġbelieve': 679,\n",
              " 'Ġinclude': 680,\n",
              " 'Ġoil': 681,\n",
              " 'Ġrecently': 682,\n",
              " 'Ġonce': 683,\n",
              " 'Ġknown': 684,\n",
              " 'Ġlost': 685,\n",
              " 'Ġsure': 686,\n",
              " 'us': 687,\n",
              " 'Ġweeks': 688,\n",
              " 'Ġfood': 689,\n",
              " 'Ġreports': 690,\n",
              " 'Ġrating': 691,\n",
              " 'ĠMinister': 692,\n",
              " 'Ġwoman': 693,\n",
              " 'Ġprovide': 694,\n",
              " 'Ġproject': 695,\n",
              " 'Ġissue': 696,\n",
              " 'Ġlive': 697,\n",
              " '10': 698,\n",
              " 'Ġclear': 699,\n",
              " 'he': 700,\n",
              " 'Ġcost': 701,\n",
              " 'Ġplayed': 702,\n",
              " 'Ġreleased': 703,\n",
              " 'Ġcoach': 704,\n",
              " 'v': 705,\n",
              " 'Ġ24': 706,\n",
              " 'Ġseven': 707,\n",
              " 'Ġplans': 708,\n",
              " 'Ġdevelopment': 709,\n",
              " 'ur': 710,\n",
              " 'ĺ': 711,\n",
              " 'Ġincrease': 712,\n",
              " 'This': 713,\n",
              " 'Ġpolicy': 714,\n",
              " 'Ġcent': 715,\n",
              " 'Ġbased': 716,\n",
              " 'E': 717,\n",
              " 'il': 718,\n",
              " 'ĠDecember': 719,\n",
              " 'Ġglobal': 720,\n",
              " 'Ġtrade': 721,\n",
              " 'Ġhours': 722,\n",
              " 'Ġhigher': 723,\n",
              " 'Ġgoal': 724,\n",
              " 'H': 725,\n",
              " 'ĠAl': 726,\n",
              " 'Ġ100': 727,\n",
              " 'Ġminutes': 728,\n",
              " 'Ġelection': 729,\n",
              " 'ĠAmerica': 730,\n",
              " 'Ġrate': 731,\n",
              " 'ĠCh': 732,\n",
              " 'Ġ21': 733,\n",
              " '...': 734,\n",
              " 'ĠWhite': 735,\n",
              " 'Ġdirector': 736,\n",
              " 'Ġposition': 737,\n",
              " 'Ġshot': 738,\n",
              " 'Ġlarge': 739,\n",
              " 'Ġc': 740,\n",
              " 'Ġb': 741,\n",
              " ']': 742,\n",
              " 'Ġissues': 743,\n",
              " 'Ġdeath': 744,\n",
              " 'Ġbuilding': 745,\n",
              " 'Ġtotal': 746,\n",
              " 'Ġoften': 747,\n",
              " 'Ġv': 748,\n",
              " 'Ġcountries': 749,\n",
              " 'Ġhistory': 750,\n",
              " 'Ġoutside': 751,\n",
              " 'Ġfederal': 752,\n",
              " 'Ġ19': 753,\n",
              " 'Ġfact': 754,\n",
              " 'ĠHigh': 755,\n",
              " 'Ġcareer': 756,\n",
              " 'im': 757,\n",
              " 'Ġinternational': 758,\n",
              " 'ĠNovember': 759,\n",
              " 'Ġfront': 760,\n",
              " 'Ġkind': 761,\n",
              " 'Ġkey': 762,\n",
              " 'ra': 763,\n",
              " 'ĠSan': 764,\n",
              " 'Ġshort': 765,\n",
              " 'Ġname': 766,\n",
              " 'ĠAccording': 767,\n",
              " 'Ġcourse': 768,\n",
              " 'Ġre': 769,\n",
              " 'Ġwanted': 770,\n",
              " 'W': 771,\n",
              " 'ĠSeptember': 772,\n",
              " 'Ġinterest': 773,\n",
              " 'Ġrole': 774,\n",
              " 'Ġresults': 775,\n",
              " 'Ġeconomic': 776,\n",
              " 'Ġ2014': 777,\n",
              " 'Ġchance': 778,\n",
              " 'ĠOctober': 779,\n",
              " 'Ġspecial': 780,\n",
              " 'Ġofficial': 781,\n",
              " 'Ġneeds': 782,\n",
              " 'um': 783,\n",
              " 'Ġl': 784,\n",
              " 'Ġproducts': 785,\n",
              " 'Ġnon': 786,\n",
              " 'Ġ@': 787,\n",
              " 'ĠBank': 788,\n",
              " 'Ġahead': 789,\n",
              " 'Ġhouse': 790,\n",
              " 'U': 791,\n",
              " 'Ġboard': 792,\n",
              " 'Ġold': 793,\n",
              " 'Ġsaw': 794,\n",
              " 'Ġlower': 795,\n",
              " 'ĠEuropean': 796,\n",
              " 'Ġcontrol': 797,\n",
              " 'ĠRussia': 798,\n",
              " 'Ġeight': 799,\n",
              " 'Ġrelease': 800,\n",
              " 'Ġpotential': 801,\n",
              " 'Ġthought': 802,\n",
              " 'Ġinvestigation': 803,\n",
              " 'Ġonline': 804,\n",
              " 'based': 805,\n",
              " 'Ġtechnology': 806,\n",
              " 'ĠDonald': 807,\n",
              " 'id': 808,\n",
              " 'Ġbody': 809,\n",
              " 'Ġrisk': 810,\n",
              " 'ian': 811,\n",
              " 'Ġcapital': 812,\n",
              " 'Ġstaff': 813,\n",
              " 'Ġaction': 814,\n",
              " 'ĠLeague': 815,\n",
              " 'Ġplaying': 816,\n",
              " 'Ġmakes': 817,\n",
              " 'Ġalmost': 818,\n",
              " 'Ġperformance': 819,\n",
              " 'Ġ22': 820,\n",
              " 'Ġg': 821,\n",
              " 'Ġfilm': 822,\n",
              " 'Ġnearly': 823,\n",
              " 'ĠCenter': 824,\n",
              " 'Ġvisit': 825,\n",
              " 'ĠGroup': 826,\n",
              " 'Ġbank': 827,\n",
              " 'Ġbit': 828,\n",
              " 'Ġreceived': 829,\n",
              " 'ĠAugust': 830,\n",
              " 'Ġmilitary': 831,\n",
              " 'ĠHis': 832,\n",
              " 'ine': 833,\n",
              " 'Ġchief': 834,\n",
              " 'ĠSchool': 835,\n",
              " 'Ġbring': 836,\n",
              " 'ĠCourt': 837,\n",
              " 'Ġ(@': 838,\n",
              " 'Ġmeans': 839,\n",
              " 'ĠSh': 840,\n",
              " 'Ġfans': 841,\n",
              " 'Ġse': 842,\n",
              " 'Ġ40': 843,\n",
              " '20': 844,\n",
              " '\".': 845,\n",
              " 'V': 846,\n",
              " 'Ġcut': 847,\n",
              " 'Ġkilled': 848,\n",
              " 'Ġ#': 849,\n",
              " 'Ġprices': 850,\n",
              " 'Ġgave': 851,\n",
              " 'ĠStreet': 852,\n",
              " 'ir': 853,\n",
              " 'ĠY': 854,\n",
              " 'Ġcurrently': 855,\n",
              " 'Ġf': 856,\n",
              " 'ay': 857,\n",
              " 'ne': 858,\n",
              " 'te': 859,\n",
              " 'Ġtry': 860,\n",
              " 'ĠPark': 861,\n",
              " 'ĥ': 862,\n",
              " 'J': 863,\n",
              " 'Ġquestion': 864,\n",
              " 'Ġhand': 865,\n",
              " 'Ġeconomy': 866,\n",
              " 'Ġinvestors': 867,\n",
              " 'able': 868,\n",
              " 'Ġplayer': 869,\n",
              " 'ĠBy': 870,\n",
              " 'ĠDavid': 871,\n",
              " 'Ġloss': 872,\n",
              " 'ab': 873,\n",
              " 'Ġbelow': 874,\n",
              " 'Ġwrote': 875,\n",
              " 'co': 876,\n",
              " 'ate': 877,\n",
              " 'Ġrunning': 878,\n",
              " 'un': 879,\n",
              " 'Ġbegan': 880,\n",
              " 'Ġsingle': 881,\n",
              " 'Ġfield': 882,\n",
              " 'Ġ23': 883,\n",
              " 'Ġleader': 884,\n",
              " 'Ġw': 885,\n",
              " 'ĠCalifornia': 886,\n",
              " 'Ġfourth': 887,\n",
              " 'Ġactually': 888,\n",
              " 'Ġlist': 889,\n",
              " 'll': 890,\n",
              " 'Ġcouple': 891,\n",
              " 'Ġstudy': 892,\n",
              " 'Ġteams': 893,\n",
              " 'He': 894,\n",
              " 'ah': 895,\n",
              " 'ĠCanada': 896,\n",
              " 'Ġla': 897,\n",
              " 'Ġresult': 898,\n",
              " 'Ġaccess': 899,\n",
              " 'Ġvote': 900,\n",
              " 'ĠMore': 901,\n",
              " 'ĠFebruary': 902,\n",
              " 'Ġrevenue': 903,\n",
              " 'Ġoffer': 904,\n",
              " 'Ġlet': 905,\n",
              " 'ier': 906,\n",
              " 'Ġbuy': 907,\n",
              " 'Ġattack': 908,\n",
              " 'Ġblack': 909,\n",
              " 'Ġr': 910,\n",
              " 'Ġareas': 911,\n",
              " 'Ġstop': 912,\n",
              " 'Ġimpact': 913,\n",
              " 'Ġmatch': 914,\n",
              " 'Ġinvestment': 915,\n",
              " 'Ġcustomers': 916,\n",
              " 'Ġleaders': 917,\n",
              " 'ies': 918,\n",
              " 'Ġmember': 919,\n",
              " 'Ġchild': 920,\n",
              " 'Ġroad': 921,\n",
              " 'ul': 922,\n",
              " 'Ġvalue': 923,\n",
              " 'Ġshows': 924,\n",
              " 'ĠDr': 925,\n",
              " 'ĠDe': 926,\n",
              " 'ant': 927,\n",
              " 'ĠLondon': 928,\n",
              " 'Ġroom': 929,\n",
              " 'Ġmusic': 930,\n",
              " 'Ġproduction': 931,\n",
              " 'Ġanything': 932,\n",
              " 'Ġfirm': 933,\n",
              " 'Ġbiggest': 934,\n",
              " 'Ġair': 935,\n",
              " 'Ġproblem': 936,\n",
              " 'Ġgeneral': 937,\n",
              " 'Ġwasn': 938,\n",
              " 'Ġi': 939,\n",
              " 'Ġprivate': 940,\n",
              " 'Ġespecially': 941,\n",
              " 'Ġadministration': 942,\n",
              " 'Ġadditional': 943,\n",
              " 'ĠCo': 944,\n",
              " 'Ġopportunity': 945,\n",
              " 'Ġhold': 946,\n",
              " '&': 947,\n",
              " 'Ġmatter': 948,\n",
              " 'Ġsenior': 949,\n",
              " 'Ġclub': 950,\n",
              " 'Ġsomeone': 951,\n",
              " 'ĠÃ': 952,\n",
              " 'ĠEast': 953,\n",
              " 'Ġ2019': 954,\n",
              " \".'\": 955,\n",
              " 'Ġneeded': 956,\n",
              " 'ĠJames': 957,\n",
              " 'time': 958,\n",
              " 'Ġhowever': 959,\n",
              " 'Ġeverything': 960,\n",
              " 'Ġeveryone': 961,\n",
              " 'Ġdied': 962,\n",
              " 'Ġinvolved': 963,\n",
              " 'Ġfriends': 964,\n",
              " 'Ġisn': 965,\n",
              " 'Ġworth': 966,\n",
              " 'ik': 967,\n",
              " 'ĠCup': 968,\n",
              " 'Ġshowed': 969,\n",
              " 'There': 970,\n",
              " 'Ġ28': 971,\n",
              " 'Ġmeet': 972,\n",
              " 'Ġ26': 973,\n",
              " 'Ġ27': 974,\n",
              " 'Y': 975,\n",
              " 'Ġregion': 976,\n",
              " 'ĠPress': 977,\n",
              " 'ĠNow': 978,\n",
              " 'Ġson': 979,\n",
              " 'Ġspace': 980,\n",
              " 'Ġleading': 981,\n",
              " 'Ġstates': 982,\n",
              " 'Ġweekend': 983,\n",
              " 'ĠÂ£': 984,\n",
              " 'Ġmother': 985,\n",
              " 'Ġprevious': 986,\n",
              " 'ĠUK': 987,\n",
              " 'ĠMichael': 988,\n",
              " 'Ġleave': 989,\n",
              " 'est': 990,\n",
              " 'em': 991,\n",
              " 'Ġz': 992,\n",
              " 'ĠSome': 993,\n",
              " 'ors': 994,\n",
              " 'out': 995,\n",
              " '15': 996,\n",
              " 'Ġwar': 997,\n",
              " 'Ġwebsite': 998,\n",
              " 'Ġstar': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_to_idx = tokenizer.get_vocab()\n",
        "tokens_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you notice that most tokens start with a \"weird\" character (Ġ)? In this particular tokenizer, this character indicates the space preceding a word. Tokens that do not start with Ġ are found either at the beginning of a sequence or as part of another word. Typical examples of the latter are the endings \"ed\" and \"ing\" which are tokens but not full words."
      ],
      "metadata": {
        "id": "4lFN-tcHYHwq"
      },
      "id": "4lFN-tcHYHwq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4243bc05",
      "metadata": {
        "id": "4243bc05",
        "outputId": "4f64bde3-c66e-4b6c-ba54-b62db340b21b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(196, 154, 129, 8338)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_to_idx['ed'], tokens_to_idx['ing'], tokens_to_idx['Ġonly'], tokens_to_idx['only']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try tokenizing a sentence that has a word with the \"ing\" ending:"
      ],
      "metadata": {
        "id": "HM1nCC5WYOhR"
      },
      "id": "HM1nCC5WYOhR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72f8d51",
      "metadata": {
        "id": "c72f8d51",
        "outputId": "57820a5c-8039-432e-a063-026b4ab81bf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'Ġam', 'Ġdissect', 'ing', 'Ġthis', ',', 'Ġam', 'ĠI', '?']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize('I am dissecting this, am I?')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word \"dissecting\" isn't used often enough to be taken as a whole word, so it's split into two components: \"Ġdissect\" which is a word on its own right, and the typical \"ing\" ending. Some words are more commonly used, though, and both versions may be considered full words by the tokenizer:"
      ],
      "metadata": {
        "id": "oyOWmSrgYiYe"
      },
      "id": "oyOWmSrgYiYe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b7615e",
      "metadata": {
        "id": "18b7615e",
        "outputId": "f96de648-98b7-4170-c359-b9baa53a95d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'Ġam', 'Ġplaying', 'Ġwith', 'Ġthe', 'Ġword', 'Ġplay', '.']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize('I am playing with the word play.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See? Even though \"play\" is a token in the vocabulary, \"playing\" is also a token (as opposed to being composed by the tokens \"play\" and \"ing\" separately)."
      ],
      "metadata": {
        "id": "Eixtft1UYmgA"
      },
      "id": "Eixtft1UYmgA"
    },
    {
      "cell_type": "markdown",
      "id": "bc22e9c0",
      "metadata": {
        "id": "bc22e9c0"
      },
      "source": [
        "#### 8.3.2.3 Max Length\n",
        "\n",
        "#### NLP: Tokenizers: Max Length\n",
        "The tokenizer will also truncate the input to the maximum length taken by the model which, in RoBERTa's case, is 512 tokens. Notice that this is different from the maximum length of a single sentence, which is two tokens shorter than the model's maximum length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b64cb8",
      "metadata": {
        "id": "b8b64cb8",
        "outputId": "54a36dd5-6a7d-4ce8-d2ec-8fe9e23ab3ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(510, 512)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.max_len_single_sentence, tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference is due to the special tokens that will be both prepended (beginning of sequence, or BOS, token) and appended (end of sequence, or EOS, token) to the sentence. Our sentences are quite short, so nothing will actually happen at this step. For the sake of illustrating the idea, we can force the tokenizer to truncate the input to a much shorter length, say, five tokens:"
      ],
      "metadata": {
        "id": "u-vu6ZtRY0cu"
      },
      "id": "u-vu6ZtRY0cu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dcce0b4",
      "metadata": {
        "id": "7dcce0b4",
        "outputId": "073fe9cb-88d4-4be1-fbf3-5a147aa24175"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 100, 524, 269, 2]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "truncated_token_ids = tokenizer(input_batch[0], truncation=True, max_length=5)['input_ids']\n",
        "truncated_token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what's left of the original message:"
      ],
      "metadata": {
        "id": "cn-6dy6QY92r"
      },
      "id": "cn-6dy6QY92r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8bcfbc",
      "metadata": {
        "id": "4c8bcfbc",
        "outputId": "da4fb272-83ba-4dc7-e29b-e568a05e7939"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>I am really</s>'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(truncated_token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually, only three tokens of the original sentence made it. The remaining two tokens (out of the five we're truncating the input to) are the subject of our next topic, special tokens."
      ],
      "metadata": {
        "id": "aod0rVFCZCoX"
      },
      "id": "aod0rVFCZCoX"
    },
    {
      "cell_type": "markdown",
      "id": "4fb4b0e6",
      "metadata": {
        "id": "4fb4b0e6"
      },
      "source": [
        "#### 8.3.2.4 Special Tokens\n",
        "\n",
        "### NLP: Tokenizers: Special Tokens\n",
        "There are many special tokens, like the token for unknown words (UNK), those words not present in our predefined vocabulary. Special tokens can be used to represent the start of a sequence (BOS), the end of a sequence (EOS), a separation between two sequences (SEP), or to simply pad (PAD) (or \"stuff\") a sequence to make it of a certain length. We can inspect all special tokens defined in the tokenizer using its special_tokens_map attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0960e9",
      "metadata": {
        "id": "aa0960e9",
        "outputId": "7508d515-18b3-481a-c3fd-7337bf621190"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bos_token': '<s>',\n",
              " 'eos_token': '</s>',\n",
              " 'unk_token': '<unk>',\n",
              " 'sep_token': '</s>',\n",
              " 'pad_token': '<pad>',\n",
              " 'cls_token': '<s>',\n",
              " 'mask_token': '<mask>'}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.special_tokens_map"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RoBERTa's tokenizer both prepends a \"start\" token to the beginning of a sequence and appends an \"end\" token to the end of the sequence:"
      ],
      "metadata": {
        "id": "50stuCFMZpee"
      },
      "id": "50stuCFMZpee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df78f82",
      "metadata": {
        "id": "8df78f82",
        "outputId": "d2056bf6-414c-4122-8be1-95f54e60faf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 100, 524, 269, 25896, 42, 768, 328, 2]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_ids = tokenizer.encode(input_batch[0], add_special_tokens=True)\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easier to see the added tokens by decoding the IDs back into text:"
      ],
      "metadata": {
        "id": "sEn4jbVLZuaO"
      },
      "id": "sEn4jbVLZuaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c240766",
      "metadata": {
        "id": "7c240766",
        "outputId": "9f254f98-2c05-438d-d338-c0c404c3ce7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>I am really liking this course!</s>'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " See? The sequence now starts with an <s> (id zero) token and ends with a </s> (id two) token.\n",
        "\n",
        "Calling the tokenizer itself (as opposed to one of its methods) does the whole thing at once, so our initial sentences are properly converted into sequences of token indices:"
      ],
      "metadata": {
        "id": "YE6T6q3SZzwm"
      },
      "id": "YE6T6q3SZzwm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a48ac7d",
      "metadata": {
        "id": "0a48ac7d",
        "outputId": "0272ebfc-30a1-4489-c395-20e0ffa29ee0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 100, 524, 269, 25896, 42, 768, 328, 2],\n",
              " [0, 713, 768, 16, 350, 6336, 328, 2]]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch = [\"I am really liking this course!\", \"This course is too complicated!\"]\n",
        "transformed = tokenizer(input_batch)['input_ids']\n",
        "transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5293abd6",
      "metadata": {
        "id": "5293abd6"
      },
      "source": [
        "### 8.3.3 NLP :- Inference\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
        "\n",
        "We still need to turn our input sequences into a PyTorch tensor. You may be wondering why the transformation didn't return tensors instead of plain Python lists. The answer is, the sequences may have different lengths, and that literally raises problems if you want to make a (single) tensor out of them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a572b840",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "a572b840",
        "outputId": "2693f8e9-a5d1-461f-bfba-6e1247e24a74"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "expected sequence of length 9 at dim 1 (got 8)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 9 at dim 1 (got 8)"
          ]
        }
      ],
      "source": [
        "torch.as_tensor(transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer lies in padding the shortest sequence, that is, appending one or more times another special token ([PAD]) to its end so its length matches the longest sentence in the batch. Padding is a common operation, both in Computer Vision and Natural Language Processing, and we'll get back to it in the chapters that follow. The tokenizer already has a special padding token, and we can easily retrieve its corresponding id through the tokenizer's pad_token_id attribute:"
      ],
      "metadata": {
        "id": "knz4vAoJaOKD"
      },
      "id": "knz4vAoJaOKD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f3ad6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14f3ad6e",
        "outputId": "fb774dde-a1b6-4cf0-f467-ffb2c6022752"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luckily, we don't have to manually add padding tokens to our inputs in order to make PyTorch tensors out of them. We only need to specify padding=True as argument of our tokenizer, thus yielding lists of equal sizes, and then specify return_tensors='pt' (pt stands for PyTorch) so the tokenizer returns tensors instead of Python lists:"
      ],
      "metadata": {
        "id": "oDp8q5qqbK1f"
      },
      "id": "oDp8q5qqbK1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f78ad3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f78ad3c",
        "outputId": "68d742ee-2bcc-428b-9f87-f7ecbc475477"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[    0,   100,   524,   269, 25896,    42,   768,   328,     2],\n",
              "         [    0,   713,   768,    16,   350,  6336,   328,     2,     1]]),\n",
              " torch.Size([2, 9]))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch = [\"I am really liking this course!\", \"This course is too complicated!\"]\n",
        "model_input = tokenizer(input_batch, padding=True, return_tensors='pt')['input_ids']\n",
        "model_input, model_input.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're set: our input has two sentences of the same length, nine tokens each.\n",
        "\n",
        "Let's see what kind of output our RoBERTa model returns:"
      ],
      "metadata": {
        "id": "zQCdvVmubOhL"
      },
      "id": "zQCdvVmubOhL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b73a50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73b73a50",
        "outputId": "f0aafd37-f8f2-4fb8-d0f9-51d424cc1fa2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 9, 768])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "output = model(model_input)\n",
        "output.last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two sentences, nine tokens each, each token represented by an array of 768 numerical values. In \"Building Your First Dataset\" we converted categorical values (such as \"Gasoline\", \"Diesel\", etc.) to arrays of numerical values. Do you remember what those were called? Those were embeddings. And so are these! RoBERTa produced embeddings for each token in our two sentences. These tokens were learned by the model during pretraining, and they are more than regular embeddings, they are contextual embeddings.\n",
        "\n",
        "We already saw regular embeddings, they work like a big lookup table. Imagine that you have every word from Webster's dictionary in your table, each row corresponding to a word, and an array assigned to every word. If you want the values for the word \"bank\", you look it up, and there you have them! But words aren't as straightforward as we'd like them to be: the word \"bank\" may stand for a financial establishment or the land alongside a river or lake. Regular embeddings do not account for these differences in meaning, but contextual embeddings do. The array corresponding to the word \"bank\" may differ depending on the context it is being used in. That's what a language model such as RoBERTa produces: contextual embeddings. We'll see all these in much more detail in \"Contextual Word Embeddings with Transformers\".\n",
        "\n",
        "In the meantime, let's see how these contextual embeddings can be used in a \"head\"."
      ],
      "metadata": {
        "id": "0ERuD0upbopa"
      },
      "id": "0ERuD0upbopa"
    },
    {
      "cell_type": "markdown",
      "id": "1b32f0cb",
      "metadata": {
        "id": "1b32f0cb"
      },
      "source": [
        "### 8.3.4 NLP : Attaching a Head\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"head\" in this context is nothing but a layer, sequence of layers, or a small model, that uses the embeddings as input and converts them into an actual prediction (e.g. \"positive\" or \"negative\" labels for sentiment analysis).\n",
        "\n",
        "Let's create a RoBERTa model with a classifier head that does just that - it takes a sequence of contextual embeddings of 768 dimensions each, and produces two logits, one for each class. There are many RoBERTa models tailored for different downstream tasks, such as sequence classification (the one we're using), token classification, question answering, multiple choice, etc.\n",
        "\n",
        "In sequence classification, we can specify the number of classes (or distinct labels) pertaining to our task using the num_labels argument from the from_pretrained() method:"
      ],
      "metadata": {
        "id": "AR3Yp6pyb6Xz"
      },
      "id": "AR3Yp6pyb6Xz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66bcbc7e",
      "metadata": {
        "id": "66bcbc7e",
        "outputId": "3b040d8d-377c-4092-f3b9-5a8a8cc322ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "torch.manual_seed(11)\n",
        "model_with_head = RobertaForSequenceClassification.from_pretrained(repo_id, num_labels=2)\n",
        "model_with_head"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, RoBERTa's classifier head is quite a simple model."
      ],
      "metadata": {
        "id": "BDU0wy6qcH9M"
      },
      "id": "BDU0wy6qcH9M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ad57a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1ad57a1",
        "outputId": "1a7e9047-68b3-431d-fedb-83e5caf6081e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaClassificationHead(\n",
              "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier_head = model_with_head.classifier\n",
        "classifier_head"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, our model is (almost) ready to perform binary classification. What if we give our model the same input as before?"
      ],
      "metadata": {
        "id": "NpmbmU4ocNr0"
      },
      "id": "NpmbmU4ocNr0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2662456",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2662456",
        "outputId": "e2a73f76-2561-4185-fd52-703ba8e27cfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(SequenceClassifierOutput(loss=None, logits=tensor([[-0.1540,  0.0212],\n",
              "         [-0.1685,  0.0220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None),\n",
              " torch.Size([2, 2]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_with_head.eval()\n",
        "output = model_with_head(model_input)\n",
        "output, output.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two sentences, two logits each.\n",
        "\n",
        "Notice that the [output](https://huggingface.co/docs/transformers/en/main_classes/output) of a Hugging Face model is, unless otherwise specified, a dictionary-like structure, SequenceClassifierOutput in this case. The typical structure returns:\n",
        "\n",
        "the loss (if in training mode, when labels are provided)\n",
        "logits\n",
        "hidden_states (the contextual embeddings we discussed in the previous section, returned if output_hidden_states=True)\n",
        "attentions (attention scores, returned if output_attentions=True)\n",
        "The returned logits are just random because our classifier head wasn't trained yet. And that's your task in the next lab!"
      ],
      "metadata": {
        "id": "VcfCnLa2cUD9"
      },
      "id": "VcfCnLa2cUD9"
    },
    {
      "cell_type": "markdown",
      "id": "55175b94",
      "metadata": {
        "id": "55175b94"
      },
      "source": [
        "### 8.3.5 nlp:  Logits and Loss Functions\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have only used, but not trained, a classifier. The classifier we used was trained to classify images into one thousand possible categories. We saw that the model produces one logit for each possible category, and that these logits may be converted to probabilities using a softmax function.\n",
        "\n",
        "Now, we'll be training RoBERTa's classifier head in the upcoming lab. What do we know so far? The classifier head, as seen in the previous section, produces two logits for each input, since its purpose is to classify sentences in two categories: \"positive\" and \"negative\" sentiment.\n",
        "\n",
        "A classification task calls for a different loss function, and the choice depends on a couple of factors:\n",
        "\n",
        "Does it output logits?\n",
        "How many outputs?\n",
        "Classifiers usually output logits, not probabilities nor log-probabilities (that's just the logarithm of the probabilities). If the last layer of the model is a regular linear layer, it's safe to assume it is indeed producing logits. Some models may have a sigmoid or logsoftmax layer at the end and, if that's the case, they will be producing probabilities or log-probabilities, respectively. For now, let's focus on the easier variety of models that simply produce logits.\n",
        "\n",
        "Let's focus on the second question now: we saw models producing 1,000 logits, and our RoBERTa classifier head produces two logits. In both cases, the appropriate loss function is the cross-entropy loss (nn.CrossEntropyLoss), and the task itself is deemed a multiclass classification task (even if we only have two possible categories, as in our case)."
      ],
      "metadata": {
        "id": "8nxtl9vYctdl"
      },
      "id": "8nxtl9vYctdl"
    },
    {
      "cell_type": "markdown",
      "id": "896ae7f9",
      "metadata": {
        "id": "896ae7f9"
      },
      "source": [
        "#### 8.3.5.1 One Logit or Two Logits?\n",
        "\n",
        "####NLP: Logits and Loss Functions: One Logit or Two Logits?\n",
        "It is common to refer to classification tasks as either binary (two categories) or multiclass classification (more than two categories). So, the fact that we have only \"positive\" and \"negative\" categories, but the loss function we should be using, cross-entropy loss, is typical of multiclass classification tasks is surely confusing.\n",
        "\n",
        "The difference boils down to the number of logits being produced by the model. As it turns out, it is possible to achieve binary classification using either a single logit or two logits. Let's quickly go over the difference between the two approaches.\n",
        "\n",
        "A single logit answers a single \"yes/no\" question, such as, is the sentence \"positive\"? It is typical to assume \"yes\" corresponds to high values of logits (e.g. positive values) and \"no\" corresponds to low values of logits (e.g. negative values). So, you can use a threshold (typically zero) to split the results in two, and each split corresponds to a category (values above zero mean \"positive\", values below zero mean \"negative\"). We'll be using a single logit to perform true binary classification in the next chapter, and then we'll go into more detail about this approach.\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/one_logit.png)\n",
        "###Binary classification with a single logit\n",
        "Multiple logits answer a series of questions or, better yet, they assign scores to each question. In our case, there are two logits or questions:\n",
        "\n",
        "- \"Is this a positive sentence?\"\n",
        "- \"Is this a negative sentence?\"\n",
        "There can be only one answer, and the logit with the highest value (score) wins, as we've already seen a few times. Notice that in this case, the \"winning\" logit may even be a negative value, it just needs to be higher than the others.\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/two_logits.png)\n",
        "###Binary classification with two logits\n",
        "\n",
        "Sure, if there are only two questions, this approach is redundant and it could be simplified to a \"yes/no\" question using a single logit. So, why haven't we done that instead?\n",
        "\n",
        "Pretrained models are usually trained to classify their inputs into multiple categories, and as we'll see later on, there are also utilities to easily map the \"winning logit\" to its human-readable category name, and they expect multiple logits, even for something that should have been a binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8afea41",
      "metadata": {
        "id": "e8afea41"
      },
      "source": [
        "#### 8.3.5.2 Cross-Entropy Loss\n",
        "\n",
        "So, in a nutshell, if your classifier head produces more than one logit (and it is OK to use two for a binary classification), you must use the cross-entropy loss.\n",
        "\n",
        "The table below may help you organize a little bit the ideas presented in this section (right now, we're going to use the last column only):\n",
        "\n",
        "|                         | BCE Loss               | BCE With Logits Loss     | NLL Loss                    | Cross-Entropy Loss   \n",
        "| --- | --- | --- | --- | --- |\n",
        "|     Classification      | binary                | binary                | multiclass / binary                | multiclass / binary\n",
        "| Model output (each data point) | probability           | logit                 | array of two or more log probabilities | array of two or more logits    \n",
        "| Label (each data point) | float (0.0 or 1.0)    | float (0.0 or 1.0)    | long (class index)         | long (class index)\n",
        "|   Model's last layer    | Sigmoid               | Linear                | LogSoftmax                 | Linear              "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027c7c12",
      "metadata": {
        "id": "027c7c12"
      },
      "source": [
        "#### 8.3.5.3 Losses in Hugging Face Models\n",
        "\n",
        "####NLP: Logits and Loss Functions: Losses in Hugging Face Models\n",
        "In the previous section, \"Attaching a Head\", we've seen that HF models return a dictionary-like structure that may return the loss if the model is in training mode. While it's perfectly possible to compute the loss manually using the returned logits as we've been doing so far, it may be convenient to use the loss values computed automatically by the model since it takes into consideration the task at hand (sequence classification with two labels, in our case).\n",
        "\n",
        "Let's go over a quick example to illustrate how it works. First, let's create some labels for our two sentences:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734f2643",
      "metadata": {
        "id": "734f2643"
      },
      "outputs": [],
      "source": [
        "input_batch = [\"I am really liking this course!\", \"This course is too complicated!\"]\n",
        "model_input = tokenizer(input_batch, padding=True, return_tensors='pt')['input_ids']\n",
        "labels = torch.as_tensor([1, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before, we forward the inputs to the model while in evaluation mode because we were interested either in the hidden states (the contextual embeddings) or in the logits (predictions). Now, we're setting it to training model, so it will return the loss information:"
      ],
      "metadata": {
        "id": "ldWuA39eeGNb"
      },
      "id": "ldWuA39eeGNb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f417af6c",
      "metadata": {
        "id": "f417af6c",
        "outputId": "b903d8b4-c043-482f-c814-964015ea459c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=tensor(0.6711, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0871,  0.1107],\n",
              "        [ 0.0525, -0.0134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_with_head.train()\n",
        "output = model_with_head(model_input, labels=labels)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In training model, it still returns the logits, so we may compute the loss ourselves if we want to:"
      ],
      "metadata": {
        "id": "wnPV-kekeLFs"
      },
      "id": "wnPV-kekeLFs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2956a18",
      "metadata": {
        "id": "d2956a18",
        "outputId": "5873deaf-5b84-490d-a143-9f3db7c497cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.6711, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss = loss_fn(output.logits, labels)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got matching values. After all, the model is using the appropriate loss function for the task at hand. If you run these cells multiple times, you may get different values for the loss, though. Remember, in training mode, some layers, such as dropout, won't behave deterministically."
      ],
      "metadata": {
        "id": "1nBE2hREeO8A"
      },
      "id": "1nBE2hREeO8A"
    },
    {
      "cell_type": "markdown",
      "id": "19dbce34",
      "metadata": {
        "id": "19dbce34"
      },
      "source": [
        "## 8.4 TensorBoard\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step4.png)\n",
        "\n",
        "So far, we haven't logged or inspected our losses in real time. Why bother, if it takes only a minute to train the model? This time is different, though: fine-tuning RoBERTa on more than 67,000 data points, even for a single epoch, will take about 15 min or so in Google Colab. So, let's use a convenient tool to see how our loss is doing as training progresses.\n",
        "\n",
        "Yes, TensorBoard is that good! So good that we’ll be using a tool from the competing framework, TensorFlow. Jokes aside, TensorBoard is a very useful tool, and PyTorch provides classes and methods so that we can integrate it with our model.\n",
        "\n",
        "First, we need to load TensorBoard’s extension for Jupyter. It is possible to run some special commands inside Jupyter Notebooks using a `%` characters at the start of a line, they are built-in [magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html). A magic is a kind of shortcut that extends a notebook's capabilities. Once it is loaded, we can run TensorBoard using the newly available magic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0c3a45",
      "metadata": {
        "id": "6a0c3a45"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The magic above tells TensorBoard to look for logs inside the folder specified by the logdir argument: runs. So, there must be a runs folder in the same location as the notebook you’re using to train the model.\n",
        "\n",
        "Initially, it looks like this:"
      ],
      "metadata": {
        "id": "eROw6i7zeoCQ"
      },
      "id": "eROw6i7zeoCQ"
    },
    {
      "cell_type": "markdown",
      "id": "b4e710b6",
      "metadata": {
        "id": "b4e710b6"
      },
      "source": [
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/empty_tensorboard.png)\n",
        "####Empty TensorBoard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is rather uninteresting unless some data is actually sent there so we can visualize it.\n",
        "\n",
        "It all starts with the creation of a SummaryWriter: since we told TensorBoard to look for logs inside the runs folder, it makes sense to actually log to that folder. Moreover, to be able to distinguish between different experiments or models, we should also specify a sub-folder: test."
      ],
      "metadata": {
        "id": "g87L97zbev9M"
      },
      "id": "g87L97zbev9M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09083010",
      "metadata": {
        "id": "09083010"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('runs/test')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about sending the loss values to TensorBoard? We can use the add_scalars() method to send multiple scalar values at once; it needs three arguments:\n",
        "\n",
        "main_tag: the parent name of the tags, or the \"group tag,\" if you will\n",
        "tag_scalar_dict: the dictionary containing the key: value pairs for the scalars you want to keep track of (for example, training and validation losses)\n",
        "global_step: step value; that is, the index you’re associating with the values you’re sending in the dictionary; the index of the mini-batch comes to mind in our case, as losses are computed for each mini-batch\n",
        "As training progresses, you can go back to the cell where TensorBoard was loaded, click on its refresh button on the top right, and observe the current loss level. It will look similar to this:"
      ],
      "metadata": {
        "id": "NnTuzGNoe3OI"
      },
      "id": "NnTuzGNoe3OI"
    },
    {
      "cell_type": "markdown",
      "id": "c3743fa2",
      "metadata": {
        "id": "c3743fa2"
      },
      "source": [
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/tensorboard_losses.png)\n",
        "\n",
        "####Losses in TensorBoard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b55ffe24",
      "metadata": {
        "id": "b55ffe24"
      },
      "source": [
        "If the losses are oscillating too much (as they will in the next lab), you may smooth the plot using the slider shown in the bottom-right corner:\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/smooth_slider.png)\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/tensorboard_losses_smooth.png)\n",
        "\n",
        "Smooth Losses in TensorBoard\n",
        "\n",
        "This is just a quick overview, so you can use TensorBoard in the next lab, and visualize the losses in real time while your model is training. If you want to know more about running TensorBoard inside notebooks, check out this official [guide](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a25cb8",
      "metadata": {
        "id": "b2a25cb8"
      },
      "source": [
        "## 8.6 HuggingFace Pipelines\n",
        "\n",
        "There are pipelines available for many different tasks in the Hugging Face Hub:\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/hf_nlp_tasks.png)\n",
        "\n",
        "####Natural Language Processing Pipelines in Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis belongs in the \"Text Classification\" bucket, so let's check the default model used by text classification pipelines using the SUPPORTED_TASKS dictionary once again:"
      ],
      "metadata": {
        "id": "DZRlCvjwiCOB"
      },
      "id": "DZRlCvjwiCOB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc836660",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc836660",
        "outputId": "c064e0e4-3872-44a1-c9f6-04f62bab7d61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model': {'pt': ('distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
              "   'af0f99b'),\n",
              "  'tf': ('distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
              "   'af0f99b')}}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers.pipelines import SUPPORTED_TASKS\n",
        "SUPPORTED_TASKS['text-classification']['default']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is a [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model fine-tuned on the [\"Stanford Sentiment Treebank (SST-2)\"](https://huggingface.co/datasets/stanfordnlp/sst2) dataset to perform binary classification. The DistilBERT model is a distilled (that is, more compact with little loss of performance) version of BERT, the famous encoder-based model that spawned a whole family of models, RoBERTa included. We'll dive deeper into these models in \"Contextual Word Embeddings with Transformers\".\n",
        "\n",
        "Now, let's create a text classification pipeline and specify its default model:"
      ],
      "metadata": {
        "id": "LrXtCyw3iMvH"
      },
      "id": "LrXtCyw3iMvH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58500d3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "d35a7ad731cc472e8fa6a7d75a67b4c4",
            "049b91646c464d8d92db7ea5039cd5b0",
            "7565fa84340c40a8aea6cf7068a0d344",
            "e22f5f0aa891496586e8a3a6e4f257ff",
            "25033fb1af4c4ceb9ffb8ffe1f3edb6c",
            "ed512836a56f4cf58ee30b4f790f844a",
            "4e72ab8feaba44709b4721587c7457f0",
            "a81dd71337844400859ef9a7510b3156",
            "4c43bb24cc2449479f3c2861d51b9aba",
            "3dab603433424c52a84576f9c3def259",
            "a5988e9e2bf84b90bcefe39ee2eca7e3",
            "de10ac4a7bb14171898a47c969865671",
            "7e8d3b0df12e47b3b17693da0269c5de",
            "925813c5f70a44e391a9daf50b2a3c9a",
            "80b1fa129f574fea8435abb7a63486a4",
            "7ee7e73f88874a10b708f29f57d99b8c",
            "40bdd701c3e9479da96c7ef6cae7ae37",
            "cadaa875dc274775abab86cb336e48d6",
            "7accfb97429647bebfe26c0f37ab3644",
            "c36316bc762c458cbb9c157fe44098ff",
            "033a0446e112412f870ec43a15984413",
            "1b3359feae254b3f85eac809f7bbc808",
            "6eb6f4985a3944d78a4b881fa852b585",
            "791fd10f7c04429f9041d45a2175a987",
            "ec5dd476939843fd9771e4b69ba173d9",
            "30ec6ce3da734ab48b9f88acad69be83",
            "7b1ee18c2b5c40d1ae3fdc876c2d16c1",
            "22689ebe3cd64eefb4ae4aa69c67c538",
            "67229bbfa8424674b2a183850d11cdba",
            "8888e4958b60464cbd4bff4bdc72d783",
            "1e685707c1584142b9716de3a5e1a3d4",
            "33b486e267454bcc826a80190ba83c4a",
            "fd073b587b064025974f2023b91c1430",
            "be3d93cf991f4446b2bb57fdce5a9462",
            "fe35b07341ab40ac905c8eebbe9e856f",
            "efd304d555ad4830aefb75a14133ab89",
            "a6515609eafc47039119daea723100dd",
            "2fe7e70c3b2a428a9face376a69b250e",
            "4c11686293ed4183b657c8795735c03d",
            "d6e70cd6ab4640c487d3fc9ae5200ced",
            "4e63ecdc2eef430fab79a29ec1575e56",
            "b7f99a0f14ad426ab712326a146205b9",
            "be3245f9a56742e68db4d8c85fa03161",
            "a921bfc5539549e0a33f84cf70e375b8"
          ]
        },
        "id": "58500d3f",
        "outputId": "fd9db1d7-1a80-482d-d8ea-451a3fe600dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dvgodoy/anaconda3/envs/pyt20/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "classifier = pipeline('text-classification', model=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2cef1f",
      "metadata": {
        "id": "8e2cef1f"
      },
      "source": [
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like before, we can feed our input batch directly to the pipeline and get instant predictions:"
      ],
      "metadata": {
        "id": "pgUbYWbeiowX"
      },
      "id": "pgUbYWbeiowX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9ea463",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b9ea463",
        "outputId": "1e3a886f-cb7c-4776-e12f-53d4b6b5a128"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9997199177742004},\n",
              " {'label': 'NEGATIVE', 'score': 0.9996912479400635}]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch = [\"I am really liking this course!\", \"This course is too complicated!\"]\n",
        "\n",
        "classifier(input_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both sentences are easily classified as positive and negative, respectively. Easy, right?\n",
        "\n",
        "We can also take a peek under the hood of our pipeline."
      ],
      "metadata": {
        "id": "9leMRy0ditKg"
      },
      "id": "9leMRy0ditKg"
    },
    {
      "cell_type": "markdown",
      "id": "a94817a5",
      "metadata": {
        "id": "a94817a5"
      },
      "source": [
        "### 8.6.1 Transforms / Tokenizer\n",
        "\n",
        "####Hugging Face Transforms / Tokenizer\n",
        "In the computer vision pipeline, the transformation was an instance of an ImageProcessor. In HF's pipeline, all these steps are performed inside an instance of a Tokenizer. We can easily access the tokenizer that matches our model using the tokenizer attribute of our pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a690f9e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a690f9e5",
        "outputId": "48ccfba2-0fa6-461f-9a01-3750fb9a687e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You probably can recognize some familiar elements:\n",
        "\n",
        "- vocab_size: the size of the vocabulary used in the tokenizer;\n",
        "- model_max_length: maximum length of the input sequence, anything longer gets truncated;\n",
        "- special_tokens: there are tokens for unknown words, for separating sentences, and for padding - we've already discussed those, but also for classification and masking - we'll get back to those two very special tokens in \"Contextual Word Embeddings with Transformers\".\n",
        "Let's tokenize our input batch and decode the result:"
      ],
      "metadata": {
        "id": "FJf6gKIGjDbv"
      },
      "id": "FJf6gKIGjDbv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80fa5fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a80fa5fc",
        "outputId": "4c14985f-c6ca-4c9e-9b05-95a364bb3ca9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1045, 2572, 2428, 16663, 2023, 2607, 999, 102], [101, 2023, 2607, 2003, 2205, 8552, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dict = classifier.tokenizer(input_batch)\n",
        "tokenized_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e986f45e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e986f45e",
        "outputId": "1a90eccb-ff6c-4b69-8fb8-2cb1c6aaa744"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] i am really liking this course! [SEP]'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.tokenizer.decode(tokenized_dict['input_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tokenizer prepended a classification token and appended a separation token to the sequence. The separation token marks the separation between two sequences or, as in our case, the end of a sequence. The classification token is a very special token whose purpose is to generate embeddings that will be used by a classifier head. Don't mind if this sounds too esoteric right now, we'll dig deeper into it in \"Contextual Word Embeddings with Transformers\".\n",
        "\n",
        "We can also load a pretrained tokenizer using the corresponding model name and the AutoTokenizer class:"
      ],
      "metadata": {
        "id": "ldAoQ57bjkVo"
      },
      "id": "ldAoQ57bjkVo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5aeff71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5aeff71",
        "outputId": "97518729-7cd6-40c4-a728-f563b765416a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  2572,  2428, 16663,  2023,  2607,   999,   102],\n",
              "        [  101,  2023,  2607,  2003,  2205,  8552,   999,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "tokenized_output = hf_tokenizer(input_batch, add_special_tokens=True, padding=True, return_tensors='pt')\n",
        "tokenized_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c081a6a1",
      "metadata": {
        "id": "c081a6a1"
      },
      "source": [
        "### 8.6.2 Model\n",
        "\n",
        "####Hugging Face Model\n",
        "No surprises there, that's the model we loaded into our pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a7f087",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4a7f087",
        "outputId": "8e0dfc53-116d-4995-87e6-5663baf17fda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we can also take a closer look at its configuration:"
      ],
      "metadata": {
        "id": "gG62DsnnkK8Q"
      },
      "id": "gG62DsnnkK8Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a530be76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a530be76",
        "outputId": "252b3b39-2e6d-4181-a551-3374efd29830"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertConfig {\n",
              "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
              "  \"activation\": \"gelu\",\n",
              "  \"architectures\": [\n",
              "    \"DistilBertForSequenceClassification\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"dim\": 768,\n",
              "  \"dropout\": 0.1,\n",
              "  \"finetuning_task\": \"sst-2\",\n",
              "  \"hidden_dim\": 3072,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"NEGATIVE\",\n",
              "    \"1\": \"POSITIVE\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"label2id\": {\n",
              "    \"NEGATIVE\": 0,\n",
              "    \"POSITIVE\": 1\n",
              "  },\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"distilbert\",\n",
              "  \"n_heads\": 12,\n",
              "  \"n_layers\": 6,\n",
              "  \"output_past\": true,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"qa_dropout\": 0.1,\n",
              "  \"seq_classif_dropout\": 0.2,\n",
              "  \"sinusoidal_pos_embds\": false,\n",
              "  \"tie_weights_\": true,\n",
              "  \"transformers_version\": \"4.44.0\",\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It tells us the architecture, dimensions, dropout probabilities, the task used to fine-tune it (sentiment analysis on the SST-2 dataset), the associated labels, and more.\n",
        "\n",
        "Just like with the tokenizer, we can load a pretrained model from Hugging Face using the corresponding model name and the AutoModel class from Transformers. Instead of loading the distilled BERT for sequence classification, let's load its plain, encoder-only, version instead:"
      ],
      "metadata": {
        "id": "j1fVG_9UkOww"
      },
      "id": "j1fVG_9UkOww"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a310f87a",
      "metadata": {
        "id": "a310f87a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "headless_model = AutoModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9be063a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9be063a",
        "outputId": "b2d66359-dce8-46e9-97ab-88c092c2caee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headless_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See? That's the same as the distilbert part of our former DistilBertForSequenceClassification. It is a headless model, and it can be used to produce contextual embeddings:"
      ],
      "metadata": {
        "id": "HXOu8BbGkgXo"
      },
      "id": "HXOu8BbGkgXo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eafb049a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eafb049a",
        "outputId": "2376c818-c274-4f93-9fc5-e08c7e53ad78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 9, 768])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "headless_model.eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    output = headless_model(tokenized_output['input_ids'])\n",
        "\n",
        "output['last_hidden_state'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two sentences, nine tokens each, each token represented by an array of 768 numerical values: contextual embeddings by DistilBERT instead of RoBERTa."
      ],
      "metadata": {
        "id": "1RrTE-3-kkYC"
      },
      "id": "1RrTE-3-kkYC"
    },
    {
      "cell_type": "markdown",
      "id": "799151b0",
      "metadata": {
        "id": "799151b0"
      },
      "source": [
        "## 8.7 Generative Models\n",
        "\n",
        "Generative models are decoder-based models. They're used to predict the next word in a sequence of words thus generating text, a task often referred to as causal language modeling. The most popular of all generative models is the Generative Pretrained Transformer, or GPT for short, developed by OpenAI and currently in its fourth generation (GPT-4).\n",
        "\n",
        "In this section, we'll briefly use GPT-2 to illustrate a generative pipeline in Hugging Face. If you have already tried its newer versions (GPT-3, chatGPT, or GPT-4) directly from OpenAI, what follows is going to be, unfortunately, quite underwhelming, but nonetheless useful to give you a glimpse of the inner workings of such models.\n",
        "\n",
        "First, let's load a pretrained GPT-2 using both AutoModel and AutoTokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ccc6f5",
      "metadata": {
        "id": "c5ccc6f5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's start with a very simple and straightforward sentence: \"Hello, how are you...\""
      ],
      "metadata": {
        "id": "psQPHbOgk-bF"
      },
      "id": "psQPHbOgk-bF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb8a8c6",
      "metadata": {
        "id": "2bb8a8c6"
      },
      "outputs": [],
      "source": [
        "sentence = \"Hello, how are you\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You probably completed the sentence in your head: \"doing\". Right? Let's see if the model does the same or not. But, we need to tokenize the sentence to make it an appropriate input for the model:"
      ],
      "metadata": {
        "id": "xPTKz_vPlDUZ"
      },
      "id": "xPTKz_vPlDUZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341b2a89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "341b2a89",
        "outputId": "8d2eb2c9-23ea-412a-85b0-2eab468d62ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[15496,    11,   703,   389,   345]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized = tokenizer(sentence, return_tensors=\"pt\")\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will produce a LOT of logits: we have a mini-batch of one sentence, the sentence having five tokens (\"Hello\", \",\", \"how\", \"are\", and \"you\"), and 50,257 logits for each token, one logit for each token in the vocabulary. For each token/word, the model is assigning a probability to every word it knows, the probability it follows the sequence observed so far."
      ],
      "metadata": {
        "id": "gI8uvnkslJwr"
      },
      "id": "gI8uvnkslJwr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d3dee5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2d3dee5",
        "outputId": "71e92351-0b51-47ac-a54b-a2069b7efb36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 50257])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**tokenized)\n",
        "outputs['logits'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply softmax to the last dimension to get the probabilities, and then take the most likely words chosen by GPT-2:"
      ],
      "metadata": {
        "id": "YCnGG7Melf8t"
      },
      "id": "YCnGG7Melf8t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c370e5b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c370e5b8",
        "outputId": "a934a346-7ab9-4169-b174-31c18691cc44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.0960],\n",
              "         [0.1005],\n",
              "         [0.0908],\n",
              "         [0.6630],\n",
              "         [0.2651]], grad_fn=<TopkBackward0>),\n",
              " tensor([[  11],\n",
              "         [ 314],\n",
              "         [ 546],\n",
              "         [ 345],\n",
              "         [1804]]))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probabilities = torch.nn.functional.softmax(outputs['logits'][0], dim=1)\n",
        "values, indices = torch.topk(probabilities, 1)\n",
        "values, indices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our first token/word, \"Hello\", the more likely word to follow is the word with index 11, which will be followed by the word with index 314, and so on. Let's decode all of them:"
      ],
      "metadata": {
        "id": "XuWssxNXluo_"
      },
      "id": "XuWssxNXluo_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3f3383",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1e3f3383",
        "outputId": "4b9d1b1f-f7d4-4741-f0fe-a2fdee953472"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "', I about you doing'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = tokenizer.decode(indices[:, 0])\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this mean? It means that, as the model receives more words in a sequence, it adjusts its predictions."
      ],
      "metadata": {
        "id": "ltTbbsSil7_L"
      },
      "id": "ltTbbsSil7_L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd46dd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecd46dd2",
        "outputId": "c33abe06-5264-4ccb-da57-0bead58f7d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Tokens so far: Hello\n",
            "   Predicted token to follow: ,\n",
            "2. Tokens so far: Hello ,\n",
            "   Predicted token to follow: I\n",
            "3. Tokens so far: Hello ,  how\n",
            "   Predicted token to follow: about\n",
            "4. Tokens so far: Hello ,  how  are\n",
            "   Predicted token to follow: you\n",
            "5. Tokens so far: Hello ,  how  are  you\n",
            "   Predicted token to follow: doing\n"
          ]
        }
      ],
      "source": [
        "tokens = [tokenizer.decode(t) for t in tokenized['input_ids'][0]]\n",
        "predicted_tokens = predictions.split(' ')\n",
        "\n",
        "for i, p in enumerate(predicted_tokens):\n",
        "    print(f\"{i+1}. Tokens so far: {' '.join(tokens[:i+1])}\\n   Predicted token to follow: {p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Hello, how are you doing\", says GPT-2. Mid-sentence, it got the first (\",\"), and fourth (\"you\") predictions right (as the most likely word, that is). A few years ago, that would be pretty cool, and people would be excited about it. Nowadays, the astounding performance of chatGPT and GPT-4 makes the example above look like child's play.\n",
        "\n",
        "Still, the idea of \"predicting the most likely words that follow\" is at the base of every generative language model, and that's what's being illustrated here. In the final chapter, we'll revisit and explore these concepts further."
      ],
      "metadata": {
        "id": "MEgWlh50l_fn"
      },
      "id": "MEgWlh50l_fn"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "033a0446e112412f870ec43a15984413": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "049b91646c464d8d92db7ea5039cd5b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed512836a56f4cf58ee30b4f790f844a",
            "placeholder": "​",
            "style": "IPY_MODEL_4e72ab8feaba44709b4721587c7457f0",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "1b3359feae254b3f85eac809f7bbc808": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e685707c1584142b9716de3a5e1a3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22689ebe3cd64eefb4ae4aa69c67c538": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25033fb1af4c4ceb9ffb8ffe1f3edb6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe7e70c3b2a428a9face376a69b250e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ec6ce3da734ab48b9f88acad69be83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b486e267454bcc826a80190ba83c4a",
            "placeholder": "​",
            "style": "IPY_MODEL_fd073b587b064025974f2023b91c1430",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.52kB/s]"
          }
        },
        "33b486e267454bcc826a80190ba83c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dab603433424c52a84576f9c3def259": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40bdd701c3e9479da96c7ef6cae7ae37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c11686293ed4183b657c8795735c03d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c43bb24cc2449479f3c2861d51b9aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e63ecdc2eef430fab79a29ec1575e56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e72ab8feaba44709b4721587c7457f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67229bbfa8424674b2a183850d11cdba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6eb6f4985a3944d78a4b881fa852b585": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_791fd10f7c04429f9041d45a2175a987",
              "IPY_MODEL_ec5dd476939843fd9771e4b69ba173d9",
              "IPY_MODEL_30ec6ce3da734ab48b9f88acad69be83"
            ],
            "layout": "IPY_MODEL_7b1ee18c2b5c40d1ae3fdc876c2d16c1"
          }
        },
        "7565fa84340c40a8aea6cf7068a0d344": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a81dd71337844400859ef9a7510b3156",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c43bb24cc2449479f3c2861d51b9aba",
            "value": 629
          }
        },
        "791fd10f7c04429f9041d45a2175a987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22689ebe3cd64eefb4ae4aa69c67c538",
            "placeholder": "​",
            "style": "IPY_MODEL_67229bbfa8424674b2a183850d11cdba",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "7accfb97429647bebfe26c0f37ab3644": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b1ee18c2b5c40d1ae3fdc876c2d16c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e8d3b0df12e47b3b17693da0269c5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40bdd701c3e9479da96c7ef6cae7ae37",
            "placeholder": "​",
            "style": "IPY_MODEL_cadaa875dc274775abab86cb336e48d6",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "7ee7e73f88874a10b708f29f57d99b8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b1fa129f574fea8435abb7a63486a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033a0446e112412f870ec43a15984413",
            "placeholder": "​",
            "style": "IPY_MODEL_1b3359feae254b3f85eac809f7bbc808",
            "value": " 268M/268M [00:00&lt;00:00, 344MB/s]"
          }
        },
        "8888e4958b60464cbd4bff4bdc72d783": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925813c5f70a44e391a9daf50b2a3c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7accfb97429647bebfe26c0f37ab3644",
            "max": 267844284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c36316bc762c458cbb9c157fe44098ff",
            "value": 267844284
          }
        },
        "a5988e9e2bf84b90bcefe39ee2eca7e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6515609eafc47039119daea723100dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be3245f9a56742e68db4d8c85fa03161",
            "placeholder": "​",
            "style": "IPY_MODEL_a921bfc5539549e0a33f84cf70e375b8",
            "value": " 232k/232k [00:00&lt;00:00, 536kB/s]"
          }
        },
        "a81dd71337844400859ef9a7510b3156": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a921bfc5539549e0a33f84cf70e375b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7f99a0f14ad426ab712326a146205b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be3245f9a56742e68db4d8c85fa03161": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3d93cf991f4446b2bb57fdce5a9462": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe35b07341ab40ac905c8eebbe9e856f",
              "IPY_MODEL_efd304d555ad4830aefb75a14133ab89",
              "IPY_MODEL_a6515609eafc47039119daea723100dd"
            ],
            "layout": "IPY_MODEL_2fe7e70c3b2a428a9face376a69b250e"
          }
        },
        "c36316bc762c458cbb9c157fe44098ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cadaa875dc274775abab86cb336e48d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d35a7ad731cc472e8fa6a7d75a67b4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_049b91646c464d8d92db7ea5039cd5b0",
              "IPY_MODEL_7565fa84340c40a8aea6cf7068a0d344",
              "IPY_MODEL_e22f5f0aa891496586e8a3a6e4f257ff"
            ],
            "layout": "IPY_MODEL_25033fb1af4c4ceb9ffb8ffe1f3edb6c"
          }
        },
        "d6e70cd6ab4640c487d3fc9ae5200ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de10ac4a7bb14171898a47c969865671": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e8d3b0df12e47b3b17693da0269c5de",
              "IPY_MODEL_925813c5f70a44e391a9daf50b2a3c9a",
              "IPY_MODEL_80b1fa129f574fea8435abb7a63486a4"
            ],
            "layout": "IPY_MODEL_7ee7e73f88874a10b708f29f57d99b8c"
          }
        },
        "e22f5f0aa891496586e8a3a6e4f257ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dab603433424c52a84576f9c3def259",
            "placeholder": "​",
            "style": "IPY_MODEL_a5988e9e2bf84b90bcefe39ee2eca7e3",
            "value": " 629/629 [00:00&lt;00:00, 39.5kB/s]"
          }
        },
        "ec5dd476939843fd9771e4b69ba173d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8888e4958b60464cbd4bff4bdc72d783",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e685707c1584142b9716de3a5e1a3d4",
            "value": 48
          }
        },
        "ed512836a56f4cf58ee30b4f790f844a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd304d555ad4830aefb75a14133ab89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e63ecdc2eef430fab79a29ec1575e56",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7f99a0f14ad426ab712326a146205b9",
            "value": 231508
          }
        },
        "fd073b587b064025974f2023b91c1430": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe35b07341ab40ac905c8eebbe9e856f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c11686293ed4183b657c8795735c03d",
            "placeholder": "​",
            "style": "IPY_MODEL_d6e70cd6ab4640c487d3fc9ae5200ced",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}