{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eef016c"
      },
      "source": [
        "# Chapter 15: Word Embeddings and Text Classification"
      ],
      "id": "5eef016c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installation Notes\n",
        "To run this notebook on Google Colab, you will need to install the following libraries: transformers, evaluate, datasets, chromadb, langchain, and gensim.\n",
        "\n",
        "In Google Colab, you can run the following command to install them:"
      ],
      "metadata": {
        "id": "CwrgLuBVbnPr"
      },
      "id": "CwrgLuBVbnPr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "928565dd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers evaluate chromadb langchain datasets gensim"
      ],
      "id": "928565dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c8fa4c2"
      },
      "source": [
        "## 15.2 Learning Objectives\n",
        "\n",
        "By the end of this chapter, you should be able to:\n",
        "- tokenize and encode sentences into their corresponding embeddings\n",
        "- train a simple model using embeddings as features\n",
        "- use vector databases to store and search documents\n",
        "- use a similarity metric to perform zero-shot text classification"
      ],
      "id": "2c8fa4c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d17cefe0"
      },
      "source": [
        "## 15.4 AG News Dataset\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
        "\n",
        "In this chapter, we'll be primarily using the AG News Dataset. The original [AG](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) is a collection of more than 1,000,000 news articles gathered from more than 2,000 news sources.\n",
        "\n",
        "The version we'll be using here, the [AG News Dataset](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv) was constructed by choosing the four largest classes from the original corpus, namely, \"world\", \"sports\", \"business\", and \"science and technology\". Each class contains 30,000 training and 1,900 testing samples, amouting to a total of 120,000 training and 7,600 testing samples.\n",
        "\n",
        "The AG News Dataset is a [built-in dataset](https://pytorch.org/text/stable/datasets.html#ag-news) from Torchtext. It downloads the corresponding files directly from the [AG News Dataset](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv) repository."
      ],
      "id": "d17cefe0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using the dataset, we need to do a little bit of cleaning up, such as replacing some special characters and HTML tags that weren't included as raw data. For example, the apostrophe is found 44,316 times as \"#39;\". By cleaning up a little, we'll get more sensible tokens and thus better results.\n",
        "\n",
        "You can download the files from the following links:\n",
        "\n",
        "- https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "- https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\n",
        "- https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/classes.txt\n",
        "Alternatively, you can download all files as a single compressed file instead:\n",
        "\n",
        "https://raw.githubusercontent.com/lftraining/LFD273-code/main/data/AGNews/agnews.zip\n",
        "\n",
        "If you're running Google Colab, you can download the files using the commands below:"
      ],
      "metadata": {
        "id": "kdfv28WfcFYF"
      },
      "id": "kdfv28WfcFYF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11466c94",
        "outputId": "4c8c42c0-8f4d-4899-f830-863831cbf06f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-09 17:13:19--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29470338 (28M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]  28.10M  11.0MB/s    in 2.5s    \n",
            "\n",
            "2024-09-09 17:13:23 (11.0 MB/s) - ‘train.csv’ saved [29470338/29470338]\n",
            "\n",
            "--2024-09-09 17:13:23--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1857427 (1.8M) [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>]   1.77M  9.19MB/s    in 0.2s    \n",
            "\n",
            "2024-09-09 17:13:24 (9.19 MB/s) - ‘test.csv’ saved [1857427/1857427]\n",
            "\n",
            "--2024-09-09 17:13:24--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/classes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31 [text/plain]\n",
            "Saving to: ‘classes.txt’\n",
            "\n",
            "classes.txt         100%[===================>]      31  --.-KB/s    in 0s      \n",
            "\n",
            "2024-09-09 17:13:24 (696 KB/s) - ‘classes.txt’ saved [31/31]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/classes.txt"
      ],
      "id": "11466c94"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d4390d4"
      },
      "source": [
        "### 15.4.1 Data Cleaning\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step2.png)"
      ],
      "id": "5d4390d4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's perform some data cleaning. We're keeping the cleanup to a minimum, namely, replacing the aforementioned special chars and HTML tags. There are also cases of duplicate rows, and even JavaScript code in it, but we won't be handling those here.\n",
        "\n",
        "Here is a non-exhaustive list of characters and tags for replacement:"
      ],
      "metadata": {
        "id": "5IDW6hzFcU8v"
      },
      "id": "5IDW6hzFcU8v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c8cbebf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "chr_codes = np.array([\n",
        "     36,   151,    38,  8220,   147,   148,   146,   225,   133,    39,  8221,  8212,   232,   149,   145,   233,\n",
        "  64257,  8217,   163,   160,    91,    93,  8211,  8482,   234,    37,  8364,   153,   195,   169\n",
        "])\n",
        "chr_subst = {f' #{c};':chr(c) for c in chr_codes}\n",
        "chr_subst.update({' amp;': '&', ' quot;': \"'\", ' hellip;': '...', ' nbsp;': ' ', '&lt;': '', '&gt;': '',\n",
        "                  '&lt;em&gt;': '', '&lt;/em&gt;': '', '&lt;strong&gt;': '', '&lt;/strong&gt;': ''})"
      ],
      "id": "7c8cbebf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here are a couple of helper functions to perform a quick cleanup:"
      ],
      "metadata": {
        "id": "BgkdUSaNcY-j"
      },
      "id": "BgkdUSaNcY-j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2108e26"
      },
      "outputs": [],
      "source": [
        "def replace_chars(sent):\n",
        "    to_replace = [c for c in list(chr_subst.keys()) if c in sent]\n",
        "    for c in to_replace:\n",
        "        sent = sent.replace(c, chr_subst[c])\n",
        "    return sent\n",
        "\n",
        "def preproc_description(desc):\n",
        "    desc = desc.replace('\\\\', ' ').strip()\n",
        "    return replace_chars(desc)"
      ],
      "id": "a2108e26"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902cca8d"
      },
      "source": [
        "### 15.4.2 Hugging Face Datasets\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step4.png)"
      ],
      "id": "902cca8d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've already used Hugging Face Datasets before, first with a tabular dataset, and then again with the Stanford Sentiment Treebank dataset for sentiment analysis.\n",
        "\n",
        "Now, we'll start by loading both CSV files using the load_dataset() method and naming the columns manually. Since each file represents a split, we'll assemble a DatasetDict manually as well:"
      ],
      "metadata": {
        "id": "YvnKdo8EciK8"
      },
      "id": "YvnKdo8EciK8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELSdY-y6aw8j",
        "outputId": "f75b3ffa-4b7f-4598-c555-1934ae722b0a",
        "colab": {
          "referenced_widgets": [
            "9346553d571a4973bc6cc3f497057da2",
            "346970ef70c94c72854acbd7a6e0c05b",
            "",
            "6ddccdf2229f400dab4ec0bf48749ccf",
            "f0da34d42b3343cc84ce565e7bd4aa20"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to /home/dvgodoy/.cache/huggingface/datasets/csv/default-e74aa9f4afc75bd6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9346553d571a4973bc6cc3f497057da2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "346970ef70c94c72854acbd7a6e0c05b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to /home/dvgodoy/.cache/huggingface/datasets/csv/default-e74aa9f4afc75bd6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "Downloading and preparing dataset csv/default to /home/dvgodoy/.cache/huggingface/datasets/csv/default-b95e4b26323eb18c/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ddccdf2229f400dab4ec0bf48749ccf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0da34d42b3343cc84ce565e7bd4aa20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to /home/dvgodoy/.cache/huggingface/datasets/csv/default-b95e4b26323eb18c/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['topic', 'title', 'news'],\n",
              "        num_rows: 120000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['topic', 'title', 'news'],\n",
              "        num_rows: 7600\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset, Split, DatasetDict\n",
        "\n",
        "colnames = ['topic', 'title', 'news']\n",
        "\n",
        "train_ds = load_dataset(\"csv\", data_files='train.csv', sep=',', split=Split.ALL, column_names=colnames)\n",
        "test_ds = load_dataset(\"csv\", data_files='test.csv', sep=',', split=Split.ALL, column_names=colnames)\n",
        "\n",
        "datasets = DatasetDict({'train': train_ds, 'test': test_ds})\n",
        "datasets"
      ],
      "id": "ELSdY-y6aw8j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look at an example from our training set:"
      ],
      "metadata": {
        "id": "mEoEE9fscoWX"
      },
      "id": "mEoEE9fscoWX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DL7rsYYaw8j",
        "outputId": "5ecd7275-4fd6-4910-bac7-900d65b119c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'topic': 3,\n",
              " 'title': 'Wall St. Bears Claw Back Into the Black (Reuters)',\n",
              " 'news': \"Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\"}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets['train'][0]"
      ],
      "id": "7DL7rsYYaw8j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks good! We'll be focusing on the topic (numbered from one to four - world, sports, business, and sci-tech) and the piece of news itself. We won't be using the \"title\" field.\n",
        "\n",
        "We can use the map() method to apply transformations to each key in the dictionary, so we're adjusting the topic numbering to a 0-based index, and we're cleaning up the news using the preproc_description() function:"
      ],
      "metadata": {
        "id": "BZEvj9RRc6gK"
      },
      "id": "BZEvj9RRc6gK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP1MCqZbaw8k",
        "outputId": "5c9548c1-55b3-401b-fd46-61d8f3aaca2b",
        "colab": {
          "referenced_widgets": [
            ""
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "datasets = datasets.map(lambda row: {'topic': row['topic']-1,\n",
        "                                     'news': preproc_description(row['news'])})\n",
        "datasets = datasets.select_columns(['topic', 'news'])"
      ],
      "id": "cP1MCqZbaw8k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a batch of four elements from our training set:"
      ],
      "metadata": {
        "id": "LVmjXgnnc-m4"
      },
      "id": "LVmjXgnnc-m4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIAMc5sPaw8k",
        "outputId": "98a76b8d-d927-4f99-9cdd-a4b372dab96f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([2, 2, 2, 2],\n",
              " [\"Reuters - Short-sellers, Wall Street's dwindling band of ultra-cynics, are seeing green again.\",\n",
              "  'Reuters - Private investment firm Carlyle Group, which has a reputation for making well-timed and occasionally controversial plays in the defense industry, has quietly placed its bets on another part of the market.',\n",
              "  'Reuters - Soaring crude prices plus worries about the economy and the outlook for earnings are expected to hang over the stock market next week during the depth of the summer doldrums.',\n",
              "  'Reuters - Authorities have halted oil export flows from the main pipeline in southern Iraq after intelligence showed a rebel militia could strike infrastructure, an oil official said on Saturday.'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = datasets['train'][:4]\n",
        "labels, descriptions = batch['topic'], batch['news']\n",
        "labels, descriptions"
      ],
      "id": "IIAMc5sPaw8k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the text is cleaned, we can focus on the next step: tokenization."
      ],
      "metadata": {
        "id": "a_5NfTXDdEjP"
      },
      "id": "a_5NfTXDdEjP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c20834dc"
      },
      "source": [
        "## 15.5 Tokenization\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)"
      ],
      "id": "c20834dc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of turning a piece of text, be it a sentence, a paragraph, or a full page, commonly referred to as a \"document\" into a sequence of its components, the tokens.\n",
        "\n",
        "If our document is a paragraph, its sentences may be considered tokens, and we would be talking about sentence tokenization. But, if our document is a sentence, its words (or sometimes subwords, e.g., syllables, prefixes, and suffixes) may be considered tokens.\n",
        "\n",
        "The simplest and most straightforward way of tokenizing a string is to use its split() method:"
      ],
      "metadata": {
        "id": "H9lKA4kIdO-g"
      },
      "id": "H9lKA4kIdO-g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjycDWdHaw8k",
        "outputId": "0b5fd7c1-9c8e-4558-d4ca-d5fc8820dfd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Reuters',\n",
              " '-',\n",
              " 'Short-sellers,',\n",
              " 'Wall',\n",
              " \"Street's\",\n",
              " 'dwindling',\n",
              " 'band',\n",
              " 'of',\n",
              " 'ultra-cynics,',\n",
              " 'are',\n",
              " 'seeing',\n",
              " 'green',\n",
              " 'again.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = descriptions[0].split()\n",
        "tokens"
      ],
      "id": "yjycDWdHaw8k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "#### Aside: Gensim\n",
        "\n",
        "Gensim is a popular library for topic modeling, which offers out-of-the-box tools for NLP-related tasks such as tokenization, vocabularies, and pretrained embeddings. In this chapter, we'll be using a few tools from Gensim such as the simple_preprocess() utility function for tokenization and the downloader() to retrieve and instantiate pretrained GloVe embeddings.\n",
        "***\n",
        "However, we'd be overlooking lots of details: lower and uppercase differences, accents, special characters, punctuation, etc. This kind of preprocessing can be tedious to implement, so we could use a utility function such as Gensim's simple_preprocess() to take care of these steps:"
      ],
      "metadata": {
        "id": "Ln1OoA-rens8"
      },
      "id": "Ln1OoA-rens8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dp4gCvbaw8k",
        "outputId": "96d8abb2-ec88-4987-9ea3-c1f80c1f7ae2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['reuters',\n",
              " 'short',\n",
              " 'sellers',\n",
              " 'wall',\n",
              " 'street',\n",
              " 'dwindling',\n",
              " 'band',\n",
              " 'of',\n",
              " 'ultra',\n",
              " 'cynics',\n",
              " 'are',\n",
              " 'seeing',\n",
              " 'green',\n",
              " 'again']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "tokens = simple_preprocess(descriptions[0])\n",
        "tokens"
      ],
      "id": "1dp4gCvbaw8k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "No punctuation, no apostrophes, lowercase words. That's some good old-fashioned tokenization, and it still is an important part of the tokenization pipeline of current tokenizers.\n",
        "\n",
        "In HF tokenizers, this kind of cleaning up and splitting up is performed by two components of the [tokenization pipeline](https://huggingface.co/docs/tokenizers/en/pipeline): the normalizer and the pre-tokenizer. Let's load a typical BERT tokenizer and see what is the output of each one of these steps:"
      ],
      "metadata": {
        "id": "puWb11PWfAT-"
      },
      "id": "puWb11PWfAT-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UZRPOIAaw8l",
        "outputId": "dd5cdfe3-8285-4ebf-9bb9-854808770e29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dvgodoy/anaconda3/envs/pyt20/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tokenizers.Tokenizer at 0x7f2c4d81ac30>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "tok_obj = tokenizer.backend_tokenizer\n",
        "tok_obj"
      ],
      "id": "3UZRPOIAaw8l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer from the transformers library is actually a wrapper around the tokenizer from the tokenizers library. We can retrieve the latter using the former's backend_tokenizer attribute.\n",
        "\n",
        "The backend tokenizer is the one that implements the pipeline. Let's take a look at its first step, the normalizer:"
      ],
      "metadata": {
        "id": "ui32SJwghe74"
      },
      "id": "ui32SJwghe74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ0wq7qDaw8l",
        "outputId": "8129054e-cd3d-401c-8c79-7cb95898d03c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, True, None)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalizer = tok_obj.normalizer\n",
        "normalizer.lowercase, normalizer.clean_text, normalizer.strip_accents"
      ],
      "id": "pZ0wq7qDaw8l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "By inspecting the attributes of the normalizer, we can see that it is configured to convert the string to lowercase, and to clean the text (removing special characters), but not strip it out of its accents. Let's put it to the test by calling its normalize_str() method:"
      ],
      "metadata": {
        "id": "dpzRiPBah_rz"
      },
      "id": "dpzRiPBah_rz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7azJUUKEaw8l",
        "outputId": "fcfc4d9f-dfe5-4b2e-e2ea-3980272b07e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"reuters - short-sellers, wall street's dwindling band of ultra-cynics, are seeing green again.\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalized = normalizer.normalize_str(descriptions[0])\n",
        "normalized"
      ],
      "id": "7azJUUKEaw8l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, the normalized string is going to be pre-tokenized (which is pretty much the same as the old-fashioned tokenization). Let's try it out by calling the pre-tokenizer's pre_tokenize_str() method:"
      ],
      "metadata": {
        "id": "OyuSxx6FiE-B"
      },
      "id": "OyuSxx6FiE-B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O81VDznQaw8l",
        "outputId": "48bc01ae-1adc-448d-9629-4ce0d02bfd1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('reuters', (0, 7)),\n",
              " ('-', (8, 9)),\n",
              " ('short', (10, 15)),\n",
              " ('-', (15, 16)),\n",
              " ('sellers', (16, 23)),\n",
              " (',', (23, 24)),\n",
              " ('wall', (25, 29)),\n",
              " ('street', (30, 36)),\n",
              " (\"'\", (36, 37)),\n",
              " ('s', (37, 38)),\n",
              " ('dwindling', (39, 48)),\n",
              " ('band', (49, 53)),\n",
              " ('of', (54, 56)),\n",
              " ('ultra', (57, 62)),\n",
              " ('-', (62, 63)),\n",
              " ('cynics', (63, 69)),\n",
              " (',', (69, 70)),\n",
              " ('are', (71, 74)),\n",
              " ('seeing', (75, 81)),\n",
              " ('green', (82, 87)),\n",
              " ('again', (88, 93)),\n",
              " ('.', (93, 94))]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokenizer = tok_obj.pre_tokenizer\n",
        "tokens = pre_tokenizer.pre_tokenize_str(normalized)\n",
        "tokens"
      ],
      "id": "O81VDznQaw8l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, its approach is very simple: even commas and apostrophes are considered individual tokens and the sentence is split accordingly. Moreover, the pre-tokenizer keeps track of the position of each token in the original sentence, denoted by the tuple of integers next to each token.\n",
        "\n",
        "So far, there's nothing extraordinary about it, right? We're getting to the interesting part, which is the definition of the tokenizer's vocabulary, in the next section."
      ],
      "metadata": {
        "id": "sIVQTtwJiKOh"
      },
      "id": "sIVQTtwJiKOh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02890969"
      },
      "source": [
        "### 15.5.1 Vocabulary\n",
        "\n",
        "The set of all unique tokens in a corpus of text (that is, a collection of documents) makes up its corresponding vocabulary. The vocabulary is like the \"dictionary\" (although not necessarily in the Python sense!) that contains entries for every token (word) that we expect to find in our sentences. We can retrieve the tokenizer's vocabulary using the get_vocab() method:"
      ],
      "id": "02890969"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GURBhwiWaw8m",
        "outputId": "98b9dc47-1da6-47b9-a2f9-6cb1588fd517"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'##camp': 26468,\n",
              " '[unused625]': 630,\n",
              " 'raid': 8118,\n",
              " 'zoological': 26168,\n",
              " 'guarantee': 11302,\n",
              " 'auckland': 8666,\n",
              " 'fai': 26208,\n",
              " 'overhead': 8964,\n",
              " '##hee': 21030,\n",
              " 'johnston': 10773,\n",
              " '##linger': 23101,\n",
              " 'acting': 3772,\n",
              " '##tablished': 28146,\n",
              " '[unused9]': 10,\n",
              " 'considered': 2641,\n",
              " 'pardon': 14933,\n",
              " 'greyish': 26916,\n",
              " '##54': 27009,\n",
              " 'caracas': 21675,\n",
              " 'renumbered': 27855,\n",
              " 'flowing': 8577,\n",
              " '[unused748]': 753,\n",
              " 'forces': 2749,\n",
              " 'credited': 5827,\n",
              " '##hell': 18223,\n",
              " 'milk': 6501,\n",
              " 'deals': 9144,\n",
              " '##kle': 19099,\n",
              " 'christy': 21550,\n",
              " 'guests': 6368,\n",
              " '[unused372]': 377,\n",
              " 'curse': 8364,\n",
              " 'alvaro': 24892,\n",
              " '##onus': 24891,\n",
              " '##千': 30310,\n",
              " 'corpses': 18113,\n",
              " 'mollusk': 13269,\n",
              " '−': 1597,\n",
              " '##graphy': 12565,\n",
              " '##lius': 15513,\n",
              " 'convincing': 13359,\n",
              " 'clutched': 13514,\n",
              " 'iraq': 5712,\n",
              " '##ggy': 22772,\n",
              " 'urgency': 19353,\n",
              " 'executives': 12706,\n",
              " 'hobart': 14005,\n",
              " 'telecommunication': 25958,\n",
              " 'def': 13366,\n",
              " 'inmates': 13187,\n",
              " 'twinned': 25901,\n",
              " 'kathleen': 14559,\n",
              " 'staircase': 10714,\n",
              " 'approached': 5411,\n",
              " '38': 4229,\n",
              " 'µ': 1085,\n",
              " '##hedral': 27310,\n",
              " 'dared': 15048,\n",
              " 'cork': 8513,\n",
              " '##worm': 22769,\n",
              " 'pol': 14955,\n",
              " 'legs': 3456,\n",
              " '##pot': 11008,\n",
              " 'insights': 20062,\n",
              " 'ant': 14405,\n",
              " '##rae': 16652,\n",
              " 'mcconnell': 28514,\n",
              " 'stirred': 13551,\n",
              " 'gu': 19739,\n",
              " 'abbas': 17532,\n",
              " '##mbre': 19908,\n",
              " 'shallow': 8467,\n",
              " '##rika': 23778,\n",
              " '1823': 12522,\n",
              " 'exclusively': 7580,\n",
              " 'ebert': 22660,\n",
              " 'climbed': 6589,\n",
              " '##boy': 11097,\n",
              " 'skipping': 25978,\n",
              " 'southend': 26104,\n",
              " 'enthusiasm': 12024,\n",
              " '##idad': 27893,\n",
              " 'berwick': 24957,\n",
              " 'anxiety': 10089,\n",
              " '##ty': 3723,\n",
              " 'avoids': 26777,\n",
              " 'acacia': 24766,\n",
              " '##ent': 4765,\n",
              " 'revenue': 6599,\n",
              " 'exhibits': 10637,\n",
              " 'lin': 11409,\n",
              " 'glaring': 16124,\n",
              " 'warwick': 13283,\n",
              " 'ড': 1360,\n",
              " '[unused712]': 717,\n",
              " 'chair': 3242,\n",
              " '[unused431]': 436,\n",
              " 'impending': 17945,\n",
              " 'heraldic': 28867,\n",
              " '1810': 11786,\n",
              " 'sv': 17917,\n",
              " 'ancestry': 11377,\n",
              " 'mounds': 19503,\n",
              " 'inhuman': 29582,\n",
              " '[unused33]': 34,\n",
              " 'elections': 3864,\n",
              " '##生': 30436,\n",
              " 'harm': 7386,\n",
              " 'softer': 19013,\n",
              " 'americana': 25988,\n",
              " '351': 28474,\n",
              " 'blouse': 18149,\n",
              " 'formation': 4195,\n",
              " 'fk': 14352,\n",
              " '[unused374]': 379,\n",
              " 'nikola': 24794,\n",
              " 'paced': 13823,\n",
              " 'turnpike': 17116,\n",
              " 'fantastic': 10392,\n",
              " 'ic': 24582,\n",
              " 'cornered': 25878,\n",
              " 'literature': 3906,\n",
              " '##ndt': 26379,\n",
              " 'seas': 11915,\n",
              " '##uf': 16093,\n",
              " 'violating': 20084,\n",
              " '##dit': 23194,\n",
              " '##¡': 29644,\n",
              " 'sodium': 13365,\n",
              " 'aquarium': 18257,\n",
              " '[unused343]': 348,\n",
              " '##cos': 13186,\n",
              " '10': 2184,\n",
              " '1970': 3359,\n",
              " 'tunic': 23002,\n",
              " 'biodiversity': 17194,\n",
              " 'blaine': 20002,\n",
              " '[unused306]': 311,\n",
              " 'convicts': 24948,\n",
              " 'wr': 23277,\n",
              " 'powerless': 25192,\n",
              " 'stats': 26319,\n",
              " 'hairy': 15892,\n",
              " 'parenting': 28586,\n",
              " 'austrian': 6161,\n",
              " 'bayer': 26367,\n",
              " 'departure': 6712,\n",
              " '##version': 27774,\n",
              " '##ios': 10735,\n",
              " 'collect': 8145,\n",
              " '1714': 25241,\n",
              " 'dirk': 17594,\n",
              " 'unlimited': 14668,\n",
              " 'distrust': 29245,\n",
              " 'ripe': 22503,\n",
              " 'encoding': 17181,\n",
              " 'bestowed': 17398,\n",
              " 'ratification': 27369,\n",
              " 'phase': 4403,\n",
              " 'culinary': 20560,\n",
              " 'bishops': 8414,\n",
              " 'ε': 1159,\n",
              " '##hmi': 26837,\n",
              " 'swimmer': 13361,\n",
              " '##hire': 20908,\n",
              " 'others': 2500,\n",
              " 'burkina': 23089,\n",
              " '##teacher': 24741,\n",
              " 'hailed': 16586,\n",
              " '##inated': 15833,\n",
              " 'এ': 1351,\n",
              " 'thirds': 12263,\n",
              " '##shima': 24772,\n",
              " 'akbar': 20730,\n",
              " 'crowd': 4306,\n",
              " 'involuntary': 26097,\n",
              " '[unused320]': 325,\n",
              " 'testimony': 10896,\n",
              " '##guchi': 16918,\n",
              " '##eves': 23047,\n",
              " 'crest': 11146,\n",
              " 'nah': 20976,\n",
              " 'land': 2455,\n",
              " '265': 20549,\n",
              " 'money': 2769,\n",
              " '##jit': 18902,\n",
              " 'japan': 2900,\n",
              " 'huge': 4121,\n",
              " 'tide': 10401,\n",
              " 'nicky': 20158,\n",
              " 'deciduous': 22411,\n",
              " '##cise': 18380,\n",
              " 'oneself': 25763,\n",
              " 'collided': 17745,\n",
              " 'hoc': 21929,\n",
              " '##^': 29637,\n",
              " 'mist': 11094,\n",
              " 'zone': 4224,\n",
              " 'perth': 9300,\n",
              " 'gina': 17508,\n",
              " '##nburg': 22642,\n",
              " '##weight': 11179,\n",
              " 'chelsea': 9295,\n",
              " 'adventure': 6172,\n",
              " 'sued': 12923,\n",
              " 'determination': 9128,\n",
              " '##set': 13462,\n",
              " 'aperture': 18892,\n",
              " 'documentary': 4516,\n",
              " 'dade': 27647,\n",
              " 'reorganisation': 24934,\n",
              " 'resign': 12897,\n",
              " 'saliva': 26308,\n",
              " 'drain': 12475,\n",
              " '##anov': 25417,\n",
              " 'acc': 16222,\n",
              " 'harrisburg': 24569,\n",
              " 'imagery': 13425,\n",
              " 'california': 2662,\n",
              " 'gap': 6578,\n",
              " '##asia': 15396,\n",
              " 'neurons': 15698,\n",
              " 'yamaha': 24031,\n",
              " 'snapping': 15790,\n",
              " 'bitterness': 22364,\n",
              " 'ふ': 1674,\n",
              " 'reductions': 25006,\n",
              " 'satisfy': 13225,\n",
              " 'yu': 9805,\n",
              " 'forward': 2830,\n",
              " 'flew': 5520,\n",
              " 'nash': 10594,\n",
              " 'reduce': 5547,\n",
              " 'puzzled': 14909,\n",
              " 'dynamo': 17205,\n",
              " '##ras': 8180,\n",
              " 'inning': 12994,\n",
              " '##sity': 17759,\n",
              " 'backed': 6153,\n",
              " 'lyrics': 4581,\n",
              " 'switch': 6942,\n",
              " '[unused950]': 955,\n",
              " 'midfielder': 8850,\n",
              " 'bred': 13680,\n",
              " 'stairway': 21952,\n",
              " 'manually': 21118,\n",
              " 'condor': 29260,\n",
              " 'bob': 3960,\n",
              " 'genetic': 7403,\n",
              " 'difficulties': 8190,\n",
              " 'ᄊ': 1462,\n",
              " 'swiss': 5364,\n",
              " 'keith': 6766,\n",
              " '##lo': 4135,\n",
              " '[unused1]': 2,\n",
              " 'eagle': 6755,\n",
              " '##宮': 30350,\n",
              " 'caine': 19881,\n",
              " 'details': 4751,\n",
              " 'overseeing': 19642,\n",
              " 'short': 2460,\n",
              " 'venom': 15779,\n",
              " 'labeled': 12599,\n",
              " 'reginald': 14435,\n",
              " 'brains': 14332,\n",
              " 'spire': 19823,\n",
              " 'brewster': 23009,\n",
              " 'cats': 8870,\n",
              " '##bol': 14956,\n",
              " '##mmer': 15810,\n",
              " 'princely': 22771,\n",
              " 'conservatives': 11992,\n",
              " 'strengthening': 16003,\n",
              " '35th': 20198,\n",
              " 'institutional': 12148,\n",
              " 'posing': 20540,\n",
              " 'convertible': 22840,\n",
              " 'alphabet': 12440,\n",
              " 'fresno': 20840,\n",
              " 'চ': 1356,\n",
              " 'maize': 21154,\n",
              " '[unused375]': 380,\n",
              " 'burials': 23109,\n",
              " '##iculate': 24153,\n",
              " 'ala': 21862,\n",
              " 'ferdinand': 9684,\n",
              " 'second': 2117,\n",
              " 'cab': 9298,\n",
              " 'obsession': 17418,\n",
              " 'ipa': 24531,\n",
              " 'rolf': 23381,\n",
              " '[unused219]': 224,\n",
              " 'seater': 23392,\n",
              " 'cipher': 27715,\n",
              " 'grandparents': 14472,\n",
              " '##torm': 20654,\n",
              " 'ssr': 20896,\n",
              " '##eron': 26534,\n",
              " 'dreamed': 13830,\n",
              " '##上': 30268,\n",
              " 'companions': 11946,\n",
              " '##ries': 5134,\n",
              " 'julia': 6423,\n",
              " 'lethal': 12765,\n",
              " 'mustang': 18851,\n",
              " 'ʁ': 1128,\n",
              " 'novels': 6002,\n",
              " 'diagrams': 26309,\n",
              " 'waivers': 28654,\n",
              " 'topical': 25665,\n",
              " '[unused239]': 244,\n",
              " 'attorney': 4905,\n",
              " '[unused688]': 693,\n",
              " 'alarmed': 19260,\n",
              " 'themed': 11773,\n",
              " 'gloves': 11875,\n",
              " '[unused943]': 948,\n",
              " 'cable': 5830,\n",
              " 'bulldog': 28628,\n",
              " '[unused114]': 119,\n",
              " 'deaf': 12419,\n",
              " '##tto': 9284,\n",
              " 'nguyen': 16577,\n",
              " 'weary': 16040,\n",
              " 'raj': 11948,\n",
              " 'tries': 5363,\n",
              " '##tania': 21013,\n",
              " 'electronically': 28926,\n",
              " 'marsh': 9409,\n",
              " 'als': 25520,\n",
              " 'clinched': 18311,\n",
              " 'greta': 26111,\n",
              " 'marxist': 15511,\n",
              " 'notion': 9366,\n",
              " '220': 10545,\n",
              " 'namesake': 17283,\n",
              " 'vaguely': 15221,\n",
              " 'unpredictable': 21446,\n",
              " 'revolution': 4329,\n",
              " 'investment': 5211,\n",
              " 'testosterone': 25937,\n",
              " 'exceeded': 14872,\n",
              " 'juarez': 25398,\n",
              " 'lace': 12922,\n",
              " 'reflections': 16055,\n",
              " 'hooker': 17074,\n",
              " '##rmin': 27512,\n",
              " 'spherical': 18970,\n",
              " 'friendship': 6860,\n",
              " 'humanity': 8438,\n",
              " 'blu': 14154,\n",
              " 'winston': 10180,\n",
              " '##vac': 24887,\n",
              " 'favourable': 18731,\n",
              " 'robertson': 9923,\n",
              " '##iii': 28954,\n",
              " 'ours': 14635,\n",
              " 'harsh': 8401,\n",
              " 'networks': 6125,\n",
              " 'ivory': 11554,\n",
              " 'javier': 13824,\n",
              " 'organic': 7554,\n",
              " 'patronage': 15694,\n",
              " 'chips': 11772,\n",
              " 'tavi': 24283,\n",
              " 'ч': 1202,\n",
              " 'sixty': 8442,\n",
              " 'liking': 16663,\n",
              " 'paxton': 27765,\n",
              " 'spectral': 17435,\n",
              " 'evan': 9340,\n",
              " 'handsome': 8502,\n",
              " '##rians': 23543,\n",
              " '##vio': 25500,\n",
              " 'criticised': 10648,\n",
              " 'suggestion': 10293,\n",
              " 'lt': 8318,\n",
              " '##?': 29632,\n",
              " 'spaniards': 20999,\n",
              " 'variations': 8358,\n",
              " 'sakura': 23066,\n",
              " '[unused611]': 616,\n",
              " 'outdoor': 7254,\n",
              " 'seldom': 15839,\n",
              " 'wc': 15868,\n",
              " '##cellular': 16882,\n",
              " 'no': 2053,\n",
              " 'crush': 10188,\n",
              " 'excel': 24970,\n",
              " 'kilometre': 13214,\n",
              " 'analytics': 25095,\n",
              " 'ᵖ': 1504,\n",
              " 'makers': 11153,\n",
              " 'freaking': 13847,\n",
              " '##ix': 7646,\n",
              " 'metal': 3384,\n",
              " 'प': 1328,\n",
              " 'properties': 5144,\n",
              " 'dish': 9841,\n",
              " 'sucking': 13475,\n",
              " 'nods': 11232,\n",
              " 'connacht': 27062,\n",
              " 'admiration': 17005,\n",
              " 'morton': 11164,\n",
              " 'curling': 11599,\n",
              " '##ngen': 25997,\n",
              " 'budgets': 26178,\n",
              " '##bau': 27773,\n",
              " 'honneur': 28197,\n",
              " 'yamamoto': 28318,\n",
              " 'intelligence': 4454,\n",
              " '1808': 13040,\n",
              " 'mention': 5254,\n",
              " 'health': 2740,\n",
              " '##nbc': 28957,\n",
              " 'wholly': 12590,\n",
              " 'vogue': 17734,\n",
              " 'negro': 12593,\n",
              " 'amar': 23204,\n",
              " 'pants': 6471,\n",
              " 'schwartz': 16756,\n",
              " 'naturalized': 27558,\n",
              " '##oto': 11439,\n",
              " 'slashed': 23587,\n",
              " 'spacecraft': 12076,\n",
              " 'dispatched': 14501,\n",
              " 'hannah': 8410,\n",
              " 'distributed': 5500,\n",
              " 'carolyn': 15611,\n",
              " '発': 1914,\n",
              " '##oran': 18842,\n",
              " 'scripted': 22892,\n",
              " 'mustache': 28786,\n",
              " '##cana': 28621,\n",
              " '##heimer': 18826,\n",
              " 'promotions': 15365,\n",
              " 'schumacher': 22253,\n",
              " '##ingham': 16445,\n",
              " 'nitrogen': 14114,\n",
              " 'inverse': 19262,\n",
              " 'pest': 20739,\n",
              " '[unused264]': 269,\n",
              " 'encourages': 16171,\n",
              " '##sus': 13203,\n",
              " '[unused48]': 49,\n",
              " 'stole': 10312,\n",
              " 'strikes': 9326,\n",
              " 'bournemouth': 22882,\n",
              " 'strategies': 9942,\n",
              " 'truly': 5621,\n",
              " 'inquired': 24849,\n",
              " 'snapped': 5941,\n",
              " '##wear': 16689,\n",
              " 'george': 2577,\n",
              " 'sway': 17812,\n",
              " 'strung': 23509,\n",
              " '街': 1946,\n",
              " '##work': 6198,\n",
              " 'logan': 6307,\n",
              " '##ographic': 13705,\n",
              " 'crypt': 19888,\n",
              " 'chained': 22075,\n",
              " 'ordeal': 23304,\n",
              " '[unused14]': 15,\n",
              " 'turkmenistan': 25432,\n",
              " 'equality': 9945,\n",
              " 'distinction': 7835,\n",
              " 'fae': 17282,\n",
              " 'mckenzie': 18506,\n",
              " 'faulty': 28927,\n",
              " '##ories': 18909,\n",
              " 'asleep': 6680,\n",
              " '##pressing': 24128,\n",
              " '##itive': 13043,\n",
              " '##yuki': 19663,\n",
              " 'ᅳ': 1481,\n",
              " '##rwin': 27349,\n",
              " 'deteriorating': 28440,\n",
              " 'establishing': 7411,\n",
              " 'aroma': 23958,\n",
              " '1868': 7582,\n",
              " '##bian': 15599,\n",
              " '##nton': 15104,\n",
              " 'protecting': 8650,\n",
              " 'compares': 22963,\n",
              " 'hitch': 27738,\n",
              " 'tamil': 6008,\n",
              " 'archers': 23118,\n",
              " 'assessing': 20077,\n",
              " 'reflect': 8339,\n",
              " 'slang': 21435,\n",
              " 'talented': 10904,\n",
              " 'sound': 2614,\n",
              " '⁵': 1539,\n",
              " '##gnon': 26977,\n",
              " 'singing': 4823,\n",
              " 'resurgence': 26303,\n",
              " 'sima': 26769,\n",
              " '##liness': 20942,\n",
              " 'zane': 6944,\n",
              " 'ceo': 5766,\n",
              " 'sustainability': 15169,\n",
              " 'pinto': 25066,\n",
              " 'responsibility': 5368,\n",
              " 'consultative': 28581,\n",
              " 'crystal': 6121,\n",
              " '276': 25113,\n",
              " 'bree': 21986,\n",
              " 'admiralty': 14179,\n",
              " 'dowry': 29603,\n",
              " 'callie': 20072,\n",
              " 'bribery': 27748,\n",
              " 'antony': 16262,\n",
              " 'joan': 7437,\n",
              " '##cam': 28727,\n",
              " '##grave': 12830,\n",
              " 'forrest': 16319,\n",
              " 'wilkinson': 16237,\n",
              " '##vil': 14762,\n",
              " 'legislatures': 27977,\n",
              " '##ght': 13900,\n",
              " '##zle': 29247,\n",
              " 'almeida': 29555,\n",
              " '##ghi': 28891,\n",
              " '##enko': 17868,\n",
              " 'ranked': 4396,\n",
              " 'wilderness': 9917,\n",
              " 'north': 2167,\n",
              " 'bianca': 18051,\n",
              " 'migrating': 28636,\n",
              " 'rabbi': 7907,\n",
              " 'plunging': 29059,\n",
              " 'yoko': 28758,\n",
              " 'cry': 5390,\n",
              " 'liquid': 6381,\n",
              " 'italians': 16773,\n",
              " 'incline': 27461,\n",
              " '##rio': 9488,\n",
              " 'drake': 7867,\n",
              " 'pearson': 12874,\n",
              " '皇': 1917,\n",
              " '##ト': 30240,\n",
              " 'instituto': 22596,\n",
              " 'suns': 19352,\n",
              " 'pius': 14363,\n",
              " '82': 6445,\n",
              " 'optimal': 15502,\n",
              " 'coyote': 20457,\n",
              " 'manning': 11956,\n",
              " 'ghent': 24202,\n",
              " '|': 1064,\n",
              " 'las': 5869,\n",
              " '我': 1855,\n",
              " 'tin': 9543,\n",
              " 'cyprus': 9719,\n",
              " '##nen': 10224,\n",
              " '##tler': 25091,\n",
              " 'epa': 19044,\n",
              " 'syllables': 20732,\n",
              " 'stripped': 10040,\n",
              " 'coupled': 11211,\n",
              " '##ɔ': 29679,\n",
              " '##δ': 29722,\n",
              " 'declared': 4161,\n",
              " 'chatham': 16727,\n",
              " 'fly': 4875,\n",
              " 'featured': 2956,\n",
              " 'defense': 3639,\n",
              " '[unused565]': 570,\n",
              " 'pussy': 22418,\n",
              " 'sumo': 28193,\n",
              " 'tristan': 9822,\n",
              " '530': 23523,\n",
              " 'government': 2231,\n",
              " '##nne': 10087,\n",
              " 'charleston': 10907,\n",
              " 'cesar': 14923,\n",
              " 'prehistoric': 14491,\n",
              " 'evidenced': 21328,\n",
              " 'ears': 5551,\n",
              " 'belize': 18867,\n",
              " 'न': 1327,\n",
              " 'bolshevik': 24477,\n",
              " '##urer': 27595,\n",
              " 'demanding': 9694,\n",
              " 'nyc': 16392,\n",
              " 'sonny': 13584,\n",
              " '##ve': 3726,\n",
              " 'formulation': 20219,\n",
              " 'fumbled': 20054,\n",
              " 'bullock': 25200,\n",
              " 'pixels': 27725,\n",
              " 'vulnerable': 8211,\n",
              " 'bonuses': 29563,\n",
              " 'swiped': 24452,\n",
              " 'ahl': 18347,\n",
              " 'crosses': 7821,\n",
              " 'bison': 22285,\n",
              " 'localized': 22574,\n",
              " '##tori': 29469,\n",
              " 'leigh': 11797,\n",
              " 'faint': 8143,\n",
              " 'inspector': 7742,\n",
              " 'ut': 21183,\n",
              " 'affect': 7461,\n",
              " '##ulent': 27581,\n",
              " 'strangers': 12358,\n",
              " 'stench': 21555,\n",
              " 'zurich': 10204,\n",
              " 'forum': 7057,\n",
              " 'closure': 8503,\n",
              " '1': 1015,\n",
              " 'outcome': 9560,\n",
              " '##hos': 15006,\n",
              " '##35': 19481,\n",
              " 'oppressive': 28558,\n",
              " 'outward': 15436,\n",
              " '##ン': 30263,\n",
              " '辶': 1956,\n",
              " 'albrecht': 25542,\n",
              " '##illa': 9386,\n",
              " 'william': 2520,\n",
              " 'privacy': 9394,\n",
              " 'teacher': 3836,\n",
              " 'dependency': 24394,\n",
              " 'frenzy': 21517,\n",
              " '##禾': 30453,\n",
              " 'organising': 21317,\n",
              " 'occurring': 10066,\n",
              " '357': 26231,\n",
              " '美': 1935,\n",
              " '1795': 13397,\n",
              " 'pleading': 16418,\n",
              " 'dreamer': 24726,\n",
              " 'gaga': 23332,\n",
              " 'hilbert': 27434,\n",
              " 'viewing': 10523,\n",
              " '##―': 30053,\n",
              " '##aia': 27131,\n",
              " '##heads': 13038,\n",
              " 'teresa': 12409,\n",
              " '[unused433]': 438,\n",
              " 'lauren': 10294,\n",
              " 'booking': 21725,\n",
              " '##成': 30380,\n",
              " 'brute': 26128,\n",
              " '##sume': 23545,\n",
              " 'cellular': 12562,\n",
              " '##cute': 26869,\n",
              " 'typed': 21189,\n",
              " '##r': 2099,\n",
              " 'rested': 8614,\n",
              " 'rector': 10935,\n",
              " '16': 2385,\n",
              " 'zero': 5717,\n",
              " 'struggled': 6915,\n",
              " 'selecting': 17739,\n",
              " 'salvage': 18340,\n",
              " 'skyline': 21343,\n",
              " '##章': 30458,\n",
              " '[unused577]': 582,\n",
              " 'buck': 10131,\n",
              " '1665': 27676,\n",
              " 'dominate': 16083,\n",
              " 'syndicate': 16229,\n",
              " '沢': 1898,\n",
              " 'reporters': 12060,\n",
              " 'explores': 15102,\n",
              " 'henry': 2888,\n",
              " '##row': 10524,\n",
              " 'relinquished': 26566,\n",
              " 'lacks': 14087,\n",
              " 'ricky': 11184,\n",
              " 'friendly': 5379,\n",
              " '##bel': 8671,\n",
              " '##puri': 24661,\n",
              " '640': 19714,\n",
              " '##chner': 28254,\n",
              " 'united': 2142,\n",
              " '1621': 27037,\n",
              " 'airfields': 25278,\n",
              " '##¤': 29647,\n",
              " '##yo': 7677,\n",
              " 'crop': 10416,\n",
              " 'manifested': 24906,\n",
              " '[unused113]': 118,\n",
              " 'extremely': 5186,\n",
              " '##oche': 23555,\n",
              " 'sacks': 14918,\n",
              " 'luigi': 15153,\n",
              " 'ӏ': 1218,\n",
              " 'curry': 15478,\n",
              " '##nare': 26148,\n",
              " 'venezuela': 8326,\n",
              " 'knowles': 22815,\n",
              " 'shropshire': 19322,\n",
              " 'additions': 13134,\n",
              " 'crimes': 6997,\n",
              " '2008': 2263,\n",
              " '##frid': 27439,\n",
              " 'judgment': 8689,\n",
              " 'westminster': 9434,\n",
              " 'comedian': 9971,\n",
              " '##ette': 7585,\n",
              " 'wee': 16776,\n",
              " '##¥': 29648,\n",
              " 'harcourt': 22714,\n",
              " 'is': 2003,\n",
              " '##iot': 25185,\n",
              " '##venting': 26703,\n",
              " 'napoleon': 8891,\n",
              " 'hospital': 2902,\n",
              " 'making': 2437,\n",
              " '##kti': 22462,\n",
              " 'rigorous': 20001,\n",
              " 'buffer': 17698,\n",
              " '##zhi': 19436,\n",
              " 'sediments': 20476,\n",
              " '##verance': 21998,\n",
              " '##raine': 26456,\n",
              " 'mercy': 8673,\n",
              " '1626': 28818,\n",
              " 'glare': 10982,\n",
              " 'ł': 1105,\n",
              " 'ₕ': 1564,\n",
              " 'workings': 24884,\n",
              " 'chassis': 11832,\n",
              " 'state': 2110,\n",
              " 'ل': 1294,\n",
              " '##achi': 21046,\n",
              " 'harvest': 11203,\n",
              " 'spinal': 16492,\n",
              " 'ricardo': 13559,\n",
              " '##མ': 29968,\n",
              " '##iac': 20469,\n",
              " '加': 1779,\n",
              " 'protocol': 8778,\n",
              " 'disaster': 7071,\n",
              " 'avenge': 24896,\n",
              " 'aura': 15240,\n",
              " '1604': 28754,\n",
              " 'lyrical': 16376,\n",
              " 'successively': 24288,\n",
              " '[unused926]': 931,\n",
              " 'strength': 3997,\n",
              " 'ship': 2911,\n",
              " 'fort': 3481,\n",
              " 'attacking': 7866,\n",
              " '[unused169]': 174,\n",
              " 'abducted': 20361,\n",
              " 'faculty': 4513,\n",
              " 'jim': 3958,\n",
              " 'entropy': 23077,\n",
              " '[unused947]': 952,\n",
              " 'dvd': 4966,\n",
              " 'gustave': 27973,\n",
              " '##quet': 12647,\n",
              " 'severed': 16574,\n",
              " 'tracked': 12808,\n",
              " 'outlined': 14801,\n",
              " 'comical': 29257,\n",
              " 'recognised': 7843,\n",
              " 'deity': 12764,\n",
              " '##事': 30277,\n",
              " '##hort': 27794,\n",
              " 'saw': 2387,\n",
              " 'balance': 5703,\n",
              " 'perfectly': 6669,\n",
              " 'airborne': 10519,\n",
              " 'marek': 29318,\n",
              " 'beasts': 15109,\n",
              " 'mack': 11349,\n",
              " 'ski': 8301,\n",
              " 'flotilla': 17150,\n",
              " 'bertram': 27515,\n",
              " '##75': 23352,\n",
              " 'handcuffs': 28338,\n",
              " '##eley': 22352,\n",
              " 'queen': 3035,\n",
              " '##gas': 12617,\n",
              " 'protections': 28548,\n",
              " '##ractive': 26884,\n",
              " 'mail': 5653,\n",
              " '1916': 4947,\n",
              " 'mca': 22432,\n",
              " 'emil': 16243,\n",
              " 'filippo': 28669,\n",
              " 'santana': 21158,\n",
              " '##ity': 3012,\n",
              " '↦': 1588,\n",
              " '##saka': 29289,\n",
              " '505': 28952,\n",
              " 'ruiz': 18773,\n",
              " 'worthy': 11007,\n",
              " '[unused959]': 964,\n",
              " 'lets': 11082,\n",
              " 'prevailed': 19914,\n",
              " 'lithuania': 9838,\n",
              " 'certification': 10618,\n",
              " '2d': 14134,\n",
              " '##het': 27065,\n",
              " 'corridor': 7120,\n",
              " 'drums': 3846,\n",
              " 'sixth': 4369,\n",
              " 'careful': 6176,\n",
              " '171': 18225,\n",
              " 'commuter': 14334,\n",
              " 'relief': 4335,\n",
              " 'jay': 6108,\n",
              " 'lengths': 10742,\n",
              " 'singleton': 28159,\n",
              " 'cantonese': 22241,\n",
              " 'soul': 3969,\n",
              " 'ua': 25423,\n",
              " '♭': 1627,\n",
              " 'travelled': 7837,\n",
              " 'avon': 16131,\n",
              " 'nicknamed': 9919,\n",
              " 'boiling': 16018,\n",
              " '[unused911]': 916,\n",
              " 'stresses': 23253,\n",
              " 'greenwich': 13861,\n",
              " 'shattered': 10909,\n",
              " 'stefano': 19618,\n",
              " 'coarse': 20392,\n",
              " 'secretive': 28607,\n",
              " 'accused': 5496,\n",
              " 'statistical': 7778,\n",
              " 'smashwords': 25151,\n",
              " 'conservatoire': 29233,\n",
              " 'airport': 3199,\n",
              " 'harassed': 28186,\n",
              " '##tream': 25379,\n",
              " 'psi': 17816,\n",
              " 'hyde': 11804,\n",
              " 'deeply': 6171,\n",
              " 'locomotives': 7830,\n",
              " 'cutler': 24975,\n",
              " '神': 1925,\n",
              " 'butterflies': 15023,\n",
              " '##cala': 25015,\n",
              " 'jaenelle': 20757,\n",
              " 'innovations': 15463,\n",
              " 'starvation': 22611,\n",
              " '海': 1902,\n",
              " '##ike': 17339,\n",
              " 'slacks': 27786,\n",
              " 'ireland': 3163,\n",
              " 'boxer': 10423,\n",
              " 'loki': 24143,\n",
              " 'łodz': 17814,\n",
              " '##ingly': 15787,\n",
              " 'thoroughbred': 18359,\n",
              " '##pl': 24759,\n",
              " 'robe': 11111,\n",
              " 'fisher': 8731,\n",
              " 'xiii': 15031,\n",
              " 'illustrate': 19141,\n",
              " 'manipulate': 17708,\n",
              " 'fastened': 24009,\n",
              " '##bies': 20536,\n",
              " 'campaigns': 8008,\n",
              " 'fourteenth': 15276,\n",
              " '[unused354]': 359,\n",
              " 'noble': 7015,\n",
              " 'pr': 10975,\n",
              " '1702': 26776,\n",
              " 'olds': 19457,\n",
              " 'findings': 9556,\n",
              " '##aus': 20559,\n",
              " 'boundary': 6192,\n",
              " 'sharon': 10666,\n",
              " 'shi': 11895,\n",
              " 'conceptual': 17158,\n",
              " 'evolve': 19852,\n",
              " '##ła': 22972,\n",
              " 'huffed': 25014,\n",
              " '##slow': 26338,\n",
              " '##ே': 29934,\n",
              " '##བ': 29967,\n",
              " '##ᄋ': 29999,\n",
              " 'mill': 4971,\n",
              " '[unused308]': 313,\n",
              " 'adverse': 15316,\n",
              " 'gunshot': 22077,\n",
              " 'cars': 3765,\n",
              " 'brandy': 17951,\n",
              " 'visionary': 28036,\n",
              " 'miss': 3335,\n",
              " 'snowfall': 26043,\n",
              " 'poison': 9947,\n",
              " 'smelling': 19773,\n",
              " 'carrot': 25659,\n",
              " 'differentiation': 20582,\n",
              " 'volatile': 20606,\n",
              " '##ym': 24335,\n",
              " 'relax': 9483,\n",
              " 'evening': 3944,\n",
              " 'philosophy': 4695,\n",
              " 'ad': 4748,\n",
              " 'peat': 23366,\n",
              " 'withdrawing': 21779,\n",
              " 'tramway': 17050,\n",
              " '##福': 30452,\n",
              " 'expressions': 11423,\n",
              " '##grants': 27444,\n",
              " 'bce': 10705,\n",
              " '⁺': 1544,\n",
              " 'hansen': 13328,\n",
              " 'bautista': 27845,\n",
              " 'accelerator': 23468,\n",
              " 'tallest': 13747,\n",
              " 'electrode': 28688,\n",
              " 'streaked': 21276,\n",
              " 'eyre': 26975,\n",
              " 'allows': 4473,\n",
              " 'advocated': 11886,\n",
              " 'willis': 12688,\n",
              " '[unused122]': 127,\n",
              " 'rms': 29311,\n",
              " '##tation': 12516,\n",
              " 'barbara': 6437,\n",
              " 'meets': 6010,\n",
              " 'perpendicular': 19581,\n",
              " '##iface': 29164,\n",
              " '##mma': 14760,\n",
              " 'schemes': 11683,\n",
              " '[unused912]': 917,\n",
              " 'flourished': 17893,\n",
              " 'movie': 3185,\n",
              " 'merit': 7857,\n",
              " 'chu': 14684,\n",
              " 'bum': 26352,\n",
              " '##mian': 20924,\n",
              " 'caden': 23600,\n",
              " 'felony': 24648,\n",
              " 'soaked': 13077,\n",
              " 'า': 1422,\n",
              " 'clung': 14752,\n",
              " 'shorts': 9132,\n",
              " '##cer': 17119,\n",
              " '##nified': 25201,\n",
              " 'whimpered': 26184,\n",
              " 'types': 4127,\n",
              " 'haiti': 12867,\n",
              " '##sitor': 28307,\n",
              " 'eva': 9345,\n",
              " 'katherine': 9477,\n",
              " 'resigned': 5295,\n",
              " 'maintains': 9319,\n",
              " 'arise': 13368,\n",
              " '221': 19594,\n",
              " 'husband': 3129,\n",
              " '##ر': 17149,\n",
              " 'jogging': 28233,\n",
              " 'vintage': 13528,\n",
              " 'emerging': 8361,\n",
              " 'warship': 21905,\n",
              " 'substantive': 27737,\n",
              " 'appears': 3544,\n",
              " 'scott': 3660,\n",
              " 'spared': 16891,\n",
              " '##jak': 18317,\n",
              " 'invaded': 10836,\n",
              " 'reviewer': 12027,\n",
              " 'sac': 17266,\n",
              " '##wife': 19993,\n",
              " 'dyke': 22212,\n",
              " 'infectious': 16514,\n",
              " 'rebellious': 22614,\n",
              " 'vaccines': 28896,\n",
              " 'identifies': 14847,\n",
              " 'coached': 8868,\n",
              " 'butte': 25024,\n",
              " '##ize': 4697,\n",
              " 'storyline': 9994,\n",
              " 'es': 9686,\n",
              " '♦': 1626,\n",
              " 'unnecessary': 14203,\n",
              " '[unused414]': 419,\n",
              " '##over': 7840,\n",
              " 'torch': 12723,\n",
              " '##hol': 14854,\n",
              " '##tag': 15900,\n",
              " '##itate': 17570,\n",
              " 'myrtle': 21381,\n",
              " 'wages': 12678,\n",
              " 'ramirez': 15206,\n",
              " '##imeters': 28136,\n",
              " '[unused203]': 208,\n",
              " 'excursion': 26144,\n",
              " 'larvae': 9673,\n",
              " 'regulate': 15176,\n",
              " 'mysore': 20761,\n",
              " 'slicing': 26514,\n",
              " 'condemn': 28887,\n",
              " 'ballistic': 19630,\n",
              " 'propulsion': 16404,\n",
              " '##pic': 24330,\n",
              " 'flaws': 21407,\n",
              " ...}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = tok_obj.get_vocab()\n",
        "vocab"
      ],
      "id": "GURBhwiWaw8m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you see anything weird? There are plenty of words starting with \"##\". Hold on to that thought, we'll understand their role shortly.\n",
        "\n",
        "The vocabulary is going to be used by the tokenizer's model to map words into their corresponding indices, so strings are translated into integers.\n",
        "\n",
        "Let's try fetching the index of a given word from our example:"
      ],
      "metadata": {
        "id": "IjBgVHlaid3_"
      },
      "id": "IjBgVHlaid3_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQudbOzeaw8m",
        "outputId": "cd3903d8-a20f-4b5e-80a7-d9e027c47258"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'dwindling'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdwindling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dwindling'"
          ]
        }
      ],
      "source": [
        "vocab['dwindling']"
      ],
      "id": "dQudbOzeaw8m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "How could it be? \"Dwindling\" isn't such a rare word, after all.\n",
        "\n",
        "Let's see how large our vocabulary is:"
      ],
      "metadata": {
        "id": "fJhonUFWik53"
      },
      "id": "fJhonUFWik53"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieoZKvxtaw8m",
        "outputId": "e8d774a1-de9e-40b3-ef0f-a540a8d4dd60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tok_obj.get_vocab_size()"
      ],
      "id": "ieoZKvxtaw8m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doesn't the vocabulary length look suspiciously short? It's no wonder that the very first sentence of our example already contained a word that is _not_ in the vocabulary.\n",
        "\n",
        "In the past, that word would be removed and deemed an \"unknown\" token. Vocabularies had to grow up to 400,000 words in length to avoid that situation as much as possible. Modern-day tokenizers, on the other hand, tackle the challenge in a different way: they implement an algorithm that maximizes coverage while keeping the vocabulary short and manageable. Enter the tokenizer's model, the third step in the pipeline."
      ],
      "metadata": {
        "id": "zpcp-8JQip0U"
      },
      "id": "zpcp-8JQip0U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ba2a34"
      },
      "source": [
        "### 15.5.2 Tokenizer's Model\n",
        "\n",
        "The basic naïve tokenizer is simple and straightforward, but it has a drawback: it either makes the vocabulary huge (to include rare words), or it leads to a lot of unknown tokens (that will replace words absent from our vocabulary).\n",
        "\n",
        "Ideally, we would be able to take any word that's thrown at us (at our vocabulary, that is) and handle it without resorting to the unknown token while keeping the size of the vocabulary manageable and limited. Seems too good to be true? Not really!\n",
        "\n",
        "The general idea of modern tokenizers is to break words into their components, so the vocabulary is actually made of the \"building blocks\", or subwords, used to assemble (almost) any word we may find. Let's say our vocabulary has only a few entries: \"any\", \"some\", \"where\", \"how\", and \"body\". These are all (full) words, but they can be used as parts to build many other words: \"anyhow\", \"anybody\", \"somehow\", \"somebody\", \"anywhere\", and \"somewhere\". We covered 11 words using a vocabulary of only five entries. Some words like \"some\" may be used to modify others, like \"awesome\", \"wholesome\", etc. Besides, we are also not limited to using full words, we can add prefixes and suffixes to the vocabulary, such as \"ly\", for \"commonly\", \"ordinarily\", and so on.\n",
        "\n",
        "The choice between keeping the full word in a vocabulary or breaking it into smaller pieces is made according to how common or rare the word is, and the algorithm being used. There are a few different algorithms, such as WordPiece, Byte-Pair Encoding (BPE), and Byte-Level BPE, which are used by BERT, GPT, and CLIP models, respectively.\n",
        "\n",
        "For more details on sub-word tokenizers such as WordPiece, Byte-Pair Encoding (BPE), and SentencePiece, please check Hugging Face’s \"[Summary of the Tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary)\" and Cathal Horan’s great post \"Tokenizers: How machines read\" on FloydHub.\n",
        "\n",
        "In a nutshell, it is the tokenizer's own internal model (yes, you can train a tokenizer!) that learns how to split a complex or unusual token/word into smaller components.\n",
        "\n",
        "BERT's tokenizer model was trained using the WordPiece algorithm:"
      ],
      "id": "91ba2a34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky5gyWV4aw8n",
        "outputId": "c5807817-d32f-49c7-9e3e-eb9a4538d954"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tokenizers.models.WordPiece at 0x7f2c4cf87890>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tok_obj.model"
      ],
      "id": "Ky5gyWV4aw8n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we call the model's token_to_id() method, it will simply look the token up in the vocabulary and, if it is not a valid key, the method returns None instead of the token's index:"
      ],
      "metadata": {
        "id": "CYdD6vnQkJNt"
      },
      "id": "CYdD6vnQkJNt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKPi3CK9aw8n",
        "outputId": "58f9d5bd-5948-408a-92e0-d1e73c125de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['reuters', '-', 'short', '-', 'sellers', ',', 'wall', 'street', \"'\", 's', 'dwindling', 'band', 'of', 'ultra', '-', 'cynics', ',', 'are', 'seeing', 'green', 'again', '.']\n",
            "[26665, 1011, 2460, 1011, 19041, 1010, 2813, 2395, 1005, 1055, None, 2316, 1997, 11087, 1011, None, 1010, 2024, 3773, 2665, 2153, 1012]\n"
          ]
        }
      ],
      "source": [
        "tokens_only = [token[0] for token in tokens]\n",
        "token_ids = [tok_obj.model.token_to_id(token) for token in tokens_only]\n",
        "print(tokens_only)\n",
        "print(token_ids)"
      ],
      "id": "LKPi3CK9aw8n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've seen before, \"dwindling\" is not part of the vocabulary, so its token ID is missing in the list above."
      ],
      "metadata": {
        "id": "LICiu5L1kOT8"
      },
      "id": "LICiu5L1kOT8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiv7EeHNaw8n",
        "outputId": "1512fdf7-1789-4d4c-97e5-0c665d3ded19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 'dwindling')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "missing_id = token_ids.index(None)\n",
        "missing_token = tokens_only[missing_id]\n",
        "missing_id, missing_token"
      ],
      "id": "uiv7EeHNaw8n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, the second step in the pipeline was called pre-tokenization for a reason! Some tokens, those that are not keys in the vocabulary dictionary, need to be further tokenized into smaller components. We can call the mode's tokenize() method on the missing token to handle this situation:"
      ],
      "metadata": {
        "id": "pZwFtOEYkSq7"
      },
      "id": "pZwFtOEYkSq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zt5bB-Saw8n",
        "outputId": "1f23fa86-dd61-44a5-ff7c-c64971c0af9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1040, 'd', (0, 1)), (11101, '##wind', (1, 5)), (2989, '##ling', (5, 9))]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_word = tok_obj.model.tokenize(missing_token)\n",
        "[piece.as_tuple() for piece in tokenized_word]"
      ],
      "id": "1Zt5bB-Saw8n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The (pre-)token was decomposed into three parts, \"d\", \"##wind\", and \"##ling\", where the \"##\" prefix denotes tokens that are not whole words themselves, but rather parts of a word.\n",
        "\n",
        "The tokenizer's encode() method does that automatically, though, as we can see in the example below:"
      ],
      "metadata": {
        "id": "tfmsk006kXj6"
      },
      "id": "tfmsk006kXj6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmfwWIghaw8n",
        "outputId": "f75e3d12-a65a-48c9-a862-e23bdced9b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['reuters', '-', 'short', '-', 'sellers', ',', 'wall', 'street', \"'\", 's', 'd', '##wind', '##ling', 'band', 'of', 'ultra', '-', 'cy', '##nic', '##s', ',', 'are', 'seeing', 'green', 'again', '.']\n",
            "[26665, 1011, 2460, 1011, 19041, 1010, 2813, 2395, 1005, 1055, 1040, 11101, 2989, 2316, 1997, 11087, 1011, 22330, 8713, 2015, 1010, 2024, 3773, 2665, 2153, 1012]\n"
          ]
        }
      ],
      "source": [
        "encoded = tok_obj.encode(descriptions[0], add_special_tokens=False)\n",
        "print(encoded.tokens)\n",
        "print(encoded.ids)"
      ],
      "id": "xmfwWIghaw8n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you notice that we explicitly set the add_special_tokens argument to False? Let's discuss these tokens in the next section."
      ],
      "metadata": {
        "id": "N8yIhnRmkdRX"
      },
      "id": "N8yIhnRmkdRX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "296731d1"
      },
      "source": [
        "### 15.5.3 Special Tokens\n",
        "\n",
        "The last step of the tokenizer's pipeline, the postprocessor, is the one responsible for prepending and appending special tokens to the tokenized inputs. Let's see what it does to the encoded sentence we got as a result of the previous step:"
      ],
      "id": "296731d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT8dFXCeaw8o",
        "outputId": "c8bb9b16-2381-4566-f906-19f854a1b752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'reuters', '-', 'short', '-', 'sellers', ',', 'wall', 'street', \"'\", 's', 'd', '##wind', '##ling', 'band', 'of', 'ultra', '-', 'cy', '##nic', '##s', ',', 'are', 'seeing', 'green', 'again', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "post_processor = tok_obj.post_processor\n",
        "post_encoded = post_processor.process(encoded)\n",
        "print(post_encoded.tokens)"
      ],
      "id": "DT8dFXCeaw8o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the way, the output is exactly what we would have obtained from the tokenizer's encode() method if we hadn't turned the special tokens off:"
      ],
      "metadata": {
        "id": "LQfql7hxnZtQ"
      },
      "id": "LQfql7hxnZtQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAO7Ug_Law8o",
        "outputId": "77c67ffa-1629-4e9f-fa5a-bf728969c459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'reuters', '-', 'short', '-', 'sellers', ',', 'wall', 'street', \"'\", 's', 'd', '##wind', '##ling', 'band', 'of', 'ultra', '-', 'cy', '##nic', '##s', ',', 'are', 'seeing', 'green', 'again', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "print(tok_obj.encode(descriptions[0]).tokens)"
      ],
      "id": "DAO7Ug_Law8o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two special tokens in the sentence above: the classification ([CLS]) token and the separation ([SEP]) token. Let's take a closer look at both of them."
      ],
      "metadata": {
        "id": "DpngGbObndWL"
      },
      "id": "DpngGbObndWL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deeb46f0"
      },
      "source": [
        "#### 15.5.3.1 `[CLS]`: Classification Token\n",
        "\n",
        "The classification ([CLS]) is a \"very\" special token that is prepended to the sequence. Unlike the other special tokens, which are primarily used to define the boundaries of a sequence, the classification token can also be used as a type of \"summary\" of the whole sequence in classification tasks (hence its name). We can retrieve the token and its corresponding ID using the cls_token and cls_token_id attributes of the tokenizer:"
      ],
      "id": "deeb46f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21803090",
        "outputId": "cbaa3cb2-0486-4250-e8b8-14fc9a794741"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('[CLS]', 101)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.cls_token, tokenizer.cls_token_id"
      ],
      "id": "21803090"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fb5ea5b"
      },
      "source": [
        "#### 15.5.3.2 `[SEP]`: Separation Token\n",
        "\n",
        "The [SEP] token is used to either separate two sentences or to mark the end of a single sentence. We can retrieve the token and its corresponding ID using the sep_token and sep_token_id attributes of the tokenizer:"
      ],
      "id": "8fb5ea5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f48c2bb",
        "outputId": "01a877a1-0370-44e7-d9d1-fba554f629d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('[SEP]', 102)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.sep_token, tokenizer.sep_token_id"
      ],
      "id": "5f48c2bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the [SEP] token is used for both separating the two sentences and marking the end of the whole sequence:"
      ],
      "metadata": {
        "id": "Ig-gFIPHn4O4"
      },
      "id": "Ig-gFIPHn4O4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a9772d4",
        "outputId": "98c75730-ce79-41bc-ad50-56ecfb1109b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'reuters', '-', 'short', '-', 'sellers', ',', 'wall', 'street', \"'\", 's', 'd', '##wind', '##ling', 'band', 'of', 'ultra', '-', 'cy', '##nic', '##s', ',', 'are', 'seeing', 'green', 'again', '.', '[SEP]', 'reuters', '-', 'private', 'investment', 'firm', 'carly', '##le', 'group', ',', 'which', 'has', 'a', 'reputation', 'for', 'making', 'well', '-', 'timed', 'and', 'occasionally', 'controversial', 'plays', 'in', 'the', 'defense', 'industry', ',', 'has', 'quietly', 'placed', 'its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "print(tok_obj.encode(*descriptions[:2]).tokens)"
      ],
      "id": "4a9772d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eedf8f81"
      },
      "source": [
        "#### 15.5.3.3 `[UNK]`: Unknown Token\n",
        "\n",
        "The [UNK] token, once a common fixture of tokenized sequences, rarely shows up anymore. The third step in the pipeline, the tokenizer's model, handles these cases and splits them further into smaller tokens that are present in the vocabulary.\n",
        "\n",
        "We can retrieve the token and its corresponding ID using the unk_token and unk_token_id attributes of the tokenizer:"
      ],
      "id": "eedf8f81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03e2c584",
        "outputId": "5246d723-301b-4aaa-eb7a-788f4236712a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('[UNK]', 100)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.unk_token, tokenizer.unk_token_id"
      ],
      "id": "03e2c584"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9dc0b2c"
      },
      "source": [
        "#### 15.5.3.4 `[PAD]`: Padding Token\n",
        "\n",
        "The [PAD] token is used to pad (or stuff) sequences so their lengths match. We can retrieve the token and its corresponding ID using the pad_token and pad_token_id attributes of the tokenizer:"
      ],
      "id": "e9dc0b2c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOKQEn5jaw8t",
        "outputId": "319a9bb1-f937-4def-a5b7-3c30646ba3cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('[PAD]', 0)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token, tokenizer.pad_token_id"
      ],
      "id": "TOKQEn5jaw8t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our mini-batch has four data points, and each data point has a description containing a different number of tokens:"
      ],
      "metadata": {
        "id": "pdJBfbjuoGbj"
      },
      "id": "pdJBfbjuoGbj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcqaH4gfaw8u",
        "outputId": "a709f103-a6c0-46c4-e313-97cd5d366bc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[28, 41, 38, 34]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[len(seq) for seq in tokenizer(descriptions)['input_ids']]"
      ],
      "id": "pcqaH4gfaw8u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot make tensors out of list of different lengths, so we need to set the padding argument to True to make every sequence the same length as the longest one:"
      ],
      "metadata": {
        "id": "QcLup4DWoKaH"
      },
      "id": "QcLup4DWoKaH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sekVM2Vvaw8u",
        "outputId": "98ab7c0d-ee97-43a5-f3c1-557fb378c43e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101, 26665,  1011,  2460,  1011, 19041,  1010,  2813,  2395,  1005,\n",
              "          1055,  1040, 11101,  2989,  2316,  1997, 11087,  1011, 22330,  8713,\n",
              "          2015,  1010,  2024,  3773,  2665,  2153,  1012,   102,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0],\n",
              "        [  101, 26665,  1011,  2797,  5211,  3813, 18431,  2571,  2177,  1010,\n",
              "          2029,  2038,  1037,  5891,  2005,  2437,  2092,  1011, 22313,  1998,\n",
              "          5681,  6801,  3248,  1999,  1996,  3639,  3068,  1010,  2038,  5168,\n",
              "          2872,  2049, 29475,  2006,  2178,  2112,  1997,  1996,  3006,  1012,\n",
              "           102],\n",
              "        [  101, 26665,  1011, 23990, 13587,  7597,  4606, 15508,  2055,  1996,\n",
              "          4610,  1998,  1996, 17680,  2005, 16565,  2024,  3517,  2000,  6865,\n",
              "          2058,  1996,  4518,  3006,  2279,  2733,  2076,  1996,  5995,  1997,\n",
              "          1996,  2621,  2079,  6392,  6824,  2015,  1012,   102,     0,     0,\n",
              "             0],\n",
              "        [  101, 26665,  1011,  4614,  2031, 12705,  3514,  9167,  6223,  2013,\n",
              "          1996,  2364, 13117,  1999,  2670,  5712,  2044,  4454,  3662,  1037,\n",
              "          8443,  8396,  2071,  4894,  6502,  1010,  2019,  3514,  2880,  2056,\n",
              "          2006,  5095,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
              "             0]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_token_ids = tokenizer(descriptions, padding=True, return_tensors='pt')['input_ids']\n",
        "padded_token_ids"
      ],
      "id": "sekVM2Vvaw8u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41c30064"
      },
      "source": [
        "### 15.5.4 Truncation\n",
        "\n",
        "What happens if a sentence is too long? Models can only take so many tokens as inputs, so we may need to truncate our sequences to the maximum length taken by our model.\n",
        "\n",
        "But, if a sequence is too long, a [SEP] token is appended to its end, and it will be truncated! Therefore, the first thing the tokenizer needs to do is to truncate our sequences to a length of two tokens shorter than the maximum length supported by our model.\n",
        "\n",
        "That's why the tokenizer max_len_single_sentence is two tokens shorter than the model_max_length:"
      ],
      "id": "41c30064"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95e0ec0f",
        "outputId": "69e95378-8982-475d-dca2-2d58ec08c6d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(510, 512)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.max_len_single_sentence, tokenizer.model_max_length"
      ],
      "id": "95e0ec0f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only after truncating the sentence, it can proceed to prepend and append the two special tokens, [CLS] and [SEP], respectively, and, if needed, pad the sequences."
      ],
      "metadata": {
        "id": "sDPd20OBoTrs"
      },
      "id": "sDPd20OBoTrs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59092fde"
      },
      "source": [
        "## 15.6 Embeddings\n",
        "\n",
        "Tokenizers are fun and all, but they are only the opening act.\n",
        "\n",
        "Let us introduce you to the main star of the NLP world: \"Embeddings\". Embeddings are at the front and center of everything we'll be doing in this course from now on. We can use them to perform text classification, semantic search, clustering, you name it.\n",
        "\n",
        "We have already touched upon the topic of embeddings a few times in this course. First, we used them to convert categorical features (from the Auto MPG dataset, remember that?) into numerical ones. Then, we used them once again while fine-tuning RoBERTa to perform sentiment analysis. Finally, we briefly talked about them while discussing the powerful CLIP model, which bridges the gap between the worlds of image and text by using, guess what, embeddings!\n",
        "\n",
        "So, as you can see, embeddings are everywhere! But, what is exactly an embedding? If we're talking about an embedding layer in PyTorch, it works just like a lookup table. We could, for example, create an embedding to handle our vocabulary of roughly 80,000 tokens, assigning a tensor for each and every token.\n",
        "\n",
        "Each token gets converted to an index, and that index is then used to look the corresponding tensor up in the embedding layer:"
      ],
      "id": "59092fde"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97f75748",
        "outputId": "58153604-36f7-4075-a558-e04dcf1aacaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(30522, 50)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "emb_dims = 50\n",
        "embeddings = nn.Embedding(len(vocab), emb_dims)\n",
        "embeddings"
      ],
      "id": "97f75748"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it out:"
      ],
      "metadata": {
        "id": "isoKIsvNpUay"
      },
      "id": "isoKIsvNpUay"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "553745ac",
        "outputId": "eb6e49b4-c044-4f23-f6a0-ce544957b0a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([26665]),\n",
              " tensor([[ 1.8288e-01, -1.3801e+00, -5.3830e-01, -1.4301e-01, -2.0827e-01,\n",
              "          -1.7362e+00,  1.0018e+00,  4.6152e-01,  3.2680e-02, -5.8854e-01,\n",
              "           3.1180e-01,  6.0066e-01, -1.2477e-01, -1.1660e+00, -1.2219e+00,\n",
              "           1.0182e+00, -2.0216e-01, -3.1973e-01, -5.4026e-01, -1.8794e+00,\n",
              "           2.2819e-01,  2.7748e-01, -1.4689e-01, -9.8170e-01, -2.1549e+00,\n",
              "           4.9118e-01, -4.7388e-01, -2.3673e-01,  2.1740e-03,  1.1351e-01,\n",
              "          -1.0422e+00,  1.4274e+00, -9.8884e-02, -7.2925e-01, -3.3722e-01,\n",
              "           4.6264e-01, -3.8414e-01, -9.5412e-01, -5.6739e-02,  2.9316e+00,\n",
              "           1.2275e-01,  5.6614e-01,  1.0147e-01, -8.2784e-01,  1.8933e-01,\n",
              "          -8.5093e-01, -3.1484e-01, -1.7159e+00,  6.4275e-01,  2.7018e+00]],\n",
              "        grad_fn=<EmbeddingBackward0>))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "idx = torch.as_tensor([vocab['reuters']])\n",
        "idx, embeddings(idx)"
      ],
      "id": "553745ac"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The token \"reuters\" corresponds to index 26,665, and there it is the corresponding tensor.\n",
        "\n",
        "Notice that this tensor is the i-th row in the tensor that represents the weights of the embedding layer:"
      ],
      "metadata": {
        "id": "ieEeL09KpZTr"
      },
      "id": "ieEeL09KpZTr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24bd2513",
        "outputId": "31006559-8042-49ea-c32f-85599ce0ec7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.8288e-01, -1.3801e+00, -5.3830e-01, -1.4301e-01, -2.0827e-01,\n",
              "         -1.7362e+00,  1.0018e+00,  4.6152e-01,  3.2680e-02, -5.8854e-01,\n",
              "          3.1180e-01,  6.0066e-01, -1.2477e-01, -1.1660e+00, -1.2219e+00,\n",
              "          1.0182e+00, -2.0216e-01, -3.1973e-01, -5.4026e-01, -1.8794e+00,\n",
              "          2.2819e-01,  2.7748e-01, -1.4689e-01, -9.8170e-01, -2.1549e+00,\n",
              "          4.9118e-01, -4.7388e-01, -2.3673e-01,  2.1740e-03,  1.1351e-01,\n",
              "         -1.0422e+00,  1.4274e+00, -9.8884e-02, -7.2925e-01, -3.3722e-01,\n",
              "          4.6264e-01, -3.8414e-01, -9.5412e-01, -5.6739e-02,  2.9316e+00,\n",
              "          1.2275e-01,  5.6614e-01,  1.0147e-01, -8.2784e-01,  1.8933e-01,\n",
              "         -8.5093e-01, -3.1484e-01, -1.7159e+00,  6.4275e-01,  2.7018e+00]],\n",
              "       grad_fn=<IndexBackward0>)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings.weight[idx]"
      ],
      "id": "24bd2513"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect match; after all, the embedding layer works as a lookup table to its own weights.\n",
        "\n",
        "Of course, the values we got now are completely meaningless. As with every other layer, its weights (the embeddings) were randomly initialized.\n",
        "\n",
        "In the non-linear regression model for the Auto MPG Dataset, the embedding layers representing the categorical features got trained together with the rest of the model so, in the end, our model learned how to represent those features numerically.\n",
        "\n",
        "When it comes to words, though, it is a much more daunting task. There are, literally, hundreds of thousands of unique words in English. But, instead of trying to accomplish this ourselves, let's stand on the shoulders of giants and use good old pretrained word embeddings."
      ],
      "metadata": {
        "id": "lAKYHWfQpdeA"
      },
      "id": "lAKYHWfQpdeA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc731723"
      },
      "source": [
        "### 15.6.1 Word2Vec\n",
        "\n",
        "It wasn't practical to use one-hot encoding with a vocabulary of hundreds of thousands of unique words. Moreover, OHE vectors are orthogonal by nature, thus making every word completely independent of all others. That's not how languages work: there are synonyms and antonyms, and words may be modified by their relationship to other dimensions such as gender, for example. \"King\" and \"Queen\" are related words, since they represent royals, respectively, a man and a woman. They shouldn't be orthogonal, there is, their numerical representations should be comparable and more similar to one another than, for example, \"King\" and \"Jester\" or \"King\" and \"Horse\". That's the general idea behind Word2Vec, proposed in 2013.\n",
        "\n",
        "In the continuous-bag-of-words (CBoW) architecture used to train the embeddings, the target was to predict the central word in a sentence, using words both before and after it as context. For example, in the sentence \"Yesterday, the Queen was crowned\", \"yesterday\" \"the\", \"was\", and \"crowned\" are the context around the central word, \"Queen\". Being crowned is a typical thing for both Queens and Kings, but not so much for anyone else, right? So, the model should quickly learn that those two words, and likely only those, are good candidates for the central word. \"Yesterday, the Aardvark was crowned\" isn't gonna cut it! By extensively training the model over lots and lots of text, it will eventually figure out appropriate numerical representations for the words.\n",
        "\n",
        "It has been (only?) a decade, but everyone marveled at the time by the possibilities of doing something rather impressive: embedding arithmetic."
      ],
      "id": "dc731723"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fd37adf"
      },
      "source": [
        "### 15.6.2 Embedding Arithmetic\n",
        "\n",
        "Embeddings are vectors, and you can easily add or subtract them from one another. What happens if you start using words as variables in an equation?\n",
        "\n",
        "**KING - MAN + WOMAN = ?**\n",
        "\n",
        "In theory, since embeddings learned how to encode abstract dimensions, \"KING - MAN\" should result in the vector that represents \"royalty\". If you add \"WOMAN\" to it, you should get a royal woman, that is, a \"QUEEN\".\n",
        "\n",
        "**KING - MAN + WOMAN = QUEEN**\n",
        "\n",
        "The figure below is a two-dimensional hypothetical representation of these relationships, in embedding space, among these four words, king, queen, man, and woman (in the figure below, 'w' stands for 'weight' since embeddings are weights in an embedding layer):\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch13/embed_arithmetic.png)"
      ],
      "id": "3fd37adf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arithmetic with kings and queens"
      ],
      "metadata": {
        "id": "ERxcSrZDp3ZF"
      },
      "id": "ERxcSrZDp3ZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0da8bb0"
      },
      "source": [
        "\n",
        "\n",
        "In its large 50-dimensional feature space, the model learned to place \"man\" as far apart from \"woman\" as \"king\" is from \"queen\" (roughly approximating the gender difference between the two). Similarly, the model learned to place \"king\" as far apart from \"man\" as \"queen\" is from \"woman\" (roughly approximating the difference of being a royal).\n",
        "\n",
        "In practice, if you try out the equation above using some pretrained word embeddings (such as GloVe, covered in the next section), you'll see that the word that's actually most similar to the result is \"King\" itself, not \"Queen\".\n",
        "\n",
        "**KING - MAN + WOMAN ~ KING**\n",
        "\n",
        "This may happen because the embedding space is very sparse (that is, each word is \"far apart\" from every word). For this reason, it is usual to exclude the original word from the results. In this particular case, the runner-up is, in fact, \"Queen\".\n",
        "\n",
        "Here is an illustration of the vectors involved in the computation. Of course, it's impossible to tell, visually, if the \"synthetic Queen\" (the result of the equation) is more similar to the vector representing \"King\" or that representing \"Queen\".\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch13/synthetic_queen.png)"
      ],
      "id": "a0da8bb0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synthetic Queen"
      ],
      "metadata": {
        "id": "HiogqqrjqEu-"
      },
      "id": "HiogqqrjqEu-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we're not adding and subtracting words, but the fact that these embeddings can capture the relationship between different words (as opposed to being orthogonal as one-hot encoded vectors) makes them quite useful in tasks such as sentiment analysis or text classification in general."
      ],
      "metadata": {
        "id": "qguy4xWpqH5h"
      },
      "id": "qguy4xWpqH5h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bee9d13"
      },
      "source": [
        "### 15.6.3 Global Vectors (GloVe)"
      ],
      "id": "2bee9d13"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stanford Global Vectors (GloVe) is one of the most successful pretrained word embeddings. It represented a great leap forward back in 2014, in the BT (Before Transformers) era, when it was released. They are simple and straightforward to use, and they can deliver very good results in tasks such as text classification, as we'll see very soon.\n",
        "\n",
        "They can be retrieved using Gensim's downloader. We'll be loading the 50-dimension version:"
      ],
      "metadata": {
        "id": "nUE3TbX6qSTD"
      },
      "id": "nUE3TbX6qSTD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NdAeqMpaw8v"
      },
      "outputs": [],
      "source": [
        "from gensim import downloader\n",
        "\n",
        "vec = downloader.load('glove-wiki-gigaword-50')"
      ],
      "id": "1NdAeqMpaw8v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim's KeyedVectors class, which the downloaded GloVe is an instance of, has plenty of methods to retrieve and compare embeddings. Moreover, we can easily peek at its internal vectors using its vectors attribute:"
      ],
      "metadata": {
        "id": "I3H4k-58qW19"
      },
      "id": "I3H4k-58qW19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f31dc5dc",
        "outputId": "65400271-5f07-412f-bd54-fc7a4be92f50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[ 0.418   ,  0.24968 , -0.41242 , ..., -0.18411 , -0.11514 ,\n",
              "         -0.78581 ],\n",
              "        [ 0.013441,  0.23682 , -0.16899 , ..., -0.56657 ,  0.044691,\n",
              "          0.30392 ],\n",
              "        [ 0.15164 ,  0.30177 , -0.16763 , ..., -0.35652 ,  0.016413,\n",
              "          0.10216 ],\n",
              "        ...,\n",
              "        [-0.51181 ,  0.058706,  1.0913  , ..., -0.25003 , -1.125   ,\n",
              "          1.5863  ],\n",
              "        [-0.75898 , -0.47426 ,  0.4737  , ...,  0.78954 , -0.014116,\n",
              "          0.6448  ],\n",
              "        [ 0.072617, -0.51393 ,  0.4728  , ..., -0.18907 , -0.59021 ,\n",
              "          0.55559 ]], dtype=float32),\n",
              " (400000, 50))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec.vectors, vec.vectors.shape"
      ],
      "id": "f31dc5dc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 400,000 entries in it, each corresponding to a token in its extensive vocabulary, each returning 50 numerical features. Let's see what the \"reuters\" embedding look like in GloVe:"
      ],
      "metadata": {
        "id": "MjR9LVLeqbTW"
      },
      "id": "MjR9LVLeqbTW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "601ace49",
        "outputId": "fef4c9c4-0b31-4df7-9b52-f19d0262b1fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.13741  , -0.25495  ,  1.8853   ,  0.1476   ,  0.63859  ,\n",
              "       -0.67678  , -1.1622   , -0.21528  ,  0.2598   , -0.52879  ,\n",
              "        0.66678  , -0.76747  , -0.52731  ,  0.06657  ,  0.076613 ,\n",
              "        0.32743  , -0.80251  , -0.4955   , -0.37393  ,  0.11261  ,\n",
              "        1.1671   ,  1.1508   ,  0.61801  ,  0.079467 ,  0.1269   ,\n",
              "       -0.072447 , -1.2037   , -0.24622  , -0.77076  ,  0.76699  ,\n",
              "        1.2745   , -0.12898  ,  0.99892  , -0.26733  , -0.57542  ,\n",
              "       -1.0151   , -0.14278  , -0.43824  ,  0.76577  , -0.0087715,\n",
              "        1.2848   ,  0.0030819,  0.1186   , -0.38817  , -0.23516  ,\n",
              "       -0.92094  , -0.51644  ,  1.5083   ,  0.36456  ,  0.59912  ],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec['reuters']"
      ],
      "id": "601ace49"
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVE embeddings, although somewhat \"old school\", are pretrained embeddings nonetheless. Therefore, we can load them into a PyTorch embedding layer, provided we convert the Numpy array into a PyTorch tensor first:"
      ],
      "metadata": {
        "id": "eFjxBHVjqfG_"
      },
      "id": "eFjxBHVjqfG_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjjrqHh6aw8v",
        "outputId": "5349d3ac-6667-421d-a163-412c772c6eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ 0.4180,  0.2497, -0.4124,  ..., -0.1841, -0.1151, -0.7858],\n",
              "                      [ 0.0134,  0.2368, -0.1690,  ..., -0.5666,  0.0447,  0.3039],\n",
              "                      [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
              "                      ...,\n",
              "                      [-0.5118,  0.0587,  1.0913,  ..., -0.2500, -1.1250,  1.5863],\n",
              "                      [-0.7590, -0.4743,  0.4737,  ...,  0.7895, -0.0141,  0.6448],\n",
              "                      [ 0.0726, -0.5139,  0.4728,  ..., -0.1891, -0.5902,  0.5556]]))])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "tensor_glove = torch.as_tensor(vec.vectors).float()\n",
        "embedding = nn.Embedding.from_pretrained(tensor_glove)\n",
        "embedding.state_dict()"
      ],
      "id": "sjjrqHh6aw8v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find out the entry corresponding to a given word, we can use the key_to_index dictionary attribute. We can also use the index_to_key attribute to get the token corresponding to a given index:"
      ],
      "metadata": {
        "id": "yPeBg-yuqjCJ"
      },
      "id": "yPeBg-yuqjCJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebdd6588",
        "outputId": "118a4a7e-49f8-49ab-fe98-f927f2923cbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10851, 'reuters')"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx = vec.key_to_index['reuters']\n",
        "token = vec.index_to_key[idx]\n",
        "idx, token"
      ],
      "id": "ebdd6588"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer is just a big lookup table, so we can use the reuters index as an argument to retrieve its corresponding embedding:"
      ],
      "metadata": {
        "id": "nciXiaO-qvfb"
      },
      "id": "nciXiaO-qvfb"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding(torch.as_tensor(idx))"
      ],
      "metadata": {
        "id": "1GhuJKYNqsjb"
      },
      "id": "1GhuJKYNqsjb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is simple and straightforward, as long as the word is part of GloVe's vocabulary.\n",
        "\n",
        "But what if we make something up?"
      ],
      "metadata": {
        "id": "5CPQTyYaqy-r"
      },
      "id": "5CPQTyYaqy-r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9da37f2",
        "outputId": "4ddf8496-1c58-4733-c3aa-b8e3197c65af"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'zzzzz'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzzzzz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'zzzzz'"
          ]
        }
      ],
      "source": [
        "vec.key_to_index['zzzzz']"
      ],
      "id": "d9da37f2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, Gensim's implementation of GloVe vectors does not handle missing words gracefully, raising an exception instead. We'll need to check ourselves whether a given word is in the vocabulary or not. In this case, we assign that token a special ID for an unknown token."
      ],
      "metadata": {
        "id": "5CZlbYCTq3Ul"
      },
      "id": "5CZlbYCTq3Ul"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYPbnnN8aw8v"
      },
      "outputs": [],
      "source": [
        "def encode_str(key_to_index, tokens, unk_token=-1):\n",
        "    token_ids = [key_to_index.get(token, unk_token) for token in tokens]\n",
        "    return token_ids"
      ],
      "id": "YYPbnnN8aw8v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the encode_str() function above, invalid tokens return -1 as their ID, for example:"
      ],
      "metadata": {
        "id": "cIVxlgwmq7Kd"
      },
      "id": "cIVxlgwmq7Kd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz0lq3rBaw8v",
        "outputId": "9c797f82-82ed-417c-c5b9-6d43182a0f8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10851, -1]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "some_ids = encode_str(vec.key_to_index, ['reuters', 'zzzzz'])\n",
        "some_ids"
      ],
      "id": "Hz0lq3rBaw8v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We still have to filter out these invalid tokens whenever we're retrieving the tokens' embeddings:"
      ],
      "metadata": {
        "id": "jybRDEG-q_Ho"
      },
      "id": "jybRDEG-q_Ho"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5r7Z1S8aw8v"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(embedding, token_ids):\n",
        "    valid_ids = torch.as_tensor([token_id for token_id in token_ids if token_id >= 0])\n",
        "    embedded_tokens = embedding(valid_ids)\n",
        "    return embedded_tokens"
      ],
      "id": "-5r7Z1S8aw8v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqCzEfDvaw8v",
        "outputId": "50e9683c-e3e4-4daa-9752-6dced553f436"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1374, -0.2549,  1.8853,  0.1476,  0.6386, -0.6768, -1.1622, -0.2153,\n",
              "          0.2598, -0.5288,  0.6668, -0.7675, -0.5273,  0.0666,  0.0766,  0.3274,\n",
              "         -0.8025, -0.4955, -0.3739,  0.1126,  1.1671,  1.1508,  0.6180,  0.0795,\n",
              "          0.1269, -0.0724, -1.2037, -0.2462, -0.7708,  0.7670,  1.2745, -0.1290,\n",
              "          0.9989, -0.2673, -0.5754, -1.0151, -0.1428, -0.4382,  0.7658, -0.0088,\n",
              "          1.2848,  0.0031,  0.1186, -0.3882, -0.2352, -0.9209, -0.5164,  1.5083,\n",
              "          0.3646,  0.5991]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_embeddings(embedding, some_ids)"
      ],
      "id": "TqCzEfDvaw8v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "See? Even though the list has two tokens, one of them is invalid, and therefore we only get one embedding back.\n",
        "\n",
        "The function builder below takes an instance of Gensim's GloVe embeddings, builds the corresponding PyTorch embedding layer, and then builds and returns the get_vecs_by_tokens() function. The resulting function, on its turn, takes a list of tokens as arguments, filters out those tokens without valid IDs in GloVe's vocabulary, and retrieves their corresponding embeddings:"
      ],
      "metadata": {
        "id": "P_XMUL5nrDs8"
      },
      "id": "P_XMUL5nrDs8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwPNgSxIaw8v"
      },
      "outputs": [],
      "source": [
        "def func_builder(vec):\n",
        "    tensor_glove = torch.as_tensor(vec.vectors).float()\n",
        "    embedding = nn.Embedding.from_pretrained(tensor_glove)\n",
        "\n",
        "    def get_vecs_by_tokens(tokens):\n",
        "        token_ids = encode_str(vec.key_to_index, tokens)\n",
        "        embedded_tokens = get_embeddings(embedding, token_ids)\n",
        "        return embedded_tokens\n",
        "\n",
        "    return get_vecs_by_tokens\n",
        "\n",
        "get_vecs_by_tokens = func_builder(vec)"
      ],
      "id": "fwPNgSxIaw8v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, GloVe vectors were trained on whole words, so we need to tokenize our sentences accordingly. We can use Gensim's own simple_preprocess() method (introduced earlier in this chapter) to get a list of tokens:"
      ],
      "metadata": {
        "id": "fDJr-80jrIUb"
      },
      "id": "fDJr-80jrIUb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPcN9eQ-aw8v",
        "outputId": "a288f30c-38f7-40bf-eceb-9cdad21d55a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['reuters',\n",
              " 'short',\n",
              " 'sellers',\n",
              " 'wall',\n",
              " 'street',\n",
              " 'dwindling',\n",
              " 'band',\n",
              " 'of',\n",
              " 'ultra',\n",
              " 'cynics',\n",
              " 'are',\n",
              " 'seeing',\n",
              " 'green',\n",
              " 'again']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "tokens = simple_preprocess(descriptions[0])\n",
        "tokens"
      ],
      "id": "LPcN9eQ-aw8v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can use the get_vecs_by_tokens() function to retrieve the GloVe embeddings for our sentence:"
      ],
      "metadata": {
        "id": "gw_wdBxDrOIR"
      },
      "id": "gw_wdBxDrOIR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9UH_sqXaw8w",
        "outputId": "876d9669-b23d-4a67-fabb-641e3b559062"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([14, 50])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedded_tokens = get_vecs_by_tokens(tokens)\n",
        "embedded_tokens.shape"
      ],
      "id": "f9UH_sqXaw8w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fourteen tokens, 50 dimensions each. Notice that a call to this method completely skips over the step of transforming tokens into indices (it's performed internally using the encode_str() function), thus returning the corresponding embeddings directly. This won't be the case for more sophisticated approaches, such as contextual embeddings, where embeddings don't actually work like a lookup table anymore. We're getting ahead of ourselves, though. Let's keep it simple for now."
      ],
      "metadata": {
        "id": "gyWdt3rurSEi"
      },
      "id": "gyWdt3rurSEi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c5cd348"
      },
      "source": [
        "## 15.7 Vector Databases\n",
        "\n",
        "###Vector Databases: Overview\n",
        "In the age of Large Language Models (LLMs), vector databases are all the rage! Why? Because they are perfect for storing vectors, that is, embeddings. They make it very easy to search for embeddings that are similar to each other, and this particular characteristic plays a crucial role in providing context to LLMs, as we'll see in a bit more detail towards the end of this course.\n",
        "\n",
        "There are many databases, some of them work in-memory only, some can be optionally persisted to disk, and some are full-fledged databases in the sense they support typical operations as in relational databases.\n",
        "\n",
        "In this course, we'll be briefly discussing ChromaDB only, given its ease of use, and the fact that it can be easily run in a Jupyter notebook."
      ],
      "id": "9c5cd348"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "676af97f"
      },
      "source": [
        "### 15.7.1 ChromaDB\n",
        "\n",
        "[ChromaDB](https://docs.trychroma.com/getting-started) is an open-source embedding database that allows you store embeddings and metadata, embed documents and queries, and search embeddings.\n",
        "\n",
        "In this example, we'll be storing a collection of GloVe embeddings for the AG News Dataset on a persisted database, and then will query the collection to search for similar items.\n",
        "\n",
        "Creating a database in ChromaDB follows a short sequence of steps:\n",
        "- getting a client, which we can configure to persist the data\n",
        "- creating a collection that will store the embeddings and metadata - you can think of it as a folder or table\n",
        "- adding documents (to be embedded by ChromaDB itself) or embeddings (as we're doing here) to the collection, along with any corresponding metadata you may wish to add\n",
        "- querying the collection to get the most similar results back\n",
        "\n",
        "Let's get a client and define `agnews_db` as the folder our collection must be saved to:"
      ],
      "id": "676af97f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get a client and define agnews_db as the folder our collection must be saved to:"
      ],
      "metadata": {
        "id": "qoszS65wyemM"
      },
      "id": "qoszS65wyemM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3efc931f"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"./agnews_db\")"
      ],
      "id": "3efc931f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the collection itself is easy, we only have to give it a name:"
      ],
      "metadata": {
        "id": "PLMa2tFvykUn"
      },
      "id": "PLMa2tFvykUn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b2b4719"
      },
      "outputs": [],
      "source": [
        "collection = client.create_collection(\"agnews_collection\")"
      ],
      "id": "6b2b4719"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we go over our dataset, compute and store embeddings, the documents (sentences) and their IDs, and their labels as metadata."
      ],
      "metadata": {
        "id": "GrVM0wagynXw"
      },
      "id": "GrVM0wagynXw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2088f815"
      },
      "source": [
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch13/vector_db.png)"
      ],
      "id": "2088f815"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Populating a Vector Database"
      ],
      "metadata": {
        "id": "VzPnuGRpyqwY"
      },
      "id": "VzPnuGRpyqwY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first helper function, tokenize_batch(), applies the tokenizer to each sentence in a mini-batch, while the second helper function, get_bag_of_embeddings(), compute the average of the embeddings of every token in every sentence of a mini-batch:"
      ],
      "metadata": {
        "id": "lApU9GD5ytd3"
      },
      "id": "lApU9GD5ytd3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THv8a8vdaw8w"
      },
      "outputs": [],
      "source": [
        "def tokenize_batch(sentences, tokenizer=None):\n",
        "    if tokenizer is None:\n",
        "        tokenizer = simple_preprocess\n",
        "\n",
        "    return [tokenizer(s) for s in sentences]\n",
        "\n",
        "def get_bag_of_embeddings(tokens):\n",
        "    embeddings = torch.cat([get_vecs_by_tokens(s).mean(axis=0).unsqueeze(0) for s in tokens], dim=0)\n",
        "    return embeddings"
      ],
      "id": "THv8a8vdaw8w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we're creating a new, unshuffled, data loader for our training set so we can assign a sequential number to each data point as its ID."
      ],
      "metadata": {
        "id": "piCwFGZKyzUs"
      },
      "id": "piCwFGZKyzUs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b04a64d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "unshuffled_dl = DataLoader(dataset=datasets['train'], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for i, batch in enumerate(unshuffled_dl):\n",
        "    labels, sentences = batch['topic'], batch['news']\n",
        "    tokens = tokenize_batch(sentences)\n",
        "    embeddings = get_bag_of_embeddings(tokens)\n",
        "    ids = [f'{i:06}' for i in np.arange(i*batch_size, i*batch_size+len(sentences))]\n",
        "\n",
        "    collection.add(embeddings=embeddings.tolist(),\n",
        "                   documents=sentences,\n",
        "                   metadatas=[{'label': v} for v in labels.tolist()],\n",
        "                   ids=ids)\n",
        "\n",
        "    if i == 300: # roughly 10k docs\n",
        "        break"
      ],
      "id": "7b04a64d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffddd9cc",
        "outputId": "91d11039-ad3d-4985-fa03-e3f8e8862d52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9632"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection.count()"
      ],
      "id": "ffddd9cc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have loaded roughly 10,000 documents to our database, let's query it!"
      ],
      "metadata": {
        "id": "rjJKRmEOy4xM"
      },
      "id": "rjJKRmEOy4xM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b119f4b8"
      },
      "source": [
        "### 15.7.2 Similarity Search\n",
        "\n",
        "In a nutshell, that's what vector databases are built for: similarity search. They allow faster comparison among hundreds of thousands of vectors. We're not discussing their implementation details, though. We're interested in using their ability to quickly search for similar vectors (embeddings) to find and group similar documents together.\n",
        "\n",
        "Let's take a sentence from the AG News Dataset that talks about a nuclear plant and compute its bag of embeddings (pretending it wasn't loaded in the database already):"
      ],
      "id": "b119f4b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "879e7f65",
        "outputId": "a8798a05-d614-47e4-87e6-8bbcb461db81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 4.0827e-01,  7.9920e-02,  3.1115e-01,  1.8721e-01, -4.7369e-02,\n",
              "         3.3698e-01, -5.0617e-01, -3.6810e-02,  2.6068e-01, -1.2847e-01,\n",
              "         2.2948e-01, -5.9424e-02, -3.3787e-01,  2.9188e-02,  2.6071e-01,\n",
              "         2.0179e-01, -7.7526e-02,  3.3718e-01, -5.2526e-01, -2.7158e-01,\n",
              "         3.7156e-01, -1.0214e-01, -1.1645e-01, -2.9637e-01,  7.9672e-02,\n",
              "        -1.6904e+00, -4.1659e-02,  1.0523e-01,  2.8247e-01,  2.9835e-03,\n",
              "         3.0708e+00, -1.5180e-01, -1.4941e-01, -2.3085e-01,  2.1777e-01,\n",
              "        -4.0086e-02,  2.0281e-01,  8.9309e-02,  5.5554e-02,  2.6830e-02,\n",
              "        -2.9215e-01, -1.5104e-01,  2.6449e-01, -1.0034e-01,  1.4842e-01,\n",
              "         8.8036e-02, -2.3717e-01,  3.1029e-01,  1.2701e-02, -1.4513e-01])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_sentence = 'The company running the Japanese nuclear plant hit by a fatal accident is to close its reactors for safety checks.'\n",
        "query_tokens = tokenize_batch([query_sentence])\n",
        "query_embeddings = get_bag_of_embeddings(query_tokens)[0]\n",
        "\n",
        "query_embeddings"
      ],
      "id": "879e7f65"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The query embeddings must be computed and/or retrieved in the same way the embeddings stored in the database were. You cannot mix and match different embeddings. If we stored bags of GloVe embeddings in the database, we need to query it using bags of GloVe embeddings as well."
      ],
      "metadata": {
        "id": "T1h79K2R7XhX"
      },
      "id": "T1h79K2R7XhX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e557588c"
      },
      "source": [
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch13/query_db.png)"
      ],
      "id": "e557588c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Querying a Vector Database"
      ],
      "metadata": {
        "id": "H-rQA-t07a4R"
      },
      "id": "H-rQA-t07a4R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, let's query the database using the computed query embeddings, and retrieve the top 5 results for it:"
      ],
      "metadata": {
        "id": "-Fx_vTzG7fEz"
      },
      "id": "-Fx_vTzG7fEz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ae3e903",
        "outputId": "c1626a40-7542-4785-836c-3b6c9de2989e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': [['000030', '001046', '004715', '002464', '006905']],\n",
              " 'distances': [[0.0,\n",
              "   0.8038501739501953,\n",
              "   0.9175586104393005,\n",
              "   0.9644219875335693,\n",
              "   0.9812381267547607]],\n",
              " 'metadatas': [[{'label': 2},\n",
              "   {'label': 0},\n",
              "   {'label': 0},\n",
              "   {'label': 0},\n",
              "   {'label': 2}]],\n",
              " 'embeddings': None,\n",
              " 'documents': [['The company running the Japanese nuclear plant hit by a fatal accident is to close its reactors for safety checks.',\n",
              "   'AP - The operator of a nuclear power plant where a long-neglected cooling pipe burst and killed four workers last week said Monday that four other pipes at its reactors also went unchecked for years.',\n",
              "   'TOKYO The operators of a Japanese nuclear plant say there was no evidence of danger at the plant before a deadly explosion this month.',\n",
              "   'Reuters - No more Japanese nuclear reactors need to be closed for inspections, electric power companies said on Wednesday after submitting reports ordered by the government following a reactor accident that killed four workers last week.',\n",
              "   \"The company managing Rio Tinto iron ore's rail lines in the Pilbara, in north-west Western Australia, says equipment failure rather than human error is to blame for a major derailment last week.\"]],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'included': ['metadatas', 'documents', 'distances']}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_embeddings = query_embeddings.tolist()\n",
        "collection.query(query_embeddings=query_embeddings, n_results=5)"
      ],
      "id": "1ae3e903"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first, and obvious, result is the sentence itself - having a distance of exactly zero because it's exactly the same sentence.\n",
        "\n",
        "The other four results, the ones most similar (with the lower distances) from the original sentence, are also about nuclear power plants.\n",
        "\n",
        "What about searching for something that's not from the database itself, as a real query, or even a typical search term such as \"asian stock market\"? Let's check it out:"
      ],
      "metadata": {
        "id": "qrCyp7E_7ka_"
      },
      "id": "qrCyp7E_7ka_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ede4123"
      },
      "outputs": [],
      "source": [
        "query_sentence = 'asian stock market'\n",
        "query_tokens = tokenize_batch([query_sentence])\n",
        "query_embeddings = get_bag_of_embeddings(query_tokens)[0]\n",
        "query_embeddings = query_embeddings.tolist()"
      ],
      "id": "7ede4123"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d245d3b",
        "outputId": "1eee1a06-8cad-420e-ac00-2c5e0e05252e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': [['007389', '006925', '004791', '007014', '006829']],\n",
              " 'distances': [[5.2573628425598145,\n",
              "   5.490711688995361,\n",
              "   5.643772125244141,\n",
              "   5.697542190551758,\n",
              "   5.810704231262207]],\n",
              " 'metadatas': [[{'label': 2},\n",
              "   {'label': 2},\n",
              "   {'label': 0},\n",
              "   {'label': 0},\n",
              "   {'label': 2}]],\n",
              " 'embeddings': None,\n",
              " 'documents': [['Asian stocks rose after oil prices fell from a record on Friday, easing concern higher energy costs will damp consumer spending and corporate profits.',\n",
              "   'Asian stocks advanced after oil prices fell from a record Friday in New York, easing concern higher energy costs will damp consumer spending and corporate profits.',\n",
              "   \"AP - Tokyo's main stock index ended lower Friday amid profit-taking of technology issues and concerns about soaring oil prices. The U.S. dollar was down against the Japanese yen.\",\n",
              "   'Japanese stocks rose after oil prices fell from a record in New York on Friday, easing concern higher energy costs will damp consumer spending and corporate profits.',\n",
              "   'Japanese stocks may rise after oil prices fell from a record in New York, easing concern higher energy costs will damp consumer spending and corporate profits.']],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'included': ['metadatas', 'documents', 'distances']}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection.query(query_embeddings=query_embeddings, n_results=5)"
      ],
      "id": "9d245d3b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values for the distances are much higher now, but the most similar results are related to the \"asian stock market\": from literally \"asian stocks\", to \"Japanese stocks\", and \"Japan's Nikkei\" (stock index).\n",
        "\n",
        "As you can see, searching for similar documents is quite easy. ChromaDB reports the results as distances (low values, meaning the documents are \"close\" to each other, corresponding to documents that are similar to each other), but that's not the only metric you can use. Alternatively, it is possible to use a similarity metric instead, such as cosine similarity."
      ],
      "metadata": {
        "id": "wom-RKum7tIY"
      },
      "id": "wom-RKum7tIY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jEGSByZaw8x"
      },
      "source": [
        "***\n",
        "**ASIDE: Cosine Similarity**\n",
        "\n",
        "If two vectors are pointing in the same direction, their cosine similarity is a perfect one. If they are orthogonal (that is, if there is a right angle between them), their cosine similarity is zero. If they are pointing in opposite directions, their cosine similarity is minus one.\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\cos \\theta = \\frac{\\sum_i{x_iy_i}}{\\sqrt{\\sum_j{x_j^2}}\\sqrt{\\sum_j{y_j^2}}}\n",
        "$$\n",
        "***\n"
      ],
      "id": "2jEGSByZaw8x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use this similarity metric, implemented in PyTorch, to tackle a task that we've already discussed briefly."
      ],
      "metadata": {
        "id": "LQ85n6Qt7zLw"
      },
      "id": "LQ85n6Qt7zLw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1916f4f7"
      },
      "source": [
        "## 15.8 Zero-Shot Text Classification\n",
        "We have already seen an example of zero-shot image classification that was based on CLIP, OpenAI's model that bridges the gap between the worlds of computer vision and natural language processing. It accomplishes zero-shot classification by producing embeddings for both images and text, and comparing the image embeddings to the embeddings of each candidate label. The candidate label that has the embeddings that are most similar to those computed for the image being classified is deemed the right one.\n",
        "\n",
        "Let's try the same approach to accomplish zero-short text classification!\n",
        "\n",
        "Our candidate labels are the classes from the AG News Dataset: \"world\", \"sports\", \"business\", and \"science and technology\". Let's compute their embeddings (or bag of embeddings, in the case of the last, three-word, label):\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)"
      ],
      "id": "1916f4f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea70a16c",
        "outputId": "ec3b193e-8fde-49c5-c6b1-ddbb67e02bdb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 50])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cand_labels = [\"world\", \"sports\", \"business\", \"science and technology\"]\n",
        "\n",
        "cand_emb = torch.vstack([get_vecs_by_tokens(tokens).mean(axis=0) for tokens in tokenize_batch(cand_labels)])\n",
        "cand_emb.shape"
      ],
      "id": "ea70a16c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare two embeddings, we can use the typical cosine similarity metric. If two vectors are identical (except for their norm), their similarity is one. If they are orthogonal to each other, their similarity is zero.\n",
        "\n",
        "We can use PyTorch's own nn.CosineSimilarity to easily compute, for example, a similarity matrix between our candidate labels:"
      ],
      "metadata": {
        "id": "ZCuD7gQE8Iwy"
      },
      "id": "ZCuD7gQE8Iwy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e73d28aa",
        "outputId": "d7558f4e-8164-4f1c-dcca-ce770617ab97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.6529, 0.6136, 0.6678],\n",
              "        [0.6529, 1.0000, 0.6410, 0.6171],\n",
              "        [0.6136, 0.6410, 1.0000, 0.8069],\n",
              "        [0.6678, 0.6171, 0.8069, 1.0000]])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cos = nn.CosineSimilarity(dim=2)\n",
        "\n",
        "cos(cand_emb.unsqueeze(1), cand_emb.unsqueeze(0))"
      ],
      "id": "e73d28aa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the main diagonal compares each label to itself, so the similarity is a perfect one. Notice the calls to each tensor's unsqueeze() method, required to make the N-to-N comparison.\n",
        "\n",
        "Now, let's do exactly the same using a mini-batch from our validation/test data loader, and comparing the computed embeddings to those of the candidate labels:"
      ],
      "metadata": {
        "id": "7QeXDmNL8NP_"
      },
      "id": "7QeXDmNL8NP_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-s2TJDMaw8x",
        "outputId": "9043fd17-5532-4bd1-99e5-8db096bd7587"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.6534, 0.5361, 0.7945, 0.7030],\n",
              "        [0.7634, 0.6130, 0.7007, 0.7579],\n",
              "        [0.6658, 0.5156, 0.7244, 0.8548],\n",
              "        [0.7275, 0.5425, 0.6800, 0.7176],\n",
              "        [0.7058, 0.5120, 0.7004, 0.7567],\n",
              "        [0.7184, 0.5626, 0.7428, 0.7919],\n",
              "        [0.7056, 0.5410, 0.7269, 0.8026],\n",
              "        [0.6613, 0.5637, 0.7525, 0.8029],\n",
              "        [0.5893, 0.4821, 0.6439, 0.6403],\n",
              "        [0.7183, 0.5389, 0.7209, 0.7500],\n",
              "        [0.6883, 0.6081, 0.8330, 0.8441],\n",
              "        [0.6728, 0.6266, 0.8165, 0.8166],\n",
              "        [0.7453, 0.6166, 0.8239, 0.8035],\n",
              "        [0.7400, 0.4860, 0.6221, 0.7080],\n",
              "        [0.6582, 0.4579, 0.6692, 0.7232],\n",
              "        [0.6819, 0.4262, 0.6630, 0.7159],\n",
              "        [0.6154, 0.4228, 0.5796, 0.6999],\n",
              "        [0.7636, 0.5894, 0.7239, 0.8140],\n",
              "        [0.6990, 0.4981, 0.6911, 0.8043],\n",
              "        [0.6747, 0.4536, 0.7283, 0.6745],\n",
              "        [0.7615, 0.5422, 0.7594, 0.7675],\n",
              "        [0.6438, 0.5324, 0.7470, 0.7863],\n",
              "        [0.5911, 0.4596, 0.7293, 0.8482],\n",
              "        [0.7202, 0.5606, 0.7810, 0.7704],\n",
              "        [0.6610, 0.5386, 0.8161, 0.7544],\n",
              "        [0.7272, 0.5415, 0.7617, 0.8091],\n",
              "        [0.8396, 0.5678, 0.5800, 0.6891],\n",
              "        [0.6894, 0.5641, 0.6844, 0.7507],\n",
              "        [0.7033, 0.5307, 0.6915, 0.7208],\n",
              "        [0.6693, 0.5329, 0.6521, 0.6608],\n",
              "        [0.6276, 0.5180, 0.6498, 0.6811],\n",
              "        [0.6656, 0.5054, 0.5853, 0.6365]])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset=datasets['test'], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "batch = next(iter(dataloader))\n",
        "labels, sentences = batch['topic'], batch['news']\n",
        "tokens = tokenize_batch(sentences)\n",
        "embeddings = get_bag_of_embeddings(tokens)\n",
        "similarities = cos(embeddings.unsqueeze(1), cand_emb.unsqueeze(0))\n",
        "similarities"
      ],
      "id": "b-s2TJDMaw8x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted class is, as already mentioned, the one the candidate label embedding is the most similar to the sentence embedding being classified:"
      ],
      "metadata": {
        "id": "MaYpXio-8Q_l"
      },
      "id": "MaYpXio-8Q_l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61450876",
        "outputId": "e4f05096-39dd-4161-a341-980922bd179b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2, 0, 3, 0, 3, 3, 3, 3, 2, 3, 3, 3, 2, 0, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2,\n",
              "        2, 3, 0, 3, 3, 0, 3, 0])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_class = similarities.argmax(dim=1)\n",
        "predicted_class"
      ],
      "id": "61450876"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is it any good? Let's see how many of those match the actual labels:"
      ],
      "metadata": {
        "id": "-guPofhD8Vce"
      },
      "id": "-guPofhD8Vce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e48b587d",
        "outputId": "beafad3b-0a1f-453c-d124-6394a521b262"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5625)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(predicted_class == labels).float().mean()"
      ],
      "id": "e48b587d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad for such a simple approach, right? But, does it hold for the whole dataset?"
      ],
      "metadata": {
        "id": "1r54g6ML8Y0o"
      },
      "id": "1r54g6ML8Y0o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9707b1fb"
      },
      "source": [
        "### 15.8.1 Evaluation\n",
        "\n",
        "We can run the very same evaluation as in the lab, except for the fact that the \"predictions\" are not logits coming out of a model anymore, but similarities among embeddings:"
      ],
      "id": "9707b1fb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d70bcb1"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric1 = evaluate.load('precision', average=None)\n",
        "metric2 = evaluate.load('recall', average=None)\n",
        "metric3 = evaluate.load('accuracy')"
      ],
      "id": "6d70bcb1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40d627b8"
      },
      "outputs": [],
      "source": [
        "for batch in dataloader:\n",
        "    labels, sentences = batch['topic'], batch['news']\n",
        "    tokens = tokenize_batch(sentences)\n",
        "    embeddings = get_bag_of_embeddings(tokens)\n",
        "\n",
        "    # predictions = model(embeddings)\n",
        "    predictions = cos(embeddings.unsqueeze(1), cand_emb.unsqueeze(0))\n",
        "\n",
        "    pred_class = predictions.argmax(dim=1).tolist()\n",
        "    labels = labels.tolist()\n",
        "\n",
        "    metric1.add_batch(references=labels, predictions=pred_class)\n",
        "    metric2.add_batch(references=labels, predictions=pred_class)\n",
        "    metric3.add_batch(references=labels, predictions=pred_class)"
      ],
      "id": "40d627b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101ae04d",
        "outputId": "ebac00c2-3906-4170-dc3b-55e8880b5723"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'precision': array([0.33205619, 1.        , 0.67253045, 0.43290471])},\n",
              " {'recall': array([4.10526316e-01, 5.26315789e-04, 7.84736842e-01, 6.91052632e-01])},\n",
              " {'accuracy': 0.47171052631578947})"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric1.compute(average=None), metric2.compute(average=None), metric3.compute()"
      ],
      "id": "101ae04d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, that's not so great. Apparently, the \"sports\" label isn't the most similar to any of the sentences in our dataset.\n",
        "\n",
        "Perhaps if we had chosen different words/sentences as candidate labels, we could have achieved better results. Wouldn't it be nice to know which words in a sentence are the most meaningful words when it comes to classifying the sentence as belonging to a given topic or not?\n",
        "\n",
        "In a way, we're interested in knowing what the model is paying attention to. The keyword here is \"attention\": it revolutionized the world of natural language processing, and fostered the development of contextual word embeddings, which are much better at, well, embedding the meaning of individual words and sentences.\n",
        "\n",
        "The quality of the results you get, either if it's searching for similar documents or performing zero-shot text classification, depends on the quality of the embeddings you use. Better embeddings, better results. So, let's up our game and go for contextual embeddings!\n",
        "\n",
        "Before moving on, however, we need to tackle one last topic here."
      ],
      "metadata": {
        "id": "jDTFPBen8muY"
      },
      "id": "jDTFPBen8muY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51585ffc"
      },
      "source": [
        "## 15.9 Chunking Strategies\n",
        "\n",
        "#### Chunking Strategies: Overview\n",
        "The AG News dataset contains very short pieces of text, usually a single sentence. Most of the time, however, pieces of text are much longer than that: paragraphs, pages, even full books. Even though language models are getting larger and able to take longer sequences, there's always a hard limit that forces a sequence to be truncated.\n",
        "\n",
        "The good news is, we can always split the original text into chunks (if you go back to the figures in the \"Vector Databases\" section, you'll see it depicts chunks being transformed into embeddings). The bad news is, there is no right or wrong answer to how you should split the text into chunks. It depends on a series of factors such as the type of text you're dealing with (long reports or short tweets, for example), the model you're using to embed the text and the nature of your queries (more on that later), and limitations in size (models typically have a maximum input length as we've already seen).\n",
        "\n",
        "For more details, check the \"[Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/)\" blog post.\n",
        "\n",
        "Having said that, it's possible to chunk your text using a fixed-length or a content-aware approach. Let's take a quick look at some of them. We'll use a paragraph of text from a financial report, reproduced below, to illustrate two different chunking strategies:"
      ],
      "id": "51585ffc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-yiUdoeaw8z"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "ITEM 1A. RISK FACTORS Our operations and financial results are subject to various risks and uncertainties, including those described below, that could adversely affect our business, financial condition, results of operations, cash flows, and the trading price of our common stock. STRATEGIC AND COMPETITIVE RISKS We face intense competition across all markets for our products and services, which may lead to lower revenue or operating margins.    Competition in the technology sector Our competitors range in size from diversified global companies with significant research and development resources to small, specialized firms whose narrower product lines may let them be more effective in deploying technical, marketing, and financial resources. Barriers to entry in many of our businesses are low and many of the areas in which we compete evolve rapidly with changing and disruptive technologies, shifting user needs, and frequent introductions of new products and services. Our ability to remain competitive depends on our success in making innovative products, devices, and services that appeal to businesses and consumers.    Competition among platform-based ecosystems An important element of our business model has been to create platform-based ecosystems on which many participants can build diverse solutions. A well-established ecosystem creates beneficial network effects among users, application developers, and the platform provider that can accelerate growth. Establishing significant scale in the marketplace is necessary to achieve and maintain attractive margins. We face significant competition from firms that provide competing platforms.\n",
        "\"\"\""
      ],
      "id": "K-yiUdoeaw8z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPdgqzqEaw8z"
      },
      "source": [
        "### 15.9.1 Fixed-Length\n",
        "\n",
        "Fixed-length approaches split the text into equal-length chunks (e.g. 300 characters/words/tokens) with or without some overlap between them. Langchain, a Python package that has grown a lot in popularity in the last few months, and that allows you to integrate different tools (e.g. embedding models, vector databases) into a workflow, also offers a convenient way of splitting text into chunks of fixed-length:"
      ],
      "id": "wPdgqzqEaw8z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeb5aacb",
        "outputId": "2fd59ac1-dfb2-4519-b56e-65172155e461"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='ITEM 1A. RISK FACTORS Our operations and financial results are subject to various risks and uncertainties, including those described below, that could adversely affect our business, financial condition, results of operations, cash flows, and the trading', metadata={}),\n",
              " Document(page_content='and the trading price of our common stock. STRATEGIC AND COMPETITIVE RISKS We face intense competition across all markets for our products and services, which may lead to lower revenue or operating margins.    Competition in the technology sector Our', metadata={}),\n",
              " Document(page_content='sector Our competitors range in size from diversified global companies with significant research and development resources to small, specialized firms whose narrower product lines may let them be more effective in deploying technical, marketing, and', metadata={})]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)\n",
        "chunks = text_splitter.create_documents([text])\n",
        "chunks[:3]"
      ],
      "id": "eeb5aacb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f607097"
      },
      "source": [
        "### 15.9.2 Content-Aware\n",
        "\n",
        "The problem with fixed-length is that the chunks will most certainly end mid-sentence. The overlap may help mitigate this issue, but it won't work for long sentences. That's when the content-aware approach comes in. We can split it by sentences or paragraphs using indications in the text's structure. We could, for example, naively split the text into sentences using the period (.) as an indication of the end of a sentence. What about exclamation and question marks?\n",
        "\n",
        "Fortunately, sentence tokenizing is a very well-known problem, and the [traditional Natural Language Toolkit (NLTK)](https://www.nltk.org/) package has a sentence tokenizer available. We only need to download the punkt tokenizer package and it will be ready to be used:"
      ],
      "id": "4f607097"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1344c93e",
        "outputId": "3a1c2fbb-6e0c-450b-88da-160a636407ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/dvgodoy/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "id": "1344c93e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eade466d",
        "outputId": "e443e851-2ace-4a27-9556-23be30dfd349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\nITEM 1A.',\n",
              " 'RISK FACTORS Our operations and financial results are subject to various risks and uncertainties, including those described below, that could adversely affect our business, financial condition, results of operations, cash flows, and the trading price of our common stock.',\n",
              " 'STRATEGIC AND COMPETITIVE RISKS We face intense competition across all markets for our products and services, which may lead to lower revenue or operating margins.']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "chunks = sent_tokenize(text)\n",
        "chunks[:3]"
      ],
      "id": "eade466d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, each element in the list of documents is a single sentence."
      ],
      "metadata": {
        "id": "rCteN1Rl-Ufw"
      },
      "id": "rCteN1Rl-Ufw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55a5898"
      },
      "source": [
        "### 15.9.3 Custom\n",
        "\n",
        "Sometimes, as in the case of our example, there's some other indication to the text's structure: it looks like paragraphs are separated by a sequence of two or more spaces. Let's try it out:"
      ],
      "id": "b55a5898"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67e8137f",
        "outputId": "2e0c5e10-b611-4856-d5d1-843b012939f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\nITEM 1A. RISK FACTORS Our operations and financial results are subject to various risks and uncertainties, including those described below, that could adversely affect our business, financial condition, results of operations, cash flows, and the trading price of our common stock. STRATEGIC AND COMPETITIVE RISKS We face intense competition across all markets for our products and services, which may lead to lower revenue or operating margins.',\n",
              " '',\n",
              " 'Competition in the technology sector Our competitors range in size from diversified global companies with significant research and development resources to small, specialized firms whose narrower product lines may let them be more effective in deploying technical, marketing, and financial resources. Barriers to entry in many of our businesses are low and many of the areas in which we compete evolve rapidly with changing and disruptive technologies, shifting user needs, and frequent introductions of new products and services. Our ability to remain competitive depends on our success in making innovative products, devices, and services that appeal to businesses and consumers.']"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks = text.split('  ')\n",
        "chunks[:3]"
      ],
      "id": "67e8137f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks good, these are definitely paragraphs. Unfortunately, this is a strategy that's specific to this particular document only."
      ],
      "metadata": {
        "id": "R58knPM--dph"
      },
      "id": "R58knPM--dph"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}